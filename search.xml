<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Gumbel</title>
    <url>/post/4c3c76c9.html</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>朴素贝叶斯</title>
    <url>/post/f77f0066.html</url>
    <content><![CDATA[<h2 id="1-朴素贝叶斯"><a href="#1-朴素贝叶斯" class="headerlink" title="1. 朴素贝叶斯"></a>1. 朴素贝叶斯</h2><h4 id="1-1-极大似然估计的朴素贝叶斯"><a href="#1-1-极大似然估计的朴素贝叶斯" class="headerlink" title="1.1 极大似然估计的朴素贝叶斯"></a>1.1 极大似然估计的朴素贝叶斯</h4><ul>
<li><p><strong>特点：</strong> 计算简单，假设约束较强，假设每个条件分布都是独立的。</p>
</li>
<li><p><strong>推导时使用的数学工具：</strong></p>
<ul>
<li>条件独立假设</li>
<li>通过贝叶斯公式，得到后验概率</li>
<li>构建0-1损失函数，对其进行推导可得到0-1损失函数时的期望风险最小化准则与后验概率最大化准则的等价性</li>
<li>由上一条得出最大后验概率</li>
<li>使用极大似然估计，对先验概率及条件概率进行估计。</li>
</ul>
</li>
<li><p><strong>算法目标即核心公式：</strong></p>
<ul>
<li><p>目标:</p>
<p>​        是为了推出最大后验概率，计算过程中用到联合概率及先验概率，因而是生成模型，将生成数据的过程全都算了一遍</p>
</li>
<li><p>核心公式：</p>
<script type="math/tex; mode=display">
P(Y|X) = \frac{P(X,Y)}{P(X)} = \frac{P(X|Y)P(Y)}{\sum_{Y}P(Y)P(X|Y)}</script><a id="more"></a>
</li>
</ul>
</li>
<li><p><strong>算法步骤：</strong></p>
</li>
</ul>
<ol>
<li><p>计算先验概率及条件概率</p>
<ul>
<li>先验：</li>
</ul>
<script type="math/tex; mode=display">
P(Y=c_k)=\frac{\sum_{i=1}^{N} \mathbb{I}(y_i = c_k)}{N},k=1,2,...,K</script><ul>
<li>条件：</li>
</ul>
<script type="math/tex; mode=display">
P(X^{(j)}=a_{jl}|Y=c_k) = \frac{\sum_{i=1}^{N} \mathbb{I}(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^{N}\mathbb{I}(y_i=c_k)}\space ,j=1,2..,N; l=1,2,...,S_j;k=1,2,...,K</script></li>
</ol>
<ol>
<li><p>对于给定的数据集计算不同类别的后验概率（j是对数据集的每个特征进行条件概率累乘）</p>
<script type="math/tex; mode=display">
P(Y=c_k|X^{(j)}=x^{(j)})=P(Y=c_k)\prod _{j=1}^{n}P(X^{(j)}=x^{(j)} | Y=c_k), k=1,2,...K</script></li>
<li><p>选择最大的后验概率作为最终分类</p>
</li>
</ol>
<script type="math/tex; mode=display">
y=argmax_{c_k}P(Y=c_k|X^{(j)}=x^{(j)})</script><h4 id="1-2-贝叶斯估计的朴素贝叶斯"><a href="#1-2-贝叶斯估计的朴素贝叶斯" class="headerlink" title="1.2 贝叶斯估计的朴素贝叶斯"></a>1.2 贝叶斯估计的朴素贝叶斯</h4><p>由于极大似然估计的朴素贝叶斯，在极大似然估计过程中可能出现概率为0的情况，影响后续计算。因而使用贝叶斯估计，在先验概率及条件概率公式中加入$\lambda$作为平滑项。</p>
<ul>
<li><p>先验：($K$为分类类别)</p>
<script type="math/tex; mode=display">
P(Y=c_k)=\frac{\sum_{i=1}^{N} \mathbb{I}(y_i = c_k)+\lambda}{N+K\lambda}</script></li>
<li><p>条件:($S_j$为第j个特征的可能的取值)</p>
</li>
</ul>
<script type="math/tex; mode=display">
P(X^{(j)}=a_{jl}|Y=c_k) = \frac{\sum_{i=1}^{N} \mathbb{I}(x_i^{(j)}=a_{jl},y_i=c_k)+\lambda}{\sum_{i=1}^{N}\mathbb{I}(y_i=c_k)+S_j\lambda}</script><p>当$\lambda = 0$的时候即为极大似然估计，$\lambda=1$的时候为拉普拉斯平滑。     </p>
<p>​              </p>
<h2 id="2-由贝叶斯得到的Sigmoid"><a href="#2-由贝叶斯得到的Sigmoid" class="headerlink" title="2. 由贝叶斯得到的Sigmoid"></a>2. 由贝叶斯得到的Sigmoid</h2><p>在二分类过程中，由上述的二分类的贝叶斯公式可以得到：</p>
<script type="math/tex; mode=display">
P(c_1|x) = \frac{P(x|c_1)P(c_1)}{P(x|c_1)P(c_1)+P(x|c_2)P(c_2)}</script><p>对该式上下除以分子得到：</p>
<script type="math/tex; mode=display">
\begin{aligned}
P(c_1|x)&=\frac{1}{1+\frac{P(c_2|x)P(c_2)}{P(c_1|x)P(c_1)}} \\
&=\frac{1}{1+e^{-z}}
\end{aligned}</script><p>其中：</p>
<script type="math/tex; mode=display">
z=ln \frac{P(x|c_1)P(c_1)}{P(x|c_2)P(c_2)}</script><h2 id="3-分类问题"><a href="#3-分类问题" class="headerlink" title="3. 分类问题"></a>3. 分类问题</h2><p>从贝叶斯角度入手，对于二分类问题，可以用两个高斯分布去对其进行极大似然估计，$N_1(\mu1,\Sigma_1)$,$N_2(\mu2,\Sigma_2)$</p>
<p>如果两个分布的$\Sigma$不同，则其分类决策面是一个非线性的，两个分布的$\Sigma$相同，其分类决策面是一个线性的，其中的原因可以由使用贝叶斯公式得到的Sigmoid函数的展开来说明。两个分布的协方差矩阵相同的话可以做一次简化，使其形式上是线性方程。</p>
<p>推导过程参考<a href="https://www.bilibili.com/video/av10590361/?p=10" target="_blank" rel="noopener">李宏毅61:35s</a></p>
<h2 id="4-Linear-Regression-和-Logistic-Regression"><a href="#4-Linear-Regression-和-Logistic-Regression" class="headerlink" title="4. Linear Regression 和 Logistic Regression"></a>4. Linear Regression 和 Logistic Regression</h2><h4 id="4-1-关于交叉熵"><a href="#4-1-关于交叉熵" class="headerlink" title="4.1 关于交叉熵"></a>4.1 关于交叉熵</h4><p>在逻辑回归中，对于二分类问题可以定义其似然函数为：</p>
<script type="math/tex; mode=display">
L(w,b) = \prod_{i=1}^{n}[f_{w,b}(x^{i})^{\hat{y}^{i}}]\cdot [1-f_{w,b}(x^{i})^{1-\hat{y}^{i}}]</script><p>对其取对数似然:</p>
<script type="math/tex; mode=display">
-lnL(w,b)=-\sum_{i=1}^{n}\hat{y}^{i}lnf_{w,b}(x^n)+ (1-\hat{y}^{i})ln(1-f_{w,b}(x^n))</script><p>这里$\sum$后面的部分就是交叉熵$C$</p>
<script type="math/tex; mode=display">
H(p,q)=-\sum_{x}p(x)ln(q(x))</script><p>其中$p$为训练集中的样本标签分布：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&p(x=1)=\hat{y}^n \\
&p(x=0)=1-\hat{y}^n
\end{aligned}</script><p>其中$q$为模型预测的标签分布：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&q(x=1)=f_{w,b}(x^n) \\
&q(x=0)=1-f_{w,b}(x^n) 
\end{aligned}</script><h4 id="4-2-二者异同"><a href="#4-2-二者异同" class="headerlink" title="4.2 二者异同"></a>4.2 二者异同</h4><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left"></th>
<th style="text-align:left">Logistic Regression</th>
<th>Linear Regression</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">函数集</td>
<td style="text-align:left">$f_{w,b}(x)=\sigma(\sum_i w_ix_i+b)$ 该函数值域介于0~1之间</td>
<td>$f_{w,b}(x)=\sum_i w_ix_i+b$ 该函数值域是任意实数</td>
</tr>
<tr>
<td style="text-align:left">Loss</td>
<td style="text-align:left">训练集：$(x^n,\hat{y}^n)$ ,$\hat{y}^n$满足伯努利分布，$L(f)= \sum _n C(f(x^n),\hat{y}^n)$</td>
<td>训练集：$(x^n,\hat{y}^n)$ ,$\hat{y}^n$是任意实数，$L(f)= \frac{1}{2}\sum _n (f(x^n),\hat{y}^n)^2$</td>
</tr>
<tr>
<td style="text-align:left">GD</td>
<td style="text-align:left">$ w_i=w_i - \eta\sum_n-(\hat{y}^n-f_{w,b}(x^n))x_i^n$</td>
<td>$ w_i=w_i - \eta\sum_n-(\hat{y}^n-f_{w,b}(x^n))x_i^n$</td>
</tr>
</tbody>
</table>
</div>
<h4 id="4-3-关于Logistic-Regression为何不使用平方误差作为loss"><a href="#4-3-关于Logistic-Regression为何不使用平方误差作为loss" class="headerlink" title="4.3 关于Logistic Regression为何不使用平方误差作为loss"></a>4.3 关于Logistic Regression为何不使用平方误差作为loss</h4><p>若像Linear Regression一样使用平方误差作为loss，公式展开后会发现，当$\hat{y}^n=0$时，无论$f_{w,b}(x^n)$为0还是1，其loss都为0，这使得训练过程极为缓慢，并且难以调参得到效果。</p>
]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>[论文]Real-Time Visual-Inertial Mapping, Re-localization and Planning Onboard MAVs in Unknown Environments</title>
    <url>/post/97601234.html</url>
    <content><![CDATA[<h2 id="1-贡献"><a href="#1-贡献" class="headerlink" title="1. 贡献"></a>1. 贡献</h2><ul>
<li>将通过视觉惯性里程计构造的local map 融合进global map</li>
<li>在地图上，使用在线重定位(online relocalization)补偿视觉惯性里程计漂移的问题</li>
<li>提出从稀疏的姿态图(pose graph) 构建稠密3D occupancy grid的方法，并用octomap八叉树地图表示</li>
<li>提出改进3D路径规划算法“Polynomial Trajectory Planning for Aggressive Quadrotor Flight in Dense Indoor Environments”</li>
<li>能够适应控制状态约束，提升稳定性及减少计算时间</li>
</ul>
<p>分层规划，</p>
<h2 id="2-平台"><a href="#2-平台" class="headerlink" title="2.平台"></a>2.平台</h2><p>AscTecFly + stereo camera + IMU</p>
<p>平台概述如下图：</p>
<ol>
<li>VI模块在local drift坐标下，返回pose消息，变换为Mission坐标（M）,该坐标系保证控制器层能够确保飞机的安全及避障</li>
<li>构建局部地图，以进行局部避障(不是本文的工作)</li>
<li>sparse mapping/relocalization层，并行构建全局地图（G），并进行<a href="https://ieeexplore.ieee.org/abstract/document/7139575/" target="_blank" rel="noopener">高效的存储</a>以便复用</li>
<li>在全局地图上构建全局三维路径规划</li>
<li>Mission controller被用来作为MAV和规划器之间的桥梁，手动控制时，速度控制指令直接给到MAV，轨迹跟踪时，全局规划信息需要转换为Mission坐标，给到MAV</li>
</ol>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1561709179/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190628160257.png" alt=""></p>
<a id="more"></a>
<h2 id="3-Mapping和Localization"><a href="#3-Mapping和Localization" class="headerlink" title="3.Mapping和Localization"></a>3.Mapping和Localization</h2><p>使用基于关键帧的视觉惯性里程计，通过在后台基于里程计的输出，来构建局部地图，进而使用局部地图进行relocation，以减少视觉惯性里程计带来的漂移误差。该方法还支持在后端跑BA进而减少误差。</p>
<h3 id="3-1-构建Local-map"><a href="#3-1-构建Local-map" class="headerlink" title="3.1 构建Local map"></a>3.1 构建Local map</h3><p>基于关键帧的视觉里程计使用的苏黎世他们自己提的“<strong>Keyframe-based visual–inertial odometry using nonlinear optimization</strong>”这篇论文的方法。由于计算能力限制，导致只有很少的历史关键帧能够被用来进行优化。文中使用了使用关键帧及关键特征点构建局部地图，来充分利用图优化过程，以此生成一个<strong>以关键帧为顶点</strong>，<strong>以IMU测量值为边</strong>的<strong>稀疏位姿图</strong>。顶点中主要包含帧图像关键点，关键点描述子，3D三角测量landmark。</p>
<p>本文所提出的姿势图的一个重要特征是，使用具有独立base帧的任务（子地图），使任务与global帧保持一致，所有顶点位姿和landmark都在local mission 帧中表示。这允许通过改变它们之间的基础帧变换来使得几个子图之间可以相对的对齐，而不必更新地图中的每个顶点或landmarks。使用时固定参考地图，并在新信息可用时不断更新局部地图和参考地图之间的转换。</p>
<h3 id="3-2-重定位"><a href="#3-2-重定位" class="headerlink" title="3.2 重定位"></a>3.2 重定位</h3><p>通过在BRISK特征描述子空间中检测最近邻，然后用RANSAC方案使用几何验证进行异常值排除。此过程中的任何内点都将作为约束添加到参考映射中的当前顶点和三角形3D关键点之间。</p>
<h3 id="3-3-全局稠密模型"><a href="#3-3-全局稠密模型" class="headerlink" title="3.3 全局稠密模型"></a>3.3 全局稠密模型</h3><p>在构建local map时，还记录了立体视差图像，将它们存储在MAV上的磁盘上，并将它们与稀疏重定位图相关联。只存储视觉测距的关键帧（位姿图中的顶点）的视差图，从而最大限度地减少对磁盘的访问。由于视觉惯性测距中的关键帧已经被选中并且其间具有足够的运动，这将产生均匀且有效的覆盖。在中断参考任务并运行BA后，迭代参考任务中的所有过去顶点，并使用更新的顶点位姿将视差图像重新投影到全局地图中。从而建立环境的3D occupancy grid。</p>
<h2 id="4-路径规划"><a href="#4-路径规划" class="headerlink" title="4. 路径规划"></a>4. 路径规划</h2><p>牺牲最优，来换取更少的计算时间，使用首先仅采样位置点使用直线路径，然后将得到的位置顶点用作支撑点以计算平滑的分段多项式轨迹，同时迭代地处理直线路径中不存在的碰撞，而不是在高维的状态空间中进行随机的采样，并生成多项式轨迹。</p>
<h3 id="4-1-无约束线性初始解决方案"><a href="#4-1-无约束线性初始解决方案" class="headerlink" title="4.1 无约束线性初始解决方案"></a>4.1 无约束线性初始解决方案</h3><p><a href="https://blog.csdn.net/q597967420/article/details/76099491" target="_blank" rel="noopener">轨迹规划参考Minimum snap</a>,这里面提到的方法使用的是将多项式轨迹的n阶导构建成一个带约束的QP问题，本文使用的方法，将其转化为一个<a href="https://blog.csdn.net/q597967420/article/details/79031791" target="_blank" rel="noopener">不带约束的QP问题</a>(适用于只有等式约束的闭式求解)。</p>
<h3 id="4-2-非线性轨迹细化"><a href="#4-2-非线性轨迹细化" class="headerlink" title="4.2 非线性轨迹细化"></a>4.2 非线性轨迹细化</h3><p>通过无约束初始方案得到分段的多项式轨迹，现在需要增加约束，使得无人机能够在每一个段中的速度及加速度最大，时间尽可能短。</p>
<p>(1) 构建非线性优化，在cost 函数中加入了每段轨迹行驶时间的平方和</p>
<p>(2) 解最大速度及加速度：如果使用在轨迹上进行离散采样，则会带来很多不等式约束的求解，对于较长的路径来说会造成计算缓慢。文中的处理方式是对速度的导数进行处理，将其变形为两个多项式方程的卷积的形式，然后使用JenkinsTraub 方法求解实数解</p>
<p>(3)将状态约束结合到非线性优化问题中:增加软约束</p>
<p>(4)使用轨迹插值进行轨迹优化防止由于速度过快，在轨迹拐点处产生过度冲击，通过插值的方式，对拐点处进行降速处理</p>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1561792488/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190629151434.png" alt=""></p>
<h2 id="5-实验结果"><a href="#5-实验结果" class="headerlink" title="5. 实验结果"></a>5. 实验结果</h2><h3 id="5-1-轨迹优化结果"><a href="#5-1-轨迹优化结果" class="headerlink" title="5.1 轨迹优化结果"></a>5.1 轨迹优化结果</h3><p>左图是使用初始线性解决方案的轨迹，右图是加入了非线性轨迹细化后的轨迹，可以看出其飞行时间更短，轨迹更接近于保持直线连接</p>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1561793743/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190629153511_1.png" alt=""></p>
<p>下表列出了轨迹分段下，线性初始化时间，优化时间，以及成功率的对比</p>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1561793743/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190629153518_1.png" alt=""></p>
<h3 id="5-2-地图生成及返航"><a href="#5-2-地图生成及返航" class="headerlink" title="5.2 地图生成及返航"></a>5.2 地图生成及返航</h3><ol>
<li>初始环境未知，飞行员给定high-level的位置指令对环境进行探索(下图的绿色线)。在探索过程中，构建稀疏位姿图以及由立体相机构建局部3D occupancy map，此时视觉里程计会产生一定的漂移，</li>
<li>返航阶段，经过BA的几次迭代后，通过将视差图从优化的关键帧位置投影到3D空间中来生成全局3D occupancy grid。</li>
<li>下图以粗体显示了reference路径，飞行轨迹使用的漂移的视觉里程计坐标。为了克服漂移，重定位模块尝试在每个关键帧（大约4Hz）对局部地图和经过BA后的参考地图进行调整，并在必要时校正参考轨迹，调整的地方在下图中用蓝色垂直线表示。可以看出，由于视点与先前看到的轨迹的较大偏差，在轨迹的开始和结束处仅存在闭环。结果还表明，控制器可以处理参考路径中的小跳跃，这表面了重定位单独运行的优势。在未来的工作中，可以触发规划器，以便在新的闭环情况下重新规划当前段。</li>
</ol>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1561796250/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190629161148.png" alt=""></p>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1561796251/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190629161201.png" alt=""></p>
]]></content>
      <tags>
        <tag>决策规划</tag>
      </tags>
  </entry>
  <entry>
    <title>[论文] An Open Source and Open Hardware Deep Learning-powered Visual Navigation Engine for Autonomous Nano-UAVs</title>
    <url>/post/2ee983bb.html</url>
    <content><![CDATA[<blockquote>
<p><a href="http://iis-projects.ee.ethz.ch/index.php?title=PULP-Shield_for_Autonomous_UAV&amp;oldid=3546" target="_blank" rel="noopener">硬件平台项目详情</a></p>
</blockquote>
<p>迁移resnet到RISC-V Soc</p>
<h2 id="1-改进Nano-scale-UAV"><a href="#1-改进Nano-scale-UAV" class="headerlink" title="1. 改进Nano-scale UAV"></a>1. 改进Nano-scale UAV</h2><ol>
<li>低功耗视觉导航模块<a href="http://iis-projects.ee.ethz.ch/index.php/PULP-Shield_for_Autonomous_UAV" target="_blank" rel="noopener">PULP-Shield</a><ul>
<li><a href="http://www.appnz.com/it/20190115_5650.html" target="_blank" rel="noopener">GreenWaves Technologies GAP8 SoC</a></li>
<li>ULP camera </li>
<li>Flash/DRAM memory</li>
<li>兼容 <a href="https://item.taobao.com/item.htm?id=531865932925&amp;ali_refid=a3_430582_1006:1121643831:N:vN07eM9kSLOskCIyJPK3aLXPPiJLN7BD:969dc8994432618e9cfbb7a8d5edb393&amp;ali_trackid=1_969dc8994432618e9cfbb7a8d5edb393&amp;spm=a230r.1.14.1#detail" target="_blank" rel="noopener">CrazyFlie 2.0 nano-UAV</a></li>
</ul>
</li>
<li>提出CNN-Based DroNet<ul>
<li>在常规尺寸无人机上做离线计算</li>
<li>在nano无人机上做在线计算</li>
</ul>
</li>
<li>功耗<ul>
<li>6fps 功耗64mW</li>
<li>18fps 功耗 272mW</li>
</ul>
</li>
</ol>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1559528609/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190603102300.png" alt=""></p>
<a id="more"></a>
<h2 id="2-DroNet"><a href="#2-DroNet" class="headerlink" title="2. DroNet"></a>2. DroNet</h2><p>Dronet的训练，将一个未处理的图片转换成两个高层的信息</p>
<p>(1) 估计障碍碰撞的概率，用来确定UAV的前向目标速度 (使用<a href="http://rpg.ifi.uzh.ch/dronet.html" target="_blank" rel="noopener">Z¨urich bicycle dataset</a>进行训练)</p>
<p>(2) 期望方向(根据障碍物、地上的白线等) (使用<a href="https://www.udacity.com/self-driving-car" target="_blank" rel="noopener">Udacity自动驾驶数据集</a>进行训练)</p>
<ol>
<li><p>网络结构</p>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1559184012/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190530103904.png" alt=""></p>
</li>
</ol>
<p>   网络输出的方向角与障碍概率，直接影响到控制决策上，通过一个低通滤波器影响目标偏航角变化率，以及目标前向速度。</p>
<ol>
<li><p>计算平台 GAP8 SoC</p>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1559188421/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190530115329.png" alt=""></p>
<ul>
<li>基于PULP开源架构， RISC-V精简指令集</li>
<li>两个子系统，一个供电模块<ul>
<li><strong>Fabric controller结构控制器</strong>，带一个RISC-V精简指令集的on-SoC微控制器<ul>
<li>FC的结构跟微控制器相同，一个内部时钟，512kB SRAM(L2 Memory)，一个用于boot的ROM，一个适用于多种接口的IO子系统($\mu$DMA)</li>
<li>L2 Memory是与cpu独立的不会被cpu中断。</li>
</ul>
</li>
<li>8核加速器簇(Cluster(<strong>CL</strong>))(RISC-V指令集)<ul>
<li>8核共享一个单独的指令缓存cache</li>
<li>8核共享一个64kB的L1暂存器</li>
<li>从L2 Memory到后端的数据移动由CL内部的DMA软件进行控制</li>
</ul>
</li>
<li>所有的core都支持RV32IMC指令集，和SIMD DSP扩展(SIMD是一个512B的单指令多数据向量)，能够实现基础的点积(加速线性代数计算及信号处理过程)。</li>
</ul>
</li>
<li>这种架构能够在具有规则的、可预测的数据访问模式的典型并行内核上实现最大的效率和利用率，同时节省共享数据缓存的区域开销。</li>
</ul>
</li>
</ol>
<ol>
<li><p>在嵌入式上应用算法</p>
<p>​    首先，导航算法必须能够以足以实现令人满意的闭环控制性能的帧速率执行主要工作负荷（一次解算需要的乘法累积操作约4100万次）。此外，虽然嵌入式处理器通常使用较低的精度来表示数据和较低分辨率的图像，但结果的质量必须保持与原始算法相似。这些限制对原始算法进行了很多调整，在无人机的情况下，可以分为两大类。</p>
<p>(1) 数据集fine-tuning 并将网络离散分层(network quantization)</p>
<ul>
<li>使用一个grayscale QVGA-resolution HiMax摄像头，采集数据，1122 张用于训练的新图片 以及228 张用于test/validation的新图片，将其丢到开源的碰撞数据集中去。</li>
<li>将3x3卷积换成2x2的</li>
<li>将浮点换成定点表示</li>
<li>通过测试，中间的feature map的动态阈值范围在+-16，精度2^-11，足够用来表示BN+激活函数后的值。</li>
<li>将所有的Relu使用16位 Q5.11定点表示（整数5位带符号，11位小数定点）。</li>
<li>网络重新训练，架构不变。</li>
</ul>
<p>(2) BN 折叠</p>
<ul>
<li><p>BN过程是线性的，可以将其计算合并到卷积层中去，具体是将BN折叠到卷积层权重W和偏置b中去，假设$\gamma, \beta,\sigma ,\mu$是normalization的参数，则有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
BN(W*x+b)&=\frac{\gamma}{\sigma}\cdot(W*x+b-\mu)+\beta \\
&=(\frac{\gamma}{\sigma} \cdot W)*x+(\beta+\frac{\gamma}{\sigma}(b-u)) \\
&=W'*x+b'
\end{aligned}</script></li>
<li><p>DroNet中主枝上的Res block是经过BN的，旁支的没有经过BN，因而还需要对上式做调整。先当做每一个res block都进行了BN，然后对旁支上的res block用下式进行反BN:</p>
<script type="math/tex; mode=display">
BN^{-1}(W'*x+b')=W''*x+b'' \\
W''=\frac{\sigma}{\gamma}\cdot W' \\
b'' =  b'+\sum_{ic}(\mu \sum_{fs}{W'}) - \sum_{ic}{\beta\cdot\frac{\sigma}{\gamma} \cdot\sum_{fs}{W'}}</script><p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1559228447/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190530225934.png" alt=""></p>
</li>
<li><p>最终的DroNet权重使用定点Q2.14， 除了第一层的分支层使用Q9.7</p>
</li>
</ul>
<h2 id="3-PULP-Shield"><a href="#3-PULP-Shield" class="headerlink" title="3. PULP-Shield"></a>3. PULP-Shield</h2><p>pulpshield：</p>
<ul>
<li>板载视觉导航引擎</li>
<li>可以兼容Crazyflie 2.0 (CF) nano-quadrotor （10cm直径，27g，支持最大15g载重）</li>
<li>其核心基于GAP8 SoC</li>
<li>两个Cypress HyperBus Memories，使其能灵活配置一个甚低功耗的 gray-scale HiMax QVGA CMOS image sensor, 通过平行相机接口协议(parallel camera interface PCI)进行交互。</li>
<li>在两个BGA memory槽上，挂载一个64 Mbit HyperRAM (DRAM) chip 和一个 128 Mbit HyperFlash memory</li>
</ul>
</li>
</ol>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1559285729/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190531145449.png" alt=""></p>
]]></content>
      <tags>
        <tag>平台</tag>
      </tags>
  </entry>
  <entry>
    <title>[论文](DAC) DISCRIMINATOR-ACTOR-CRITIC: ADDRESSING SAMPLE INEFFICIENCY AND REWARD BIAS IN ADVERSARIAL IMITATION LEARNING</title>
    <url>/post/4ed814e8.html</url>
    <content><![CDATA[<h2 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h2><h3 id="1-1-AIL存在的问题"><a href="#1-1-AIL存在的问题" class="headerlink" title="1.1 AIL存在的问题"></a>1.1 AIL存在的问题</h3><ul>
<li>bias reward，由于不正确的MDP吸收态导致的回报偏差</li>
<li>需要大量的交互，才能使得策略收敛</li>
<li>absorbing state（MDP吸收态）无法学习</li>
</ul>
<h3 id="1-2-本文所提方案"><a href="#1-2-本文所提方案" class="headerlink" title="1.2 本文所提方案"></a>1.2 本文所提方案</h3><ul>
<li>Discriminator-Actor-Critic (DAC)  兼容GAIL及AIRL框架，在二者原有框架基础上扩展 off-policy discriminator及 off-policy actor-critic 算法</li>
<li>改进：<ul>
<li>通过上述改进去除了AIL算法中由于不准确的吸收态导致的偏差(bias due to the incorrect absorbing state)</li>
<li>加速从demonstration中的学习速度(off policy扩展)</li>
<li>增加鲁棒性。</li>
</ul>
</li>
</ul>
<h2 id="二、方法"><a href="#二、方法" class="headerlink" title="二、方法"></a>二、方法</h2><h3 id="2-1-Bias-in-Reward"><a href="#2-1-Bias-in-Reward" class="headerlink" title="2.1 Bias in Reward"></a>2.1 Bias in Reward</h3><ul>
<li>Absorbing states in MDPs:  GAIL (GMMIL), Option GAN, AIRL，等这种AIL框架的算法，都忽略了absorbing state, 无法学习到吸收态的回报，所以导致吸收态的回报是0</li>
<li>一种常见的reward类型：$r(s,a)=-\log(1-D(s,a))​$，这种严格正值reward容易导致局部最优，而且agent都被这种positive reward带跑偏了，去追求更高的reward，而不是真正的去学习demonstration。</li>
<li>另一种常见的reward类型：$r(s,a)=\log(D(s,a))$ ,这种回报经常用在单步penalty的场景，加入一个固定的单步惩罚，这种的并不能很好的学到一个优秀的策略，事实上，这种强先验式回报即使不用模仿demonstration也可能获得一个好结果.</li>
</ul>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1556548109/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190429222819.png" alt=""></p>
<h3 id="2-2-Unbias-in-Reward"><a href="#2-2-Unbias-in-Reward" class="headerlink" title="2.2 Unbias in Reward"></a>2.2 Unbias in Reward</h3><p>作者要明确吸收态的回报$R_T=r(s_T,a_T) + \sum_{t=T+1}^{\infin}\gamma^{t-T}r(s_a,\cdot)$， 注意这里使用的是一个学到的回报$r(s_a,\cdot)$, 而非直接使用$R_T=r(s_T,a_T)$，这样将吸收态回报引入到学习过程，根据吸收态回报进行策略的学习。</p>
<p>为了能够兼容AIL以及RL框架，来兼顾吸收态进行学习，作者提出以下方案进行rollout：</p>
<p>完成一个episode的时候，将终态到吸收态的transition ($s_T,s_a$) 以及吸收态到吸收态的transition ($s_a,s_a​$) 一起作为transition。</p>
<script type="math/tex; mode=display">
Q(s_T,a)=r(s_T,a) + \gamma Q(s_a,\cdot) \\
Q(s_a,\cdot)=r(s_a,\cdot) + \gamma Q(s_a,\cdot)</script><p>实现的时候，需要一个标志位来标识其是否是吸收态。</p>
<h3 id="2-3-解决采样效率问题"><a href="#2-3-解决采样效率问题" class="headerlink" title="2.3 解决采样效率问题"></a>2.3 解决采样效率问题</h3><p>使用off-policy RL以及 off-policy Discriminator来改善GAIL。</p>
<ol>
<li><p>将从策略采样换成从replay buffer采样</p>
<script type="math/tex; mode=display">
\max_{D} \mathbb{E}_R[\log(D(s,a))]+\mathbb{E}_{\pi_E}[\log(1-D(s,a))]-\lambda H(\pi)</script><p>采样上使用重要性采样：</p>
<script type="math/tex; mode=display">
\max_{D} \mathbb{E}_R[\frac{p_{\pi_\theta}(s,a)}{p_R(s,a)}\log(D(s,a))]+\mathbb{E}_{\pi_E}[\log(1-D(s,a))]-\lambda H(\pi)</script><p>在实践过程中，重要性采样以及Discriminator的大方差，导致其训练效果不好，因而实践中省略掉了重要性采样权重。</p>
</li>
<li><p>TRPO 效果不如 PPO，这里使用了off-policy的TD3来替换on-policy，使用Discriminator的值作为回报进行训练。off-policy还可以应对multi modal情况，避免GAN带来的mode collapse问题。</p>
</li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>[论文]Crowd-Robot Interaction:Crowd-aware Robot Navigation with Attention-based Deep Reinforcement Learning</title>
    <url>/post/1edace88.html</url>
    <content><![CDATA[<h2 id="一、-概述"><a href="#一、-概述" class="headerlink" title="一、 概述"></a>一、 概述</h2><h3 id="1-1-解决的问题"><a href="#1-1-解决的问题" class="headerlink" title="1.1 解决的问题"></a>1.1 解决的问题</h3><p>​    Crowd-Robot Interaction (CRI)， <a href="https://github.com/vita-epfl/CrowdNav" target="_blank" rel="noopener">开源地址</a></p>
<h3 id="1-2-先前方法总结"><a href="#1-2-先前方法总结" class="headerlink" title="1.2 先前方法总结"></a>1.2 先前方法总结</h3><ol>
<li><p><strong>存在的问题</strong></p>
<ul>
<li>人群的集体影响通常是由成对相互作用的简化集合建模的，例如Maximin运算符或LSTM，它可能无法完全表示所有的相互作用。</li>
<li>大多数方法只关注从人到机器人的单向交互，而忽略了人群中可能间接影响机器人的交互。</li>
</ul>
</li>
<li><p><strong>已有的方法</strong></p>
<p>【第一类基于人工设计函数】</p>
<ul>
<li><p>Social Force </p>
</li>
<li><p>Interacting Gaussian Process (IGP)：</p>
<p>将每个agent的traj建模为一个独立的高斯过程，并提出一个相互作用势项来结合单个的高斯过程，进行相互作用。</p>
</li>
</ul>
<p>【第二类基于Imitation Learning】</p>
<ul>
<li>BC</li>
<li>IRL</li>
<li>GAIL</li>
</ul>
<p>【第三类基于强化学习】</p>
<a id="more"></a>
</li>
<li><p><strong>本文的方法</strong></p>
<ul>
<li>用self-attention mechanism重新思考人-机器人的配对互动</li>
<li>在强化学习框架中联合建模Human-Robot和Human-Human 交互</li>
</ul>
</li>
</ol>
<h3 id="1-3-问题的建模"><a href="#1-3-问题的建模" class="headerlink" title="1.3 问题的建模"></a>1.3 问题的建模</h3><p>robot从n个人的人群中穿越到目标点。</p>
<p>对于每一个agent(人和机器人)，已知：</p>
<p>【可相互观测】位置$p=[p_x,p_y]$，速度$v=[v_x,v_y]$，半径$r$</p>
<p>【不可观测】目标位置$p_g$，期望速度$v_{pref}$</p>
<p>【假设】robot的速度$v_t$，在执行action $a_t$ 后可以即时获得，$s_t$ 表示robot的状态，$w_t=[w_t^1,w_t^2…w_t^n]$表示人的状态，Human-Robot联合状态可以表示为：$s_t^{jn}=[s_t,w_t]$</p>
<p>【最优策略的定义】$\pi^*(s_t^{jn}):s_t^{jn}→a_t​$</p>
<script type="math/tex; mode=display">
\pi^*(s_t^{jn})=argmax_{a_t}R(s_t^{jn},a_t)+\gamma^{\Delta t \cdot v_{pref}} \int_{s_{t+\Delta t}^{jn}}P(s_t^{jn},a_t,s_{t+\Delta t}^{jn})V^*(s_{t+\Delta t }^{jn})ds_{t+\Delta t}^{jn}</script><script type="math/tex; mode=display">
V^*(s_t^{jn})=\sum_{t'=t}^T \gamma^{t'\cdot v_{pref}}R_{t'}(s_{t'}^{jn},\pi^*(s_{t'}^{jn}))</script><p>【回报定义】$d_t$为$[t-\Delta t,t]$之间的robot与human的最小间隔</p>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1555902069/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190422105816.png" alt=""></p>
<h3 id="1-4-价值网络训练"><a href="#1-4-价值网络训练" class="headerlink" title="1.4 价值网络训练"></a>1.4 价值网络训练</h3><p>基于TD、Replay buffer、target net。</p>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1555902502/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190422110340.png" alt=""></p>
<ul>
<li><p>1-3行使用demonstration experience 进行 imitation learning 初始化</p>
</li>
<li><p>4-14行 使用强化学习进行学习</p>
</li>
<li><p>7行的下一时刻状态，使用的是通过仿真获得的真实值，而非通过线性模型进行估计，减少动态模型误差带来的影响。</p>
</li>
<li>状态转移概率可以使用<a href="https://ieeexplore.ieee.org/document/7780479" target="_blank" rel="noopener">trajectory prediction model</a>进行估计</li>
</ul>
<p>价值网络模型需要精确地近似最优价值函数$V$，该函数隐式地encode了agent之间的social coorperation。以前的工作并没有模拟人群交互，这降低了对人口稠密场景的价值估计的准确性。</p>
<h2 id="二、-方法"><a href="#二、-方法" class="headerlink" title="二、 方法"></a>二、 方法</h2><h3 id="2-1-基本结构"><a href="#2-1-基本结构" class="headerlink" title="2.1 基本结构"></a>2.1 基本结构</h3><ul>
<li><p>Interaction module</p>
<p>对Human-Robot 交互进行显式建模，并通过粗粒度局部映射对Human-Robot进行encode。</p>
</li>
<li><p>Pooling module</p>
<p>通过self-attention制将相互作用聚合成固定长度的嵌入向量。</p>
</li>
<li><p>Planning module</p>
<p>估计robot和human联合状态对social navigation的value。</p>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1555903432/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190422112341.png" alt=""></p>
</li>
</ul>
<h3 id="2-2-详细模块说明"><a href="#2-2-详细模块说明" class="headerlink" title="2.2 详细模块说明"></a>2.2 详细模块说明</h3><h4 id="2-2-1-Parameterization"><a href="#2-2-1-Parameterization" class="headerlink" title="2.2.1 Parameterization"></a>2.2.1 Parameterization</h4><p>使用 robot-center parameterization。 以robot所在位置为中心，以当前位置到目标位置的向量方向为x轴。以此进行建模得到的robot及human的states如下：</p>
<script type="math/tex; mode=display">
s=[d_g,v_{pref},v_x,v_y,r]\\
w_i=[p_x,p_y,v_x,v_y,r_i,d_i,r_i+r]</script><p>其中 $d_g=||p-p_g||_2$ 表示robot到目标点的距离，$d_i=||p-p_i||_2$代表robot与neighbor $i$ 之间的距离。</p>
<h4 id="2-2-2-Interaction-Module"><a href="#2-2-2-Interaction-Module" class="headerlink" title="2.2.2 Interaction Module"></a>2.2.2 Interaction Module</h4><p>如果考虑相互间的交互，那么算法的时间复杂度较高是$O(n^2)$的，本文使用local map 表示粗粒度的human-human的交互。</p>
<ol>
<li><p>构建local map 特征</p>
<p>假设neighbor大小为$L$, 那么以每个human $i$ 为中心构建一个$L×L×3$大小的 tensor $M_i$， 来encode neighbor的位置及速度特征。</p>
</li>
<li><p>构建 embedded encode</p>
<p>使用human的state， map tensor以及 robot state 构建embedded vector $e_i$ (固定长度)，这里用一个MLP</p>
</li>
</ol>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1555904323/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190422113834.png" alt=""></p>
<h4 id="2-2-3"><a href="#2-2-3" class="headerlink" title="2.2.3"></a>2.2.3</h4><p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1555904815/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190422114638.png" alt=""></p>
]]></content>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>[论文] Go-Explore</title>
    <url>/post/b1fd4278.html</url>
    <content><![CDATA[<p>​    该论文的思路主要是考虑使用确定性的环境进行训练找到一个可行解，然后使用随机探索的测试，通过鲁棒化操作让随机探索更加可靠。目前尚未进行复现，但是关于这篇论文论坛上的争议很大，其实验结果可靠性有待验证，不过是一个很有趣的思路，文中说这是一个框架性的研究，许多细节可以进行深入挖掘。论文中确定性训练部分并没有使用神经网络，与传统的RL的做法有所不同，其开源代码与具体的环境交联很大，需要一定时间消化一下。</p>
<p><img src="https://www.alexirpan.com/public/go-explore/downscale.gif" alt="Downscaling"></p>
<h2 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h2><h4 id="1-1-Hard-Exploration-Problems的定义"><a href="#1-1-Hard-Exploration-Problems的定义" class="headerlink" title="1.1 Hard-Exploration Problems的定义"></a>1.1 Hard-Exploration Problems的定义</h4><ol>
<li>稀疏回报(sparse)</li>
<li>误导回报(deceptive)</li>
</ol>
<h4 id="1-2-之前的解决方法存在的问题"><a href="#1-2-之前的解决方法存在的问题" class="headerlink" title="1.2 之前的解决方法存在的问题"></a>1.2 之前的解决方法存在的问题</h4><p>​    之前的方法一般是采用内部激励(Intrinsic Motivation[IM])给智能体提供一些内在回报intrinsic rewards(IRs)，去激励他们去探索。常用的手段有curiosity，novelty-seeking。但是其表现并不是很好，作者给出了表现不好的两个原因<strong>detachment</strong> 和 <strong>derailment</strong></p>
<a id="more"></a>
<p><strong>Detachment:</strong> </p>
<ul>
<li>Detachment是指由IM驱动的agent可能脱离高内在奖励（IR）的边界；</li>
<li>假设内在奖励是一种消耗性资源，agent会在一个内在奖励高的区域集中消耗内在奖励，当其消耗殆尽很难重新发现agent在初始区域中的分离边界，由于灾难性遗忘(<a href="https://arxiv.org/abs/1705.07241" target="_blank" rel="noopener">catastrophic forgetting</a>) agent 可能不会记得如何回到那个边界；</li>
<li>每次这个过程发生时，潜在的探索途径都可能会丢失，或者至少很难重新发现。在最坏的情况下，当前policy所访问的状态空间区域附近可能缺乏剩余的IR（即使其他地方可能存在大量的IR），因此没有任何学习信号可以指导agent进一步有效探索的方式；</li>
<li>随着时间的推移，可以慢慢地增加内在奖励，但是整个无效的过程可以无限重复；</li>
<li>理论上讲， replay buffer 可以防止 detachment，但实际上它必须很大，以防止有关abandoned边界的数据在需要之前不被清除，大的 replay buffer 引入它们自己的优化稳定性困难(参考<a href="https://arxiv.org/abs/1712.01275" target="_blank" rel="noopener">sutton的分析</a>)；</li>
<li>Go-Explore算法通过显式存储所访问的有希望状态的存档来解决detachment的问题，以便随后可以重新访问和探索它们。</li>
</ul>
<p><strong>Derailment:</strong></p>
<ul>
<li><p>当agent发现了一个有希望的状态(promising state)时，可能会发生偏离（Derailment），返回该状态并从中进行探索是有益的；</p>
</li>
<li><p>典型的RL算法试图通过运行可以回到初始状态的策略来实现这种期望的行为，并对现有策略的加入一些随机扰动以鼓励其产生略微不同的行为（例如，进一步探索 exploring further）；</p>
</li>
<li><p>加入随机扰动的原因是因为IM agetn有两层探索机制</p>
<ul>
<li>在达到新状态时奖励的高层IR激励；</li>
<li>更基本的探索机制，如epsilon-greedy探索，动作空间噪声或参数空间噪声；</li>
</ul>
<p>IM agent依赖第二种探索机制进行高IR状态的探索，而依赖于第一种机制回到高IR状态；</p>
</li>
<li><p>然而，为了达到先前发现的高IR状态，需要一系列更长，更复杂和更精确的动作，这种随机扰动将更有可能“破坏(Derail)”agent不再返回那个状态。这是因为所需的精确动作被basic探索机制天真地扰乱，导致agent很少成功地达到它所记录的已知状态，并且进一步探索可能是最有效的；</p>
</li>
<li><p>为了解决Derailment问题，Go-Explore的一个见解是，在进一步探索之前，有效的探索可以分解为第一次回到有希望的状态（无需有意添加任何探索）。</p>
</li>
</ul>
<h4 id="1-3-Go-Explore概述"><a href="#1-3-Go-Explore概述" class="headerlink" title="1.3 Go-Explore概述"></a>1.3 Go-Explore概述</h4><ul>
<li><p><strong>方案优势</strong></p>
<p>解决了一下detachment和derailment的问题，并且提出了在随机环境下的鲁棒的解决方案。</p>
</li>
<li><p><strong>方案概述：</strong></p>
<ul>
<li><strong>phase1</strong>: 首先用一个比较简单的方法，用一个确定性的方法来解决问题，即发现如何解决问题；</li>
<li><strong>phase2</strong>: 然后进行鲁棒化（即训练在存在随机性的情况下能够可靠地执行的解决方案）。</li>
</ul>
</li>
</ul>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1555383642/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190416105539.png" alt=""></p>
<p>​    <strong>【phase1】</strong> 寻找少见的state形成解决系数回报或者误导回报问题的基本解决方案，phase1 使用两种策略，通过记录状态和到达状态的方式来解决detachment还有derailment的问题：</p>
<p>​    (1) 将到目前为止所有访问过的不同的状态存下来</p>
<p>​    (2) 每次选择存储池（archive）中选择一个状态进行探索 ，</p>
<p>​    首先【Go back】回到这个状态(不加任何随机探索)，</p>
<p>​    然后【Explore】从这个状态开始进行探索新的state。因而算法框架称为<strong>Go-Explore</strong>。</p>
<p>​    作者举搜索房屋的例子说明了phase1和IM探索的区别：<br>​    IM算法类似于在房子中搜索一个射灯，它在房子的一个区域中投射出一束狭窄的探测光束，然后是另一个区域，另一个区域，以及另一个区域，等等，随着光被吸引到其非常小的可见区域边缘的IM区域。如果在任何一点上光束不能落在任何具有IM的区域上，它就会迷失。 Go-Explore更像是在房子的一个房间，然后是相邻的房间，然后是相邻的房间等等，直到整个房子都被照亮。因此，Go-Explore将逐步扩展其所有方向的知识范围，直到找到解决方案。</p>
<p>​    <strong>【phase2】</strong> 通过imitation learning的方式进行鲁棒化，与正常的模仿学习的方式不同，Go-Expolore的方法使用的不是人工demonstration而是由phase1生成的demonstration。该阶段的输入是一个或多个回报较高的trajectory，输出是一个能够达到相似表现能力的鲁棒的策略。</p>
<h2 id="二、-Go-Explore-算法"><a href="#二、-Go-Explore-算法" class="headerlink" title="二、 Go-Explore 算法"></a>二、 Go-Explore 算法</h2><p>​    算法的主要思路是围绕着如何可靠的记录并且回到promising state。以此为思路可以扩展很多算法。</p>
<h4 id="2-1-Phase1-探索直到问题解决"><a href="#2-1-Phase1-探索直到问题解决" class="headerlink" title="2.1 Phase1 探索直到问题解决"></a>2.1 Phase1 探索直到问题解决</h4><p>​    Phase1 存储不同的interesting states，论文中称为”Cell”, 并且存储能够到达这些状态的trajectory。</p>
<p>开始的时候只包含初始状态，然后不断重复以下步骤：</p>
<p>​    (1) 从当前archive中选择一个cell</p>
<p>​    (2) 回到cell的状态，并且不加任何随机扰动</p>
<p>​    (3) 从cell状态进行随机探索</p>
<p>​    (4) 新出现的cell和到达cell的trajectory将加入archive中</p>
<h5 id="2-1-1-Cell-表示"><a href="#2-1-1-Cell-表示" class="headerlink" title="2.1.1 Cell 表示"></a>2.1.1 Cell 表示</h5><p>​    理论上，人们可以直接在高维状态空间（其中每个Cell正好包含一个状态）中进行run-go探索；然而，这样做在实践中是很难做到的。为了能够在像Atari这样的高维状态空间中进行处理，Go Explore的第一阶段需要一个较低维的空间来进行搜索（尽管最终策略仍将在相同的原始状态空间中发挥作用，在本例中是像素）。因此，单元表示应该合并相似的状态，而不是合并有意义的不同的状态。</p>
<p>​    这样，一个好的Cell表示应该将观测的维数减少到一个有意义的低维空间。丰富的文献研究如何从像素获得良好的表示。一种选择是从传统的RL算法训练的神经网络中间提取latent code，最大化外在和/或内在motivation，可选地添加辅助任务，如预测奖励。其他选项包括无监督技术，如自动编码或预测未来状态的网络，以及其他辅助任务，如像素控制。</p>
<p>​    在未来的工作中使用Go-Explore测试这些技术很有趣，对于这些使用Go-Explore的初始实验，我们使用两种不同的表示来测试其性能：一种是不利用对特定游戏的领域知识，另一种则利用易于提供的领域知识。</p>
<ul>
<li><p>不使用domain knowledge</p>
<p>在蒙特祖玛的复仇里面，论文采用的是（1）将每帧的图像转为灰度图（2）使用线性插值将灰度图downscale为11x8（3）将像素密度由0~255缩放到0~8之间。</p>
</li>
<li><p>使用domain knowledge</p>
<p>​    在蒙特祖玛的复仇中，领域知识表示为：agent的xy位置(使用16x16网格离散化)， 房间数目，level数，找到钥匙的房间等。</p>
<p>​    在Pitfall中，领域知识表示为：agent的xy位置，以及房间数</p>
<p>​    所有这些信息都是用简单的手工编码分类器直接从像素中提取出来的，以检测诸如主要角色的位置等对象，并结合我们对两个游戏中地图结构的了解。虽然Go Explore提供了在第一阶段的cell表示中利用领域知识的机会，但是第二阶段产生的机器人化神经网络仍然只从像素直接播放。</p>
</li>
</ul>
<h5 id="2-1-2-选择Cell"><a href="#2-1-2-选择Cell" class="headerlink" title="2.1.2 选择Cell"></a>2.1.2 选择Cell</h5><p>​    在第一阶段的每一次迭代中，都会从archive中选择一个Cell进行探索。这种选择可以使用均匀分布随机地进行，但在许多情况下，我们可以通过创建（或学习）一个启发式的方法来改进基线，使某些Cell优于其他Cell。在初步实验中，我们发现这种启发式方法可以提高均匀随机抽样的性能。具体的启发式定义根据所解决的问题而不同，但在较高的层次上，论文中的启发式为每个被认为更promising的Cell分配一个更高的正权重。例如，由于没有经常被访问，所以这个Cell状态是首选的，这有助于发现一个新的Cell，或期望可以接近未发现的Cell。将所有Cell的权重归一化，以表示下一个选择的每个Cell的概率。没有一个Cell的权重等于0，所以原则上所有单元都可以继续进行进一步的探索。(关于启发式算法细节在后文有详细说明)</p>
<h5 id="2-1-3-利用确定性的Simulator返回到Cell状态"><a href="#2-1-3-利用确定性的Simulator返回到Cell状态" class="headerlink" title="2.1.3 利用确定性的Simulator返回到Cell状态"></a>2.1.3 利用确定性的Simulator返回到Cell状态</h5><p>​    Go-Explore的一个主要原则是，在从一个Cell进行探索之前，先返回一个Promising cell，而不需要进行额外的探索。Go-Explore的哲学是，考虑到问题的限制，我们应该尽可能容易地回到那个Cell。返回到一个Cell的最简单的方法是，如果世界是确定的和可重置的，这样就可以将Simulator的状态重置为以前访问该Cell的状态。</p>
<p>​    这里作者提出了两个思路：一个是仅在测试的时候允许随机，第二个是在训练和测试的时候都可以随机。</p>
<ul>
<li><strong>仅在测试的时候允许随机</strong></li>
</ul>
<p>​    由于当前的RL算法可以采取unsafe的action并需要大量的经验来学习，因此在可预见的将来，RL的大部分应用可能需要在模拟器中进行训练，然后才能转移到现实世界（并可选择在现实世界中进行微调）。例如，在将解决方案转移到现实世界之前，大多数机器人学习算法在模拟器中训练；这是因为直接在机器人上学习速度慢、采样效率低、可能会损坏机器人，并且可能不安全。幸运的是，对于许多领域，模拟器都是可用的（例如，机器人模拟器、交通模拟器等）。Go-Explore的一个观点是，我们可以利用这样一个事实，即可以使模拟器具有确定性来提高性能，特别是在困难的探索问题上。对于许多类型的问题，我们需要一个可靠的最终解决方案（例如，一个在自然灾害后能够可靠地找到幸存者的机器人），并且没有理论性的理由去关心我们是否通过最初的确定性训练获得这个解决方案。如果我们能够解决以前无法解决的问题，包括在评估（测试）时是随机的问题，通过使模拟器具有确定性，我们应该利用这个机会。</p>
<ul>
<li><strong>在训练过程中也需要随机</strong></li>
</ul>
<p>​    由于某些情况下无法提供仿真环境，导致算法必须在training的时候使用随机探索。对于这种情况Go-Explore算法也可以应对，其通过训练 goal-conditioned 策略(HER, Universal value function approximators )，该策略能够在探索阶段可靠地返回到archive中的cell，这是一个值得研究的方向。虽然计算成本要高得多，但这种策略会在探索阶段结束时产生完全训练有素的policy，这意味着最终不需要鲁棒化阶段。这里存在一些问题，其中环境具有随机性的形式，阻止算法可靠地返回到特定cell，无论agent采取何种action（例如，在扑克中，没有可靠的行动顺序导致您进入你有两个ace的状态）。</p>
<p>​    考虑到上述的两种模式，我们现在可以问蒙特祖玛的复仇与陷阱是否代表第一类（我们关心的所有都是对测试时随机性具有鲁棒性的解决方案）或第二类领域（算法必须处理训练时随机性的情况）。本文中的所有结果和声明均适用于在训练期间不需要随机性的版本（即仅在评估期间需要随机性）。在训练是随机的时候应用Go-Explore仍然是不久的将来一个令人兴奋的研究途径。</p>
<p>​    论文在实验过程中，并没存储到cell的序列，只存了cell可以提高运算效率</p>
<h5 id="2-1-4-从Cell状态开始探索"><a href="#2-1-4-从Cell状态开始探索" class="headerlink" title="2.1.4 从Cell状态开始探索"></a>2.1.4 从Cell状态开始探索</h5><p>​    到达Cell状态后，可以应用任何探索方法来寻找新Cell。在这项工作中，agent通过对$k = 100$个训练帧进行随机动作进行探索，在每个训练帧中重复前一个动作的概率为95％（允许agent采取行动的帧，因此不包括任何跳过的帧由于frame skip）。除了达到$k = 100$训练框架的探索限制之外，探索也会在episode结束时中止，导致episode结束的动作将被忽略，因为它不会产生目标Cell。</p>
<p>​    有趣的是，这种探索不需要神经网络或其他控制器，实际上在本文的任何实验中都没有使用神经网络进行探索阶段（阶段1）（直到第2阶段才开始训练神经网络）。完全随机探索如此有效的事实凸显了在进一步探索之前简单地回到promising Cell的惊人力量，尽管我们认为智能地探索（例如通过训练有素的政策）可能会改善我们的结果并且是未来的研究方向。</p>
<h5 id="2-1-5-更新存储archive"><a href="#2-1-5-更新存储archive" class="headerlink" title="2.1.5 更新存储archive"></a>2.1.5 更新存储archive</h5><p>​    更新的两种情况：</p>
<ol>
<li><p>agent访问了没在archive中出现过的cell</p>
<p>此时需要存储agent如何到达这个cell 的整个trajectory，当前cell的状态，trajectory的累积回报，trajectory的长度。</p>
</li>
<li><p>新的能到达已存在于archive中cell的trajectory 比之前的trajectory好</p>
<p>好的定义：new trajectory 的分数较高，同等分数时new trajectory的长度较短</p>
</li>
</ol>
<p>​    此外，将重置影响选择该Cell可能性的信息，包括选择该Cell的总次数和从发现另一个Cell以来选择该Cell的次数。当Cell合并了许多不同的状态时，重置这些值是有益的，因为到达Cell的新方法实际上可能是一个更有前景的垫脚石（因此我们希望鼓励选择它）。我们不重置记录访问Cell次数的计数器，因为这将使最近发现的Cell与最近更新的Cell不可区分，并且最近发现的Cell（即访问计数较低的Cell）更有希望进行探索，因为它们很可能接近我们不断扩大的知识领域的表面。</p>
<p>​    因为细胞合并了许多状态，所以我们不能保证，如果用一种不同的、更好的方式从A到B，从起始状态 A到 Cell B到 Cell C的trajectory仍然会到达 C；因此，到达Cell的更好的方法并没有被整合到建立在原始轨道上的其他Cell的轨道中。这里还有待研究。</p>
<h4 id="2-2-Phase2-鲁棒化"><a href="#2-2-Phase2-鲁棒化" class="headerlink" title="2.2 Phase2: 鲁棒化"></a>2.2 Phase2: 鲁棒化</h4><p>​    如果成功，阶段1的结果是一个或多个高性能的trajectory。然而，如果Go探索的第一阶段在模拟器中利用了确定性，那么这样的轨迹对于测试时出现的任何随机性都不具有鲁棒性。第2阶段通过imitation learning 创建一个对噪音具有鲁棒性的策略来解决这一差距。重要的是，在第2阶段中增加了随机性，以便最终策略对其在测试环境中评估时所面临的随机性具有鲁棒性。因此，正在training的policy必须学习如何模拟和/或执行，以及从Go-Explore勘探阶段获得的轨迹，同时处理原始轨迹中不存在的情况。根据环境的随机性，这种调整可能具有很高的挑战性，但是比起从头开始尝试解决稀疏的奖励问题要容易得多。</p>
<p>​    虽然大多数模仿学习算法可以用于第2阶段，但不同类型的模仿学习算法可以定性地影响最终的策略。试图接近模拟演示行为的LFD算法可能难以改进。为此，我们选择了一个已经证明可以改进的LFD算法：Salimans和Chen提出的反向算法。它的工作原理是在轨迹中的最后一个状态附近启动agent，然后从那里运行一个普通的RL算法（ppo）。一旦算法学会从接近轨迹末端的起始位置获得与示例轨迹相同或更高的奖励，该算法将代理的起始点沿轨迹备份到稍早的位置，并重复该过程，直到最终代理学会获得大于或等于从初始状态开始的整个示例轨迹。请注意，Resnick等人在大约同一时间独立发现了类似的算法。</p>
<p>​    虽然这种机器人化方法有效地将专家轨迹视为代理的课程，但该策略只进行优化，以最大限度地提高自己的分数，而不是被迫准确地模拟轨迹。由于这个原因，这个阶段能够进一步优化专家的轨迹，并在它们之外进行归纳，这两个都是我们在实验中实际观察到的（第3节）。除了寻求比原始轨迹更高的分数之外，由于它是一种带有折扣因子的RL算法，因此对短期奖励的奖励比之后收集的奖励多，因此它还具有提高奖励收集效率的压力。因此，如果原始轨迹包含不必要的动作（如访问死角和返回），那么在机器人化过程中可以消除这种行为（我们也观察到这种现象）。</p>
<h3 id="三、-关于本论文的争议"><a href="#三、-关于本论文的争议" class="headerlink" title="三、 关于本论文的争议"></a>三、 关于本论文的争议</h3><p><a href="https://www.alexirpan.com/2018/11/27/go-explore.html" target="_blank" rel="noopener">Quick Opinions on Go-Explore</a></p>
]]></content>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>tensorflow常用基础变换</title>
    <url>/post/d4f958cd.html</url>
    <content><![CDATA[<h3 id="tf-slice"><a href="#tf-slice" class="headerlink" title="tf.slice"></a>tf.slice</h3><ul>
<li><p>函数原型 tf.slice(inputs,begin,size,name=’’)</p>
</li>
<li><p>用途：从inputs中抽取部分内容</p>
<p>inputs：可以是list,array,tensor<br>begin：n维列表，begin[i] 表示从inputs中第i维抽取数据时，相对0的起始偏移量，也就是从第i维的begin[i]开始抽取数据<br>size：n维列表，size[i]表示要抽取的第i维元素的数目<br>有几个关系式如下:<br>（1） i in [0,n]<br>（2）tf.shape(inputs)[0]=len(begin)=len(size)<br>（3）begin[i]&gt;=0   抽取第i维元素的起始位置要大于等于0<br>（4）begin[i]+size[i]&lt;=tf.shape(inputs)[i]</p>
<a id="more"></a>
</li>
</ul>
<p>代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]]</span><br><span class="line">y = np.arange(<span class="number">24</span>).reshape([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">z = tf.constant([[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]], [[<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]], [[<span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>], [<span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>]]])</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">begin_x = [<span class="number">1</span>, <span class="number">0</span>]  <span class="comment"># 第一个1，决定了从x的第二行[4,5,6]开始，第二个0，决定了从[4,5,6] 中的4开始抽取</span></span><br><span class="line">size_x = [<span class="number">1</span>, <span class="number">2</span>]  <span class="comment"># 第一个1决定了，从第二行以起始位置抽取1行，也就是只抽取[4,5,6] 这一行，在这一行中从4开始抽取2个元素</span></span><br><span class="line"></span><br><span class="line">out = tf.slice(x, begin=begin_x, size=size_x)</span><br><span class="line"><span class="keyword">print</span> (sess.run(out) ) <span class="comment"># 结果:[[4 5]]</span></span><br><span class="line"></span><br><span class="line">begin_y = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">size_y = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">out = tf.slice(y, begin_y, size_y)</span><br><span class="line">print(sess.run(out)  )<span class="comment"># 结果:[[[12 13 14] [16 17 18]]]</span></span><br><span class="line"></span><br><span class="line">begin_z = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">size_z = [<span class="number">-1</span>, <span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">out = tf.slice(z, begin_z, size_z)</span><br><span class="line">print(sess.run(out))  <span class="comment"># size[i]=-1 表示第i维从begin[i]剩余的元素都要被抽取，结果：[[[ 5  6]] [[11 12]] [[17 18]]]</span></span><br></pre></td></tr></table></figure>
<h3 id="tf-transpose"><a href="#tf-transpose" class="headerlink" title="tf.transpose"></a>tf.transpose</h3><p>主要注意多维的时候perm的用法，其实很简单，就是正常perm顺序是[0,1,2]，如果原来的shape是2x3x4的，对应的新perm顺序是多少，就把乘数的位置换到多少，也就是perm变成[1,0,2]那么shape就是3x2x4</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"> </span><br><span class="line"><span class="comment">#x = tf.constant([[1, 2 ,3],[4, 5, 6]])</span></span><br><span class="line">x = [[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>],[<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>]],[[<span class="number">21</span>,<span class="number">22</span>,<span class="number">23</span>,<span class="number">24</span>],[<span class="number">25</span>,<span class="number">26</span>,<span class="number">27</span>,<span class="number">28</span>],[<span class="number">29</span>,<span class="number">30</span>,<span class="number">31</span>,<span class="number">32</span>]]]</span><br><span class="line"><span class="comment">#a=tf.constant(x)</span></span><br><span class="line">a=tf.transpose(x, [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">b=tf.transpose(x, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line">c=tf.transpose(x, [<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line">d=tf.transpose(x, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>])</span><br><span class="line">e=tf.transpose(x, [<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line">f=tf.transpose(x, [<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">output:</span><br><span class="line">--------------- a: 2 x 3*4</span><br><span class="line">[[[ 1  2  3  4]</span><br><span class="line">  [ 5  6  7  8]</span><br><span class="line">  [ 9 10 11 12]]</span><br><span class="line"> </span><br><span class="line"> [[21 22 23 24]</span><br><span class="line">  [25 26 27 28]</span><br><span class="line">  [29 30 31 32]]]</span><br><span class="line">--------------- b: 2 x 4*3</span><br><span class="line">[[[ 1  5  9]</span><br><span class="line">  [ 2  6 10]</span><br><span class="line">  [ 3  7 11]</span><br><span class="line">  [ 4  8 12]]</span><br><span class="line"> </span><br><span class="line"> [[21 25 29]</span><br><span class="line">  [22 26 30]</span><br><span class="line">  [23 27 31]</span><br><span class="line">  [24 28 32]]]</span><br><span class="line">--------------- c: 3 x 2*4</span><br><span class="line">[[[ 1  2  3  4]</span><br><span class="line">  [21 22 23 24]]</span><br><span class="line"> </span><br><span class="line"> [[ 5  6  7  8]</span><br><span class="line">  [25 26 27 28]]</span><br><span class="line"> </span><br><span class="line"> [[ 9 10 11 12]</span><br><span class="line">  [29 30 31 32]]]</span><br><span class="line">--------------- d: 3 x 4*2</span><br><span class="line">[[[ 1 21]</span><br><span class="line">  [ 2 22]</span><br><span class="line">  [ 3 23]</span><br><span class="line">  [ 4 24]]</span><br><span class="line"> </span><br><span class="line"> [[ 5 25]</span><br><span class="line">  [ 6 26]</span><br><span class="line">  [ 7 27]</span><br><span class="line">  [ 8 28]]</span><br><span class="line"> </span><br><span class="line"> [[ 9 29]</span><br><span class="line">  [10 30]</span><br><span class="line">  [11 31]</span><br><span class="line">  [12 32]]]</span><br><span class="line">--------------- e: 4 x 3*2</span><br><span class="line">[[[ 1 21]</span><br><span class="line">  [ 5 25]</span><br><span class="line">  [ 9 29]]</span><br><span class="line"> </span><br><span class="line"> [[ 2 22]</span><br><span class="line">  [ 6 26]</span><br><span class="line">  [10 30]]</span><br><span class="line"> </span><br><span class="line"> [[ 3 23]</span><br><span class="line">  [ 7 27]</span><br><span class="line">  [11 31]]</span><br><span class="line"> </span><br><span class="line"> [[ 4 24]</span><br><span class="line">  [ 8 28]</span><br><span class="line">  [12 32]]]</span><br><span class="line">---------------f: 4 x 2*3</span><br><span class="line">[[[ 1  5  9]</span><br><span class="line">  [21 25 29]]</span><br><span class="line"> </span><br><span class="line"> [[ 2  6 10]</span><br><span class="line">  [22 26 30]]</span><br><span class="line"> </span><br><span class="line"> [[ 3  7 11]</span><br><span class="line">  [23 27 31]]</span><br><span class="line"> </span><br><span class="line"> [[ 4  8 12]</span><br><span class="line">  [24 28 32]]]</span><br><span class="line">---------------</span><br></pre></td></tr></table></figure>
<h3 id="tf-nn-conv1d"><a href="#tf-nn-conv1d" class="headerlink" title="tf.nn.conv1d"></a>tf.nn.conv1d</h3><p> conv1d的参数含义：(以NHWC格式为例，即，通道维在最后)</p>
<ul>
<li><p>value：在注释中，value的格式为：[batch, in_width, in_channels]，batch为样本维，表示多少个样本，in_width为宽度维，表示样本的宽度，in_channels维通道维，表示样本有多少个通道。 </p>
</li>
<li><p>filters：在注释中，filters的格式为：[filter_width, in_channels, out_channels]。按照value的第二种看法，filter_width可以看作每次与value进行卷积的行数，in_channels表示value一共有多少列（与value中的in_channels相对应）。out_channels表示输出通道，可以理解为一共有多少个卷积核，即卷积核的数目。</p>
</li>
<li><p>stride：一个整数，表示步长，每次（向下）移动的距离</p>
</li>
<li><p>padding：同conv2d，value是否需要在下方填补0。 <strong>Valid:</strong> 用过滤器在输入的矩阵中按步长移动时候，会把最后的不足部分的列和行抛弃；<strong>Same:</strong> 先在输入矩阵上下各加个值为0的行，在左右各加个个值为0的列，也就是用0把原先的矩阵包裹一层，然后在移动的时候如果输入矩阵的列或者行长度不够，就用0来补齐</p>
</li>
<li><p>name：名称。</p>
</li>
</ul>
<p>测试代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个矩阵a，表示需要被卷积的矩阵。</span></span><br><span class="line">a = np.array(np.arange(<span class="number">1</span>, <span class="number">1</span> + <span class="number">20</span>).reshape([<span class="number">1</span>, <span class="number">10</span>, <span class="number">2</span>]), dtype=np.float32)</span><br><span class="line">print(a, a.shape)</span><br><span class="line"><span class="comment"># 卷积核，此处卷积核的数目为1</span></span><br><span class="line">kernel = np.array(np.arange(<span class="number">1</span>, <span class="number">1</span> + <span class="number">4</span>), dtype=np.float32).reshape([<span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line">print(kernel, kernel.shape)</span><br><span class="line"><span class="comment"># 进行conv1d卷积</span></span><br><span class="line">conv1d = tf.nn.conv1d(a, kernel, <span class="number">1</span>, <span class="string">'VALID'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># 初始化</span></span><br><span class="line">    tf.global_variables_initializer().run()</span><br><span class="line">    <span class="comment"># 输出卷积值</span></span><br><span class="line">    out = sess.run(conv1d)</span><br><span class="line">    print(out, out.shape)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a: shape (1, 10, 2)</span><br><span class="line">a:</span><br><span class="line">[[[ 1.  2.]</span><br><span class="line">  [ 3.  4.]</span><br><span class="line">  [ 5.  6.]</span><br><span class="line">  [ 7.  8.]</span><br><span class="line">  [ 9. 10.]</span><br><span class="line">  [11. 12.]</span><br><span class="line">  [13. 14.]</span><br><span class="line">  [15. 16.]</span><br><span class="line">  [17. 18.]</span><br><span class="line">  [19. 20.]]]</span><br><span class="line">  </span><br><span class="line">filter: shape (2, 2, 1)</span><br><span class="line">filter:</span><br><span class="line">[[[1.]</span><br><span class="line">  [2.]]</span><br><span class="line"></span><br><span class="line"> [[3.]</span><br><span class="line">  [4.]]]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">output: shape(1, 9, 1)</span><br><span class="line">output:</span><br><span class="line">[[[ 30.]</span><br><span class="line">  [ 50.]</span><br><span class="line">  [ 70.]</span><br><span class="line">  [ 90.]</span><br><span class="line">  [110.]</span><br><span class="line">  [130.]</span><br><span class="line">  [150.]</span><br><span class="line">  [170.]</span><br><span class="line">  [190.]]]</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>Wasserstein GAN 与 WGAN-GP</title>
    <url>/post/64ddacfe.html</url>
    <content><![CDATA[<h3 id="一、fGAN使用JS散度存在的问题"><a href="#一、fGAN使用JS散度存在的问题" class="headerlink" title="一、fGAN使用JS散度存在的问题"></a>一、fGAN使用JS散度存在的问题</h3><p>当$P_G$分布与$P_{data}$分布没有重叠的时候，此时的JSD恒等于$log2$，这样导致$P_G$分布难以向$P_{data}$进行移动。</p>
<p>而大多数情况下，$P_G$与$P_{data}$是难以重叠的，其原因有两方面理解：</p>
<ol>
<li><p>原始数据角度</p>
<p>$P_{data}$与$P_G$和二维图像相似都属于高维空间的低维流形。因而其重叠的部分几乎可以被忽略(三维空间中的两个交叉曲面投影到二维中可能只有两个点是相交的)</p>
</li>
<li><p>采样角度</p>
<p>由于采样的数量较少，而二者重叠的部分又比较小，所以很容易采样出来的都不是重叠部分的，因而会产生不重叠的现象。</p>
</li>
</ol>
<h3 id="二、-WGAN"><a href="#二、-WGAN" class="headerlink" title="二、 WGAN"></a>二、 WGAN</h3><blockquote>
<p>参考：<a href="https://zhuanlan.zhihu.com/p/25071913?columnSlug=f00cb0979b57ab6d7f70e287b0cba55d" target="_blank" rel="noopener">令人拍案叫绝的Wasserstein GAN</a></p>
</blockquote>
<ol>
<li><p><strong>使用Wasserstein距离来替代JSD</strong></p>
<p>​    Wasserstein距离又称作是Earth Mover’s 距离，他的目的是考虑一个最小的代价，将$P$分布转换成近似$Q$分布，如果用条形图来表示的话就和搬土块差不多。</p>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1554973227/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190411165958.png" alt=""></p>
<p>​    这样就带来一个问题，如上图所示，搬土块的方式有很多，那么哪种方式代价最小？这里常用的方式就是穷举<strong>moving plans</strong>找到一个代价小的方式。</p>
<p>​    下图就是一个moving plan $\gamma$其中每个小方块代表从P到Q需要搬动多少土，越亮搬得越多，对于P和Q上的每一个Bar都代表着对应的一行或者一列的小方块的累和。那么<strong>Wasserstein距离</strong>对应的就是<strong>求解一个最优化问题，使得穷举出来的不同的moving plan代价最小</strong>。</p>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1554973438/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190411170338.png" alt=""></p>
<a id="more"></a>
</li>
<li><p><strong>WGAN</strong></p>
<p>那么将Wasserstein距离应用到GAN中，就需要通过$V(G,D)$来评估Wasserstein距离，将原来的JSD换掉。其形式为：</p>
</li>
</ol>
<script type="math/tex; mode=display">
V(G,D)=\max\limits_{D\in1-Lipschitz}\{\mathbb{E}_{x \sim P_{data}}[D(x)]-\mathbb{E}_{x\sim P_G}[D(x)]\}</script><p>​    其目标是让在$P_{data}$分布上的D值更大，让在$P_G$分布上的D值更小。</p>
<ol>
<li><p><strong>1-Lipschitz条件</strong></p>
<p>Lipschitz function的目的是约束Discriminator函数不要过于崎岖，应该趋于平滑。如果没有进行平滑性约束，D可能在$P_G$部分趋于负无穷，而在$P_{data}$部分趋于正无穷。</p>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1554975185/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190411173246.png" alt=""></p>
<p><strong>Lipschitz function:</strong></p>
<script type="math/tex; mode=display">
||f(x_1)-f(x_2)||≤K|||x_1-x_2||</script><p>当$K=1$的时候称为”1-Lipschitz”，其中不等式左边部分为输出变化率，右边部分为输入变化率，可以看出来该条件是将D的导数约束在-1~1之间。</p>
</li>
<li><p><strong>Weight Clipping</strong></p>
<p>​    这个是作者在发表论文的时候采取的一种比较粗暴的方式，而且并不能产生真正意义上的1-Lipschitz约束。对于$||f_w||_L \leq K​$这个限制。我们其实不关心具体的$K​$是多少，只要它不是正无穷就行，因为它只是会使得梯度变大$K​$倍，并不会影响梯度的方向。所以作者采取了一个非常简单的做法，就是限制神经网络$f_\omega​$的所有参数$w_i​$的不超过某个范围$[-c, c]​$，比如$w_i \in [- 0.01, 0.01]​$，此时关于输入样本$x​$的导数$\frac{\partial f_w}{\partial x}​$也不会超过某个范围，所以一定存在某个不知道的常数$K​$使得$f_w​$的局部变动幅度不会超过它，Lipschitz连续条件得以满足。具体在算法实现中，只需要每次更新完$w​$后把它clip回这个范围就可以了。</p>
</li>
</ol>
<h3 id="三、Improved-WGAN-WGAN-GP"><a href="#三、Improved-WGAN-WGAN-GP" class="headerlink" title="三、Improved WGAN (WGAN-GP)"></a>三、Improved WGAN (WGAN-GP)</h3><ol>
<li><p><strong>原理</strong> </p>
<p>先来看一下在WGAN-GP中是如何改进以满足1-Lipschitz条件的。由上面的Lipschitz function可以得到，满足约束条件的等价条件是</p>
</li>
</ol>
<script type="math/tex; mode=display">
||\nabla_xD(x)|| ≤ 1， \forall x</script><p>​    也就是说，我们可以通过在原有的V(G,D)上加一个惩罚项，使得其满足上述约束：</p>
<script type="math/tex; mode=display">
V(G,D)=\max_\limits{D}\{\mathbb{E}_{x \sim P_{data}}[D(x)]-\mathbb{E}_{x\sim P_G}[D(x)]\}-\lambda\int_x\max\{0,||\nabla_xD(x)||-1\}dx</script><p>​    由于上述条件说明对于$\forall x​$都满足，而难以实际采样到所有的样本，因而需要使用一个$P_{penalty}​$分布来进行采样。即将上式变成：</p>
<script type="math/tex; mode=display">
V(G,D)=\max_\limits{D}\{\mathbb{E}_{x \sim P_{data}}[D(x)]-\mathbb{E}_{x\sim P_G}[D(x)]\}-\lambda\mathbb{E}_{x\sim P_{penalty} }\max\{0,||\nabla_xD(x)||-1\}</script><p>​    在WGAN-GP中采用的$P_{penalty}$是通过$P_G$与$P_{data}$间的随机插值方式进行采样的：</p>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1554976290/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190411175107.png" alt=""></p>
<p>​    这种方式<strong>并不是一个绝对的方式</strong>，论文中说明该种采样方式是可以达到一定效果的。</p>
<ol>
<li><strong>形式改变</strong>：</li>
</ol>
<ul>
<li>判别器最后一层去掉sigmoid</li>
<li>生成器和判别器的loss不取log</li>
<li>每次更新判别器的参数之后把它们的绝对值截断到不超过一个固定常数c</li>
<li>不要用基于动量的优化算法（包括momentum和Adam），推荐RMSProp，SGD也行</li>
</ul>
<p>原始GAN的判别器做的是真假二分类任务，所以最后一层是sigmoid，但是现在WGAN中的判别器$f_w​$做的是近似拟合Wasserstein距离，属于回归任务，所以要把最后一层的sigmoid拿掉。</p>
<h3 id="四、WGAN-GP-GAN的稳定性"><a href="#四、WGAN-GP-GAN的稳定性" class="headerlink" title="四、WGAN-GP(GAN的稳定性)"></a>四、WGAN-GP(GAN的稳定性)</h3><p><strong>WGAN梯度裁剪的问题</strong></p>
<p>梯度裁剪会导致最优化困难。在梯度裁剪约束下，大多数神经网络架构只有在学习极其简单地函数时才能达到k地最大梯度范数。因此，通过梯度裁剪来实现k-Lipschitz约束将会导致critic偏向更简单的函数。如下图所示，在小型数据集上，权重剪枝不能捕捉到数据分布的高阶矩。</p>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1551773009/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190305160316.png" alt=""></p>
<p><strong>算法：</strong></p>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1551771655/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190305153950.png" alt=""></p>
<p>先随机采一对真假样本，还有一个0-1的随机数:</p>
<script type="math/tex; mode=display">
x_r\sim P_r, x_g \sim P_g, \epsilon \sim Uniform[0,1]</script><p>在$x_r$和$x_g$的连线上进行随机线性插值采样:</p>
<script type="math/tex; mode=display">
\\
\hat{x}=\epsilon x_r+(1-\epsilon)x_g</script><p>最终的Loss:</p>
<script type="math/tex; mode=display">
L=\mathbb{E}_{\hat{x}\sim\mathbb{P}_g}[D(\tilde{x})]-\mathbb{E}_{x\sim\mathbb{P}_r}[D(x)]+\lambda\mathbb{E}_{\hat{x}\sim\mathbb{P}_\hat{x}}[(||\nabla_{\hat{x}}D(\hat{x})||_2-1)^2]</script><ul>
<li>Gradient Penalty 项： $\lambda\mathbb{E}_{\hat{x}\sim\mathbb{P}_\hat{x}}[(||\nabla_{\hat{x}}D(\hat{x})||_2-1)^2]$</li>
<li>Wasserstein 距离项:  $\mathbb{E}_{\hat{x}\sim\mathbb{P}_g}[D(\tilde{x})]-\mathbb{E}_{x\sim\mathbb{P}_r}[D(x)]$ </li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># [GP] 先随机采一对真假样本，还有一个0-1的随机数:</span></span><br><span class="line">epsilon = tf.random_uniform(shape=[batch_size, <span class="number">1</span>], minval=<span class="number">0.</span>, maxval=<span class="number">1.</span>)</span><br><span class="line"><span class="comment"># [GP] 在x_r和x_g的连线上进行随机线性插值采样:</span></span><br><span class="line">X_hat_State = self.expert_s + epsilon * (self.agent_s - self.expert_s)</span><br><span class="line">X_hat_Action = expert_a_one_hot + epsilon * (agent_a_one_hot - expert_a_one_hot)</span><br><span class="line">X_hat_s_a = tf.concat([X_hat_State, X_hat_Action], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'Discriminator_loss'</span>):</span><br><span class="line">    wasserstein = tf.reduce_mean(crit_A) - tf.reduce_mean(crit_e)  <span class="comment"># Wasserstein 距离</span></span><br><span class="line">    grad_D_X_hat = tf.gradients(X_hat_crit, [X_hat_s_a])[<span class="number">0</span>]</span><br><span class="line">    slopes = tf.sqrt(tf.reduce_sum(tf.square(grad_D_X_hat), reduction_indices=[<span class="number">1</span>])) </span><br><span class="line">    gradient_penalty = tf.reduce_mean((slopes - <span class="number">1.</span>) ** <span class="number">2</span>)  <span class="comment">#[GP] Gradient Penalty</span></span><br><span class="line">    loss = wasserstein + LAMBDA * gradient_penalty</span><br><span class="line">    tf.summary.scalar(<span class="string">'discriminator'</span>, loss)</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>GANs</tag>
      </tags>
  </entry>
  <entry>
    <title>MAML</title>
    <url>/post/3df84f8.html</url>
    <content><![CDATA[<h3 id="一、-One-shot-Learning"><a href="#一、-One-shot-Learning" class="headerlink" title="一、 One-shot Learning"></a>一、 One-shot Learning</h3><ul>
<li><p>解决的问题</p>
<p>使用少量的样本进行快速的学习</p>
</li>
<li><p>解决的方法</p>
<ul>
<li>Transfer Learning</li>
<li>Meta Learning<ul>
<li>One-shot Learning with Memory-Augumented Neural Networks</li>
<li>Optimization as a Model for Few Shot Learning</li>
<li>Model-Agnostic Meta Learning (MAML)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="二、Model-Agnostic-Meta-Learning"><a href="#二、Model-Agnostic-Meta-Learning" class="headerlink" title="二、Model Agnostic Meta Learning"></a>二、Model Agnostic Meta Learning</h3><ul>
<li>Intuition<ul>
<li>在任务之间的相关性，(Interal representations)</li>
<li>任务之间迁移</li>
<li>类似于迁移学习</li>
</ul>
</li>
<li>Meta Learning <ul>
<li>参数对微小的变化敏感</li>
<li>极大的优化了在任何问题上的Loss function</li>
</ul>
</li>
</ul>
<h4 id="2-1-定义"><a href="#2-1-定义" class="headerlink" title="2.1 定义"></a>2.1 定义</h4><ul>
<li>模型$f(\theta):(x→a)$, 输入$x$, 输出$a$</li>
<li>任务$T =\{L(x_1, a_1, . . . , x_H, a_H), q(x_1), q(x_{t+1}|x_t, a_t),H\}$ ,其中$L$是trajectory的Loss，其将产生一个针对于特定任务的反馈 ，$H$是episode length，$q(x_1)$是初始observation 分布，$q(x_{t+1}|x_t,a_t)$是状态转移。</li>
<li>对于监督学习任务来说，$H=1$</li>
<li>$P(T)$是模型期望的任务分布，元学习场景中，希望模型能够适应的任务分布。</li>
<li>对于K-shot Learning 来说，在元学习过程中，从$P(T)​$中采样一个任务$T_i​$，模型用$K​$个样本进行训练，然后从$T_i​$对应的Loss中进行feedback，之后在从$T_i​$上采样的新样本上进行测试。</li>
</ul>
<h4 id="2-2-方法"><a href="#2-2-方法" class="headerlink" title="2.2 方法"></a>2.2 方法</h4><ul>
<li><p>模型$f_\theta$，当去适应新的任务$T_i$时，模型参数从$\theta$更新为$\theta’_i$:   </p>
<script type="math/tex; mode=display">
\theta'_i=\theta-\alpha\nabla_\theta L_{T_i}(f_\theta)</script><p>这里的参数$\alpha​$可以当成变量也可以当成超参数</p>
</li>
<li><p>目标函数(Meta objective)</p>
<script type="math/tex; mode=display">
min_\theta\sum_{T_i \sim p(T)} L_{T_i}(f_{\theta'_i})</script></li>
<li><p>Meta optimization</p>
<script type="math/tex; mode=display">
\theta←\theta-\beta\nabla_\theta\sum_{T_i \sim p(T)}L_{T_i}(f_{\theta_i'})</script></li>
</ul>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1554368445/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190404170030.png" alt=""></p>
<p>直观表示适应过程</p>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1554370299/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190404173122.png" alt=""></p>
]]></content>
  </entry>
  <entry>
    <title>[论文]CONTINUOUS ADAPTATION VIA META-LEARNING IN NONSTATIONARY AND COMPETITIVE ENVIRONMENTS</title>
    <url>/post/177e574f.html</url>
    <content><![CDATA[<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1554355299/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190404132053.png" alt=""></p>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1554355299/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190404132118.png" alt=""></p>
<a id="more"></a>
<h3 id="一、摘要"><a href="#一、摘要" class="headerlink" title="一、摘要"></a>一、摘要</h3><h3 id="二、先序工作"><a href="#二、先序工作" class="headerlink" title="二、先序工作"></a>二、先序工作</h3><h3 id="三、方法"><a href="#三、方法" class="headerlink" title="三、方法"></a>三、方法</h3><h4 id="3-1-Overview"><a href="#3-1-Overview" class="headerlink" title="3.1 Overview"></a>3.1 <strong>Overview</strong></h4><p>（1）<strong>使用few-shot 方法解决非确定性环境下的连续自适应问题存在的问题:</strong></p>
<p>agent必须从有限的过往经验中学习，并且这些过往经验是在没有环境变化下收集的。</p>
<p>（2）<strong>解决方法： </strong>  </p>
<p>​    基于MAML进行构建，其中MAML (gradient-based model-agnostic meta-learning)，在few-shot方法中效果较好。</p>
<p>​    本文从概率的角度对MAML在multi-task RF方向上进行扩展，并最终将其扩展到动态变化的任务中去。</p>
<hr>
<h4 id="3-2-MAML的概率视角"><a href="#3-2-MAML的概率视角" class="headerlink" title="3.2 MAML的概率视角"></a>3.2 <strong>MAML的概率视角</strong></h4><p><strong>2-1 定义</strong></p>
<p>给定任务的分布 $D(T)$，对于每个任务$T$，$T$是一个如下的四元组</p>
<script type="math/tex; mode=display">
T:=(L_T,P_T(x),P_T(x_{t+1}|x_t,a_t),H)</script><ul>
<li>$L_T$表示任务相关的Loss function，其将trajectory $\tau:=(x_0,a_1,x_1,R_1,…a_H,x_H,R_H)$与loss值进行映射;</li>
<li>$P_T(x)$和$P_T(x_{t+1}|x_t,a_t)$定义了任务$T$中的马尔科夫决策过程;</li>
<li>$H$表示任务的时间域(Horizon)；</li>
<li>$x_t$表示observations, $a_t$表示actions,</li>
</ul>
<p>对于每个任务的Loss function $L_T$定义为：</p>
<script type="math/tex; mode=display">
L_T(\tau):=-\sum_{t=1}^HR_t</script><p><strong>2-2 Adaption Update</strong></p>
<ul>
<li><p>元学习的目标是找到一个过程，该过程能够通过在从$D(T) $采样的任务中获得有限经验的情况下，产生解决元学习问题的良好策略。具体为：</p>
</li>
<li><p>在使用策略$\pi_\theta$，从$D(T)$上采样了$K$个task $T \sim D(T)$，表示成$\tau_\theta^{1:K}$之后，构建一个新的针对采样出来的某个特定任务的策略$\pi_\phi$， 该策略的目标是在任务$T$上减小子序列的loss的期望 (expected subsequent loss)。</p>
</li>
<li><p>公式化上述描述即<strong>Adaption Update（Meta Upate）过程</strong>，MAML使用参数为$\theta$的$L_T$的梯度，构建了针对特定任务策略的参数$\phi$:</p>
</li>
</ul>
<script type="math/tex; mode=display">
\phi:=\theta-\alpha\nabla_\theta L_T(\tau^{1:K}) \\
where \space\space L_T(\tau_\theta^{1:K}):=\frac{1}{K}\sum_{k=1}^K L_T(\tau_\theta^k)\\
and \space\space  \tau_\theta^k\sim P_T(\tau|\theta)</script><p><strong>2-3 Meta Loss定义</strong></p>
<p>Meta-Loss的定义如下：</p>
<script type="math/tex; mode=display">
min_\theta\mathbb{E}_{T\sim D(T)}[L_T(\theta)]</script><p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1554303429/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190403225700.png" alt=""></p>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1554301995/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190403223301.png" alt=""></p>
<ul>
<li>一般情况下，$\phi$从某种条件概率分布 $P_T(\phi|\theta, \tau_{1:k})$ 中生成， <strong>Meta Update公式等价于假设delta分布</strong>，即<script type="math/tex; mode=display">
P_T(\phi|\theta,\tau_{1:K}):=\delta(\theta-\alpha\nabla_\theta\frac{1}{K}\sum_{k=1}^K L_T(\tau_\theta^k))</script></li>
</ul>
<p><strong>2-4 Meta Loss优化</strong></p>
<ul>
<li>对于<strong>优化Meta-Loss</strong>，可以采用策略梯度的方法(TRPO，PPO等)按下述梯度进行优化。</li>
</ul>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1554300870/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190403221335.png" alt=""></p>
<hr>
<h4 id="3-3-通过Meta-Learning实现连续自适应"><a href="#3-3-通过Meta-Learning实现连续自适应" class="headerlink" title="3.3 通过Meta-Learning实现连续自适应"></a>3.3 <strong>通过Meta-Learning实现连续自适应</strong></h4><p>​    在经典的多任务中，我们不假设任务的分布，$D(T)$。当环境是non-stationary时，可以把它看作是在某个时间尺度上的一系列stationary任务，其中任务对应于环境的不同动态。然后，$D(T)$由环境变化定义，任务将依赖于一定连续任务。因此，利用连续任务和meta-learn之间的依赖性，不断update policy，从而最大限度地减少与不断变化的环境交互过程中遇到的总loss期望。</p>
<p>​    例如，在Multi-agent中，当与不断优化自身策略的对手（例如，由于学习）进行比赛时，我们的agent应该理想地进行元学习，以预测更改并相应地更新其策略。</p>
<p><strong>3-1 non-stationary的马尔科夫表示</strong></p>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1554302085/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190403223439.png" alt=""></p>
<p>​    以概率的角度来描述non-stationary环境，该环境等价于上图所示的Markov 链表示的任务序列分布。 其目标是最小化某个长度$L$的马尔科夫任务序列上的loss期望。</p>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1554302355/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190403223905.png" alt=""></p>
<p>​        其中$P(T_0)$和$P(T_{i+1}|T_i)$表示初始概率及任务序列的马尔科夫链的概率转移。 值得注意的是，</p>
<p>​    （1）这里的动态马尔科夫链是二级层次化的，其中高层为任务的动态变化，下层为具体任务的MDP；</p>
<p>​    （2）目标$L_{T_i,T_{i+1}}$依赖于Meta learning过程定义的形式。</p>
<p><strong>3-2 连续任务的Meta Loss定义</strong></p>
<p>​    由于我们对任务间马尔可夫变换的最佳Adaption Update感兴趣，因此我们将一对连续任务的Meta Loss    定义如下：</p>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1554303319/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190403225513.png" alt=""></p>
<p>​    对于连续任务的Meta Loss与第二章定义的Meta Loss的不同之处在于：</p>
<p><strong>（1）</strong> 连续任务的Meta Loss的trajectory $\tau_{i,\theta}^{1:K} $来自当前的任务$T_i$ , 并且用来构建Policy $\pi_\phi$ ，以便于执行下一个任务$T_{i+1}$；<br><strong>（2）</strong> 虽然策略参数$\phi_i$是依赖于序列的，但是在上面<strong>连续任务的Meta Loss中一般初始参数设置为$\theta$</strong>。 P.S.(这是出于稳定性考虑。我们从经验上发现优化顺序更新$\phi_{i}$到$\phi_{i+1}$是不稳定的，通常倾向于发散，而从相同的初始化开始导致更好的行为。)</p>
<p>​    因此，优化$L_{T_i，T_{i + 1}}(θ)$ 相当于使用任务链中的单位滞后，截断随着时间的推移反向传播。</p>
<p><strong>3-3 连续任务的Adaption Update</strong></p>
<p>​    为了构建任务$T_{i+1}$的策略参数，我们从参数$\theta$开始，使用adaptive update进行多次 meta-gradient step。(假设step数是$M$，经验参考值为2~5次): <strong>【原文中的公式(7)】</strong></p>
<script type="math/tex; mode=display">
\begin{aligned}
&\phi^0_{i}:=\theta, \space\space\space\space \tau_\theta^{1:K}\sim P_{T_i}(\tau|\theta), \\
&\phi_i^m:=\phi_i^{m-1}-\alpha_m\nabla_{\phi_i^{m-1}}L_{T_i}(\tau_{i,\phi_i^{m-1}}^{1:K}),\space\space\space\space m=1,...,M-1,\\
&\phi_{i+1}:=\phi_i^{M-1}-\alpha_M\nabla_{\phi_i^{M-1}}L_{T_i}(\tau_{i,\phi_i^{M-1}}^{1:K}) 
\end{aligned}</script><p>​    其中$\{\alpha_m\}^M_{m=1}​$是meta-gradient step sizes 集合，其优化与$\theta​$ 相关。 这部分的meta-update计算图如下图所示:</p>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1554346096/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190404104758.png" alt=""></p>
<p><strong>3-4 连续任务的Meta Loss优化</strong></p>
<p>​    连续任务的Meta Loss优化和Meta Loss优化的形式基本相同，只是现在要同时考虑$T_i$和$T_{i+1}$的期望：<strong>【原文中的公式(8)】</strong></p>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1554346986/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190404110256.png" alt=""></p>
<p>​    需要注意的是：</p>
<p>​    计算adaption update的时候需要在策略$\pi_\theta$下与环境进行交互并同时计算meta-loss。</p>
<hr>
<h4 id="3-4-训练过程中的Meta-Learning"><a href="#3-4-训练过程中的Meta-Learning" class="headerlink" title="3.4 训练过程中的Meta-Learning"></a>3.4 训练过程中的Meta-Learning</h4><p>​    当获得了一系列连续任务对的分布$P(T_{i-1},T_i)$之后，我们可以通过使用梯度方法，对参数$\theta$和$\alpha$同时优化来实现对 adaptation updates 的 meta learn。</p>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1554348074/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190404112109.png" alt=""></p>
<p><strong>Algorithm1：</strong> 在任务$T_{i+1}$中交互时，使用$\pi_\theta$ 策略从$T_i$和$\pi_\phi$收集 trajectory。(line 5)</p>
<p>​    直观地说，该算法搜索$θ$和$α$，这样根据$T_i$的轨迹计算出的adaptive update(参照原文公式(7))，产生一个策略$π_\phi$，这有助于对于求解$T_{i+1}$。这里的主要假设是，$T_i$的轨迹包含了一些关于$T_{i+1}$的信息。注意，将adaption steps 作为计算图(图c)的一部分，并通过整个图的反向传播来优化$θ$和$α$，这需要计算二阶导数。</p>
<hr>
<h4 id="3-5-执行过程中的自适应"><a href="#3-5-执行过程中的自适应" class="headerlink" title="3.5 执行过程中的自适应"></a>3.5 执行过程中的自适应</h4><p>​    为了在training过程中计算无偏自适应梯度，必须要使用策略$\pi_\theta $在任务$T_i$上收集经验。在test过程中，由于环境的不确定性，通常很难多次遇到同样的任务。因此，保持使用$\pi_\phi$进行决策，并且对每个新任务，reuse过往经验来计算更新参数$\phi$ (参考algorithm2)：</p>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1554353682/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190404125425.png" alt=""></p>
<p>​    为了修正过往经验的策略与当前的策略$\pi_\theta$的区别，使用重要性采样进行修正，在每一个单步meta update过程中，执行以下操作：<strong>【原文中的公式(9)】</strong></p>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1554353917/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190404125820.png" alt=""></p>
<p>​    其中$\pi_{\phi_{i-1}}$和$\pi_{\phi_i}$分别用来从任务$T_{i-1}$和$T_{i}$中生成轨迹。对于公式(7)中的多步更新，直接将对应项按照公式(9)换成重要性采样就行了</p>
]]></content>
      <tags>
        <tag>元学习</tag>
      </tags>
  </entry>
  <entry>
    <title>[论文] Data-Efficient Hierarchical Reinforcement Learning(HIRO)</title>
    <url>/post/e567270d.html</url>
    <content><![CDATA[<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1554045806/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190331232312.png" alt=""></p>
<hr>
<h3 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h3><ol>
<li><p>提出一种较为通用的，去除繁杂假设的，高效的算法层次强化学习算法框架；</p>
</li>
<li><p>框架</p>
<p>$High-Level  \space\space\space\space\space     {———&gt;}^{auto}  \space\space\space\space Goal  ————&gt; ^{supervised}   \space\space\space\space Low-Level controller$</p>
</li>
<li><p>使用Off-Policy进行High-level及Low-level训练，通用化的设计，使得较好的在low-level controller中使用DDPG，TD3等确定性、off-policy的算法，兼容性强；</p>
</li>
<li><p>提出针对于High-level的Off-Policy Correction。</p>
</li>
</ol>
<a id="more"></a>
<hr>
<h3 id="二、-方法"><a href="#二、-方法" class="headerlink" title="二、 方法"></a>二、 方法</h3><p>HIRO(HIerarchical Reinforcement learning with Off-policy correction)</p>
<ul>
<li><strong>[Low-level $\mu^{lo}$]</strong><ol>
<li>使用 <strong>Parameterized reward function</strong> 来表达特定的底层策略(潜在的无穷集合)；</li>
<li>通过训练相关策略，使其能够将obs $S_t$ 与期望Goal $g_t$相匹配。</li>
</ol>
</li>
<li><strong>[High-level $\mu^{hi}$]</strong> <ol>
<li>为temporally-extended experience选择目标序列；</li>
<li>使用<strong>off-policy correction</strong>使得其可以使用过往经验，来适应新的底层控制器策略。</li>
</ol>
</li>
</ul>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1554041157/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190331220545.png" alt=""></p>
<ul>
<li><p><strong>[综合]</strong> </p>
<ol>
<li><p>高层控制器构建较粗的抽象的目标给底层；</p>
</li>
<li><p>这些目标直接对应于底层控制器想要尝试去达到的states；</p>
</li>
<li><p>每一个timestep, 高层控制器使用由环境交互产生的state，并通过</p>
<p>​    ①每隔一段时间从高层控制器网络$\mu^{hi}$中进行采样($g_t\sim\mu^{hi}$)，或者</p>
<p>​    ②使用一个固定的目标转换函数$g_t=h(s_{t-1},g_{t-1},s_t)$</p>
<p>等方式来生成high-level action(or goal) $g_t\in\mathbb{R}^{d_s}$；</p>
</li>
<li><p>对于上述产生的目标成为临时目标，每固定c步将进行重新采样；</p>
</li>
<li><p>底层控制器通过当前的状态及目标进行决策到仿真环境的动作$a_t \sim \mu^{lo}(s_t,g_t)$；</p>
</li>
<li><p>高层控制器使用一个固定的Parameterized reward function $r$ 向底层控制器提供一个内在的回报函数(intrinsic reward) $r_t=r(s_t,g_t,a_t,s_{t+_1})$；</p>
</li>
<li><p>底层策略将存储经验$(s_t,g_t,a_t,r_t,s_{t+1},h(s_t,g_t,s_{t+1}))$用于off-policy training；</p>
</li>
<li><p>高层控制器每隔c步获取环境回报$R_t$，存储高层状态转移序列$(s_{t:t+c-1},g_{t:t+c-1},a_{t:t+c-1},R_{t:t+c-1},s_{t+c})​$用于off-policy training。</p>
</li>
</ol>
</li>
</ul>
<hr>
<h3 id="三、具体策略"><a href="#三、具体策略" class="headerlink" title="三、具体策略"></a>三、具体策略</h3><ol>
<li><p><strong>Parameterized reward function</strong></p>
<p><strong>(1) 如何理解目标$g_t$</strong></p>
<p>​    高层控制器产生的Goal $g_t$ 意味着对于当前的观测状态的期望相对变化，即$g_t$代表着从当前$s_t$开始，底层控制器应该如何决策才能使得c步之后的$s_{t+c}$与$s_t+g_t$能够足够的接近。</p>
<p><strong>(2) 如何设计一个比较泛化的目标空间</strong></p>
<p>​    为了在不考虑状态变化的情况下保持目标的相同绝对位置，目标转换模型$h$定义为:</p>
<script type="math/tex; mode=display">
h(s_t,g_t,s_{t+1})=s_t+g_t-s_{t+1}</script><p><strong>(3) 如何设计高层到底层的内在回报函数Parameterized reward function</strong></p>
<p>​    该回报函数通过定义当前状态与目标状态之间的欧式距离来表示:</p>
<script type="math/tex; mode=display">
r(s_t,g_t,a_t,s_{t+1})=-||s_t+g_t-s_{t+1}||_2</script><p><strong>(4) 底层控制器的策略算法如何选择</strong></p>
<p>​    通过简单的将目标$g_t$加入观测元组的方式，使得底层框架可以使用强化学习的标准方法进行学习。</p>
<p>​    比如使用DDPG那么loss就加入目标：</p>
<script type="math/tex; mode=display">
L_{DDPG}=[(r(s_t,g_t,a_t,s_{t+1})+\gamma Q_\theta^{lo}(s_{t+1},g_{t+1},\mu_φ^{lo}(s_{t+1},g_{t+1})-Q_{\theta}^{lo}(s_t,g_t,a_t)]^2</script><p><strong>(5) 与以往工作的不同</strong></p>
<p>​    Parameterized reward function 在之前的工作中也用过，但是他们用的时候目标都是间接通过表征学习给出的，而本文直接将目标当做一种预期的状态给到底层，更加直接。</p>
</li>
<li><p><strong>高层训练使用的Off-policy Correction</strong></p>
<p><strong>(1)以往的工作大多基于on-policy ，这里引入了off-policy进行学习会带来怎样的问题</strong></p>
<p>​    引入off-policy的话，由于底层策略网络$\mu^{lo}$在当前和从replay buffer中采样的时候的策略网络是不同的，因而对于同一个目标，在前后两种策略网络下的状态转移序列是不同的。</p>
<p><strong>(2) 如何解决以上问题</strong></p>
<p>​    高层策略网络的状态转移序列为$(s_{t:t+c-1},g_{t:t+c-1},a_{t:t+c-1},R_{t:t+c-1},s_{t+c})$，我们将其转换为可以push到replay buffer中的state-action-reward 转移序列$(s_t,g_t\sum R_{t:t+c-1},s_{t+c})$，以兼容各种标准off-policy强化学习方法。</p>
<p>​    更改$g_t$使其能够适应新的$\mu^{lo}$:</p>
<ul>
<li>先前的目标$g_t$诱导底层产生$a_{t:t+c−1} ∼ \mu^{lo}(s_{t:t+c−1}, g_{t:t+c−1})$行为。    </li>
<li>调整上述目标变成$\tilde {g_t}$使其能够诱导新的底层策略产生最大化概率：$\mu^{lo}(a_{t:t+c−1}|s_{t:t+c−1}, \tilde{g}_{t:t+c−1})$</li>
<li>$\tilde{g}$使用固定的目标转移函数$h$来得到</li>
</ul>
<p><strong>(3) 如何获得新目标$\tilde{g}​$的最大化概率</strong></p>
<p>​    对于一般的强化学习算法而言策略的log概率可以表示为:</p>
<script type="math/tex; mode=display">
log \mu^{lo}(a_{t:t+c−1}|s_{t:t+c−1}, \tilde{g}_{t:t+c−1}) ∝ −\frac{1}{2}\sum^{
t+c−1}_{i=t}
||a_i − \mu^{lo}(s_i, \tilde{g}_i)||_2^2 + const.</script><p>​    这里文章通过离散采样的方式选择一个较大的值作为最大概率的估计，采样10个点为：</p>
<ul>
<li><p>$g_t$</p>
<pre><code>  * 按高斯分布在以$s_{t+c} − s_{t}​$为中心的分布上采样得到8个点

  * $s_{t+c} − s_t$
</code></pre></li>
</ul>
</li>
</ol>
<p>   ​    </p>
]]></content>
      <tags>
        <tag>分层强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>【确定性策略梯度类】 DPG,DDPG,TD3,D4PG</title>
    <url>/post/dad17569.html</url>
    <content><![CDATA[<blockquote>
<p>参考</p>
<p> <a href="https://blog.csdn.net/weixin_37895339/article/details/84881872" target="_blank" rel="noopener">【强化学习】确定性策略强化学习-DPG&amp;DDPG算法推导及分析</a></p>
<p><a href="https://blog.csdn.net/kenneth_yu/article/details/78478356" target="_blank" rel="noopener">Deep Reinforcement Learning - 1. DDPG原理和算法</a></p>
</blockquote>
<h3 id="一、确定性策略梯度"><a href="#一、确定性策略梯度" class="headerlink" title="一、确定性策略梯度"></a>一、确定性策略梯度</h3><p>​    Deepmind的D.Silver等在2014年提出DPG： Deterministic Policy Gradient， 即确定性的行为策略，每一步的行为通过函数$μ$直接获得确定的值：     </p>
<script type="math/tex; mode=display">
a_t=μ(s_t|θ_μ)</script><p>​    这个函数$μ$即最优行为策略，不再是一个需要采样的随机策略。为何需要确定性的策略？简单来说，PG方法有以下缺陷：</p>
<p>​    即使通过PG学习得到了随机策略之后，在每一步行为时，我们还需要对得到的最优策略概率分布进行采样，才能获得action的具体值；而action通常是高维的向量，比如25维、50维，在高维的action空间的频繁采样，无疑是很耗费计算能力的。在PG的学习过程中，每一步计算policy gradient都需要在整个action space进行积分:      </p>
<script type="math/tex; mode=display">
\nabla_θ=∫_S∫_Aρ(s)π_θ(a|s)Q_π(s,a)dads</script><p>​    这个积分我们一般通过Monte Carlo 采样来进行估算，需要在高维的action空间进行采样，耗费计算能力。如果采取简单的Greedy策略，即每一步求解$ argmax_a Q(s,a)$也不可行，因为在连续的、高维度的action空间，如果每一步都求全局最优解，太耗费计算性能。<br>​    将DPG算法融合进actor-critic框架，结合Q-learning或者Gradient Q-learning这些传统的Q函数学习方法，经过训练得到一个确定性的最优行为策略函数。</p>
<a id="more"></a>
<h3 id="二、DPG"><a href="#二、DPG" class="headerlink" title="二、DPG"></a>二、DPG</h3><p>DPG算法本身采用的是PG方法,且是Off-Policy方法(也可以是On-Policy)，因而直接对轨迹的价值回报进行求导。如下式求导，其中$\mu_\theta (s)$为生成确定性行动的策略函数。</p>
<script type="math/tex; mode=display">
\nabla_\theta v_{\mu_{\theta}} (s)=\nabla_\theta[q_\mu(s,\mu_\theta(s))]</script><p>根据链式法则，由于$\mu_\theta(s)$与确定性策略价值函数$q_\mu$有关,因而：</p>
<script type="math/tex; mode=display">
\nabla_\theta v_\mu(s) = \nabla_{\mu_\theta(s)}[q_\mu(s, \mu_\theta(s))]\nabla_\theta\mu_\theta(s)</script><p>由于是确定性策略，在价值函数$q(s,μ_\theta(s))$中有策略参数$\theta$，因此需要将价值函数对策略求导。</p>
<p>相较于随机策略梯度算法而言,如下是随机性策略梯度的目标函数梯度:</p>
<script type="math/tex; mode=display">
∇_θJ(θ)=\mathbb{E}_{τ∼π_θ}[(\sum_{t=0}^T∇_θ\logπ_θ(a_{i,t}∣s_{i,t}))(\sum_{t=0}^Tr(s_{i,t},a_{i,t}))]</script><p>在DPG这个梯度公式中，没有了与动作有关的期望项，因此相对于随机性策略，确定性策略需要的学习数据少，算法效率高，尤其对于动作空间维数很大的情况。</p>
<hr>
<h3 id="三、策略模型参数的一致性"><a href="#三、策略模型参数的一致性" class="headerlink" title="三、策略模型参数的一致性"></a>三、策略模型参数的一致性</h3><p>​    在DPG中为了更好地使用Off-policy,并使用TD降低方差，定义函数 $Q^\omega (s,a):S×A→R$ 用来拟合真实的状态动作值函数$\hat{Q}^\pi(s,a)$，如果$Q^\omega$收敛，那么L2范数梯度将满足如下公式:    </p>
<script type="math/tex; mode=display">
\begin{aligned}
\nabla_\omega Loss &∝ \nabla_\omega\mathbb{E}_{s,a}[\hat{Q}^\pi(s,a)-Q^\omega(s,a)]^2\\&= \mathbb{E}_{s,a\sim\tau}[(\hat{Q}^\pi(s,a)-Q^\omega(s,a))-\nabla _\omega Q^\omega(s,a)]\\&=0
\end{aligned}</script><script type="math/tex; mode=display">
\nabla_\omega Q^\omega(s,a)=\nabla_\theta\log\pi_\theta(s,a)=\nabla_\theta\pi_\theta(s,a)\frac{1}{\pi_\theta(s,a)}</script><p>​    在计算梯度时可以使用$Q^ω:S×A→R $代替真实的动作状态值函数$Q(s,a) $。并且神经网络满足这个性质，因此可以使用神经网络拟合动作状态值函数。这样价值模型不需要遵循某个具体的策略，因此可以使用off-policy的方式进行学习更新。</p>
<hr>
<h3 id="四、DPG目标函数"><a href="#四、DPG目标函数" class="headerlink" title="四、DPG目标函数"></a>四、DPG目标函数</h3><ul>
<li><p>on-policy的确定性策略梯度算法</p>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1553679079/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190327173019.png" alt=""></p>
</li>
<li><p>off-policy的确定性策略梯度算法(Replay buffer中的数据是通过$\beta$采样得到的)                  </p>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1553679073/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190327173100.png" alt=""></p>
</li>
</ul>
<p>​        上两式的区别就是使用了不同的数据采样分布。可以看到off-policy缺少了重要性采样，这是由于确定性策略的动作是固定值，不是一个分布；其次是因为确定性策略值函数的评估采用的是Q-learning的方法，即使用TD(0)估计动作值函数并忽略重要性权重，值函数不依赖于任何策略，并贪心获取下一个动作。</p>
<p>​     这个$β$不是我们想要得到的最优策略，仅仅在训练过程中，生成下达给环境的action， 从而获得我们想要的数据集，比如状态转换(transitions)、或者agent的行走路径等，然后利用这个数据集去 训练策略$μ$，以获得最优策略。在test 和 evaluation时，使用$μ$，不会再使用$β$。</p>
<hr>
<ul>
<li><strong>[Actor]</strong>衡量一个策略网络的表现(策略网络目标函数)，最大化策略目标: (根据上面推导的DPG)</li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned}
J_\beta (\theta)&=\int_S ρ^\beta(s)Q^\omega(s, \mu(s))ds\\
&=\mathbb{E}_{s\simρ^\beta}[Q^\omega(s, \mu(s))]
\end{aligned}</script><p>​    $s$是环境的状态，这些状态(或者说agent在环境中走过的状态路径)是基于agent的behavior策略产生的，它们的分布函数(pdf) 为$ρ^β$；<br>​    $Q^\omega(s,μ(s))$ 是在每个状态下，如果都按照$μ$策略选择acton时，能够产生的$Q$值。 也即，<strong>$J_β(μ)$是在$s$根据$ρ^β$分布时，$Q^\omega(s,μ(s))​$ 的期望值。</strong></p>
<ul>
<li><p><strong>[Critic]</strong>最小化值网络目标:</p>
<script type="math/tex; mode=display">
\hat{Q}^\omega(s_t, a_t)=\mathbb{E}[r(s_t,a_t)+\gamma Q^\omega(s_{t+1},a_{t+1})]</script><script type="math/tex; mode=display">
J_\beta(\omega)=\mathbb{E}_{s\simρ^\beta}[\frac{1}{2}(r_t+\gamma Q^\omega(s_{t+1},a_{t+1})-Q^\omega(s_t,a_t))^2]</script></li>
</ul>
<p><strong>最终DPG的目标函数为:</strong>（$\omega$是值网络(SGD优化)，$\theta$是策略网络(SGA优化)）</p>
<p>​                            <img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1553679178/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190327173249.png" alt=""></p>
<p>参数更新为：</p>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1553680271/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190327175049.png" alt=""></p>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1553680273/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190327175059.png" alt=""></p>
<hr>
<h3 id="五、DDPG"><a href="#五、DDPG" class="headerlink" title="五、DDPG"></a>五、DDPG</h3><p>DDPG借用了target net还有滑动平均方法更新behavior网络参数</p>
<p>其更新方法如下：</p>
<script type="math/tex; mode=display">
\begin{align}
&\delta_t=r_t+\gamma Q^{\omega'}(s_{t+1},\mu_{\theta'}(s_{t+1}))-Q^\omega(s_t,a_t)\\
&\omega_{t+1} = \omega_t+\alpha_\omega \delta_t \nabla_\omega Q^\omega(s_t,a_t) \\
&\theta_{t+1} = \theta_t+\alpha_\theta \nabla_\theta\mu_\theta(s_t)\nabla_a Q^\omega(s_t,a_t)|_{a=\mu_\theta(s_t)} \\
&\theta'=\tau\theta+(1-\tau)\theta' \\
&\omega ' = \tau\omega+(1-\tau)\omega'
\end{align}</script><p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1553680662/20181207173225338.png" alt=""></p>
<p>对比一下DQN</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-14d930f99a102729?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp" alt="img"></p>
<h3 id="六、TD3"><a href="#六、TD3" class="headerlink" title="六、TD3"></a>六、TD3</h3><blockquote>
<p>参考：<a href="https://zhuanlan.zhihu.com/p/47182584" target="_blank" rel="noopener">TD3</a></p>
</blockquote>
<p>TD3(Twin Delayed Deep Deterministic policy gradient algorithm )主要对DDPG做了一些改进:</p>
<ol>
<li><p><strong>引入DoubleDQN的思想，来消除过拟合问题</strong>(两个Critic, 一个Actor)</p>
<p>DoubleDQN中使用Current Net（Behavior net）来代替TargetNet，以减小Bias。</p>
<script type="math/tex; mode=display">
y_j = r_{j+1}+\gamma Q(s_{j+1},argmax_{a'}Q(s_{j+1},a';\theta);\theta^-)</script><p>映射到DPG过程中,其中$\pi_φ(s’)$是CurrentNet:</p>
<script type="math/tex; mode=display">
y = r + \gamma Q_\theta(s',\pi_φ(s'))</script><p>即critic用更新较慢的target network，actor还是更新快的；但由于本身actor更新也不快，所以没啥效果。</p>
<p>如果类比double Q-learning，使用两个actor、两个critic写出来的更新目标为</p>
<p><img src="https://pic2.zhimg.com/80/v2-8648a8a5f3c47a76def05e4f5e7db259_hd.jpg" alt="img"></p>
<p>本着“宁可低估，也不要高估”的想法（因为actor会选择高的，因此高估的会累积起来），再把目标改写成</p>
<p><img src="https://pic3.zhimg.com/80/v2-b303f91e57ab55ff1884cf1d9cd36442_hd.jpg" alt="img"></p>
<p>最后发现两个actor也没啥用，就用一个actor，这个actor根据 $Q_{\theta_1}​$ 来更新。两个critic的更新目标都是一样的，即 $y_2 = y_1​$ 。这样的算法相比于改变之前的就等于多了一个和原来critic同步更新的辅助critic $Q_{\theta_2} ​$，在更新target的时候用来取min。</p>
</li>
<li><p><strong>使用TargetNet</strong></p>
<p>实验结果表明，当policy固定不变的时候，是否使用target network其价值函数都能最后收敛到正确的值；但是actor和critic同步训练的时候，不用target network可能使得训练不稳定或者发散。因此算法的中critic的更新目标都由target network计算出来</p>
<p><img src="https://pic3.zhimg.com/80/v2-f9c9703f8892c46cb17d5fb24a95b012_hd.jpg" alt="img"></p>
<p>并且，价值函数估计准确之后再来更新policy会更好，因此采用了delayed policy update，即以较高的频率更新价值函数，以较低的频率更新policy。</p>
</li>
<li><p><strong>使用Target Policy 平滑正则</strong></p>
<p>希望学到的价值函数在action的维度上更平滑，因此价值函数的更新目标每次都在action上加一个小扰动</p>
<p><img src="https://pic1.zhimg.com/80/v2-f7e163f79d41d3bc8939c8c8607f5e24_hd.jpg" alt="img"></p>
</li>
</ol>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1553831070/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190329114325.png" alt=""></p>
<h3 id="七、D4PG"><a href="#七、D4PG" class="headerlink" title="七、D4PG"></a>七、D4PG</h3><blockquote>
<p>论文 <a href="https://arxiv.org/pdf/1804.08617.pdf" target="_blank" rel="noopener">DISTRIBUTED DISTRIBUTIONAL DETERMINISTIC<br>POLICY GRADIENTS</a></p>
</blockquote>
<p>总体框架与DDPG相同，引入了一些trick：</p>
<ol>
<li><p>支持分布式，由于是off-policy的算法，因此可以使用多个actor去分布式地采样，然后存储在同一个replay buffer中，learner从buffer中采样，更新之后再将权重同步到各个actor上</p>
</li>
<li><p>critic使用价值函数分布，损失函数变为(d是距离度量)：</p>
<script type="math/tex; mode=display">
L(w)=\mathbb{E}_ρ[d(\Tau_{\pi_{\theta'}}Z_{\omega'}(x,a),Z_\omega(x,a))]</script><p>其中$(T_\pi Z)(x, a)=r(x,a)+\gamma\mathbb{E}[Z(x’,\pi(x’))|x,a]$ 为 distributional Bellman operator, $Z$是用来估计Q的， $Q_\pi(x,a) = \mathbb{E}Z_\pi(x,a)$</p>
</li>
<li><p>引入n-step TD error：这样可以减少更新的variance</p>
</li>
<li><p>使用prioritized experience replay：可以加速学习</p>
</li>
</ol>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1555731589/qweqwzsdxa.png" alt=""></p>
]]></content>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>聚类方法</title>
    <url>/post/2f107dda.html</url>
    <content><![CDATA[<h3 id="距离衡量指标"><a href="#距离衡量指标" class="headerlink" title="距离衡量指标"></a>距离衡量指标</h3><ul>
<li>参考：<a href="http://www.spiderpy.cn/blog/detail/33" target="_blank" rel="noopener">常用样本相似性和距离度量方法</a></li>
</ul>
<h3 id="聚类效果衡量指标"><a href="#聚类效果衡量指标" class="headerlink" title="聚类效果衡量指标"></a>聚类效果衡量指标</h3><ul>
<li><p>参考: <a href="https://www.jianshu.com/p/b9528df2f57a" target="_blank" rel="noopener">聚类模型评估</a></p>
</li>
<li><p>常用评估:</p>
<ul>
<li><p>类别信息已知</p>
<ul>
<li>调和兰德系数 (ARI)</li>
<li>调和互信息 (AMI)</li>
<li>调和平均 (V-Measure)</li>
</ul>
</li>
<li><p>类别信息未知</p>
<ul>
<li>轮廓系数 (Silhouette Coefficient)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="基于划分的聚类"><a href="#基于划分的聚类" class="headerlink" title="基于划分的聚类"></a>基于划分的聚类</h3><ul>
<li>KMeans 参考:<a href="https://plushunter.github.io/2017/02/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8811%EF%BC%89%EF%BC%9A%E8%81%9A%E7%B1%BB%EF%BC%882%EF%BC%89/" target="_blank" rel="noopener">机器学习算法系列（11）：聚类（2）—Kmeans</a></li>
<li>K-mediods聚类，将Kmeans的平均值换成中值，避免噪声的干扰</li>
<li>KMeans++: 优化KMeans的聚类中心初始化，选择距离当前聚类中心的距离概率最大可能的点作为下一个聚类中心。参考:<a href="https://blog.csdn.net/google19890102/article/details/53284285" target="_blank" rel="noopener">K-Means++算法</a></li>
</ul>
<h3 id="层次聚类"><a href="#层次聚类" class="headerlink" title="层次聚类"></a>层次聚类</h3><p><a href="http://blog.sina.com.cn/s/blog_69c3ea2b0100nitu.html" target="_blank" rel="noopener">参考</a></p>
<ul>
<li><p>自上而下的分裂层次聚类(DIANA)</p>
</li>
<li><p>自下而上的凝聚层次聚类(AGNES)</p>
</li>
</ul>
<h3 id="密度聚类"><a href="#密度聚类" class="headerlink" title="密度聚类"></a>密度聚类</h3><ul>
<li>DBSCAN : 参考 <a href="https://plushunter.github.io/2017/02/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8811%EF%BC%89%EF%BC%9A%E8%81%9A%E7%B1%BB%EF%BC%883%EF%BC%89/" target="_blank" rel="noopener">机器学习算法系列（11）：聚类（3）—DBSCAN</a></li>
<li>密度最大值聚类: 参考 <a href="https://plushunter.github.io/2017/02/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8811%EF%BC%89%EF%BC%9A%E8%81%9A%E7%B1%BB%EF%BC%884%EF%BC%89/" target="_blank" rel="noopener">机器学习算法系列（11）：聚类（4）—密度最大值聚类</a></li>
<li>AP ：参考 <a href="https://blog.csdn.net/maoyaozong/article/details/40450067" target="_blank" rel="noopener">affinity propagation 近邻传播算法</a></li>
</ul>
<h3 id="谱聚类"><a href="#谱聚类" class="headerlink" title="谱聚类"></a>谱聚类</h3><ul>
<li>谱聚类基本原理: 参考 <a href="https://blog.csdn.net/u011089523/article/details/78906286" target="_blank" rel="noopener">谱聚类算法(Spectral Clustering)</a></li>
<li><p>谱聚类与PCA的异同: 参考 <a href="https://blog.csdn.net/bwangk/article/details/52674025" target="_blank" rel="noopener">特征值与特征向量，PCA和谱聚类</a></p>
</li>
<li><p>一般步骤：</p>
<p>1）输入：相似度矩阵S（Rn∗n）、目标聚类数目k （在此之前需要完成两项工作： 1.选择合适的相似度函数，2.选择合适的聚类数目k）<br>2）构造出相似图及其赋权的邻接矩阵（weighted adjacency matrix） （这一步需要选择：相似图的类型以及相应的参数）<br>3）计算出相似图的Laplacian矩阵 （这一步需要选择：Laplacian矩阵的类型）<br>4）计算Laplacian矩阵的前k个特征值对应的特征向量，以这k个特征向量为列，拼出新的矩阵Un∗k）<br>5）视矩阵U的每一行为Rk中的一个点，对这n个点y1，y2，…yn进行k−means聚类，得到k个聚类C1，C2，…Ck<br>6）输出聚类结果A1,A2,…Ak：yi被分到Cj中的哪一类，xi就被分到相应的Aj类</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>动态规划</title>
    <url>/post/a80d0031.html</url>
    <content><![CDATA[<h3 id="Leetcode-198-打家劫舍"><a href="#Leetcode-198-打家劫舍" class="headerlink" title="[Leetcode 198] 打家劫舍"></a>[Leetcode 198] <a href="https://leetcode-cn.com/problems/house-robber/" target="_blank" rel="noopener">打家劫舍</a></h3><ul>
<li><p>解法：一个经典的dp题，从选与不选的角度进行考虑， </p>
<script type="math/tex; mode=display">
OPT(i)=max\{(OPT(i-2)+arr[i]), OPT(i-1)\}</script><p>终止条件是第0个的时候只有一个可以选，有两个的时候，选二者中大的那个。</p>
</li>
<li><p>参考：<a href="https://www.bilibili.com/video/av18512769?t=2342" target="_blank" rel="noopener">动态规划（第2讲 第一个demo）</a></p>
</li>
<li><p>代码：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">rob</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(nums.empty()) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">if</span>(nums.size()==<span class="number">1</span>) <span class="keyword">return</span> nums[<span class="number">0</span>];</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span> &gt; dp(nums.size(), <span class="number">0</span>); <span class="comment">//构造dp数组</span></span><br><span class="line">                </span><br><span class="line">        <span class="comment">// dp终止条件</span></span><br><span class="line">        dp[<span class="number">0</span>] = nums[<span class="number">0</span>];</span><br><span class="line">        dp[<span class="number">1</span>] = max(nums[<span class="number">1</span>], nums[<span class="number">0</span>]);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// dp递推</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">2</span> ; i&lt;nums.size(); i++)&#123;</span><br><span class="line">                dp[i] = max((dp[i<span class="number">-2</span>] + nums[i]), (dp[i<span class="number">-1</span>]));</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> dp[nums.size()<span class="number">-1</span>];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Leetcode-213-打家劫舍-II"><a href="#Leetcode-213-打家劫舍-II" class="headerlink" title="[Leetcode 213] 打家劫舍 II"></a>[Leetcode 213] <a href="https://leetcode-cn.com/problems/house-robber-ii/" target="_blank" rel="noopener">打家劫舍 II</a></h3><ul>
<li>解法：由于有环，那么给他拆分成[0,n-1] 和 [1,n]两个部分进行分别dp，最后取两个区间中取值大的。</li>
<li>代码：</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">rob</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(nums.empty()) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">if</span>(nums.size() == <span class="number">1</span>) <span class="keyword">return</span> nums[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">if</span>(nums.size() == <span class="number">2</span>) <span class="keyword">return</span> max(nums[<span class="number">0</span>],  nums[<span class="number">1</span>]);</span><br><span class="line">        </span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; dp_n_1(nums.size(), <span class="number">0</span>); <span class="comment">//[0,n-1]</span></span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; dp_n(nums.size(), <span class="number">0</span>); <span class="comment">//[1, n]</span></span><br><span class="line">        </span><br><span class="line">        dp_n[<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">        dp_n[<span class="number">1</span>] = nums[<span class="number">1</span>];</span><br><span class="line">        dp_n[<span class="number">2</span>] = max(nums[<span class="number">1</span>], nums[<span class="number">2</span>]);</span><br><span class="line">        </span><br><span class="line">        dp_n_1[<span class="number">0</span>] = nums[<span class="number">0</span>];</span><br><span class="line">        dp_n_1[<span class="number">1</span>] = max(nums[<span class="number">0</span>], nums[<span class="number">1</span>]);</span><br><span class="line">        dp_n_1[<span class="number">2</span>] = max(dp_n_1[<span class="number">1</span>], nums[<span class="number">2</span>]);</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">2</span>; i&lt;nums.size()<span class="number">-1</span>; i++)&#123;</span><br><span class="line">            dp_n_1[i] = max((dp_n_1[i<span class="number">-2</span>] + nums[i]),(dp_n_1[i<span class="number">-1</span>]));</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">3</span>; i&lt;nums.size(); i++)&#123;</span><br><span class="line">            dp_n[i] = max((dp_n[i<span class="number">-2</span>] + nums[i]),(dp_n[i<span class="number">-1</span>]));</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> max(dp_n_1[nums.size()<span class="number">-2</span>], dp_n[nums.size()<span class="number">-1</span>]);</span><br><span class="line">                                               </span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h3 id="Leetcode-139-单词拆分"><a href="#Leetcode-139-单词拆分" class="headerlink" title="[Leetcode 139] 单词拆分"></a>[Leetcode 139] <a href="https://leetcode-cn.com/problems/word-break/" target="_blank" rel="noopener">单词拆分</a></h3><ul>
<li><p>解法(4种)</p>
<ul>
<li><p>解法一:  DFS</p>
</li>
<li><p>解法二:  记忆化DFS</p>
</li>
<li><p>解法三:  bottom up DP </p>
<p><strong>[子问题定义] :</strong> DP子问题是从0开始到当前位置的子串是否可分(dp[i] == true?)，当前位置总共有n个可能，所以子问题的个数是n个。</p>
<p>使用hashset转储dict</p>
<p>构建dp数组，默认初始空串是可分的即dp[0]=1,</p>
<p>遍历初始串，验证从前面可分的子串尾部到当前位置的字符串([j为dp[j]==1, i])是否在字典中，如果在字典中则记录当前位置可分</p>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1552362068/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190312114003.png" alt=""></p>
</li>
<li><p>解法四:  Bottom up DP + max trick</p>
</li>
</ul>
<a id="more"></a>
</li>
<li><p>代码:</p>
</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">wordBreak</span><span class="params">(<span class="built_in">string</span> s, <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&amp; wordDict)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">unordered_set</span>&lt;<span class="built_in">string</span>&gt; wordSet(wordDict.cbegin(), wordDict.cend());</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span> &gt; dp(s.size()+<span class="number">1</span>, <span class="number">0</span>); <span class="comment">//加1是给空串留位置</span></span><br><span class="line">        dp[<span class="number">0</span>] = <span class="number">1</span>;<span class="comment">// 空串可分</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">//解n个子问题</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; s.size()+<span class="number">1</span>; i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j  = <span class="number">0</span>; j &lt; i; j++)&#123;</span><br><span class="line">                <span class="comment">// j之前可分，判断j~i这段是不是可分的，可分的话，i的位置记为1</span></span><br><span class="line">                <span class="keyword">if</span>(dp[j])&#123;</span><br><span class="line">                    <span class="built_in">string</span> tmp = s.substr(j, i-j);</span><br><span class="line">                    <span class="keyword">if</span>(wordSet.find(tmp) != wordSet.end())&#123;</span><br><span class="line">                        dp[i] = <span class="number">1</span>;</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> dp[s.size()];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h3 id="Leetcode-124-二叉树中的最大路径和"><a href="#Leetcode-124-二叉树中的最大路径和" class="headerlink" title="[Leetcode 124] 二叉树中的最大路径和"></a>[Leetcode 124] <a href="https://leetcode-cn.com/problems/binary-tree-maximum-path-sum/" target="_blank" rel="noopener">二叉树中的最大路径和</a></h3><ul>
<li><p>解法:</p>
<p>找二叉树中的最大路径和，首先要考虑清楚是从上往下找，还是从下往上找，通过观察树的结构， 我们发现从下到上最好找。</p>
</li>
</ul>
<p><strong>1、最优子结构</strong><br>    因为树是由一个个更小的结点树组成，所以我们可以把问题分解成一个个更小的树。<br>    当树的结点只有一个时，最大的路径就是他自身，让树的高度为2时，根节点的最大路径为左右结点中的最大值加上根节点本身的值：max(l, r) + root.val， 如果左右结点都为负数，还没有自身的值大呢，所以我们取其中的最大值。maxSubSubTree = max(max(l, r) + root.val, root.val)<br>    知道了二叉树的最优左右路径，我们需要比较整体路径，maxSubTree = max(maxSubSubTree, l+r+root.val)。<br>    再将以该结点为根节点的二叉树的最大路径和，和全局的路径和比较，取两者最大值，res = max(res, maxSubTree)<br><strong>2、重叠子问题</strong><br>    从下往上走，当底层的最优路径找出来了， 上一层结点就能直接用下一层的结果,依次向上递推，求解过程都简化成了对若干个个高度为2 的二叉树的操作。当递归完成时，根节点的值就是整颗二叉树的最大路径和。    </p>
<ul>
<li>代码:</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"> * struct TreeNode &#123;</span></span><br><span class="line"><span class="comment"> *     int val;</span></span><br><span class="line"><span class="comment"> *     TreeNode *left;</span></span><br><span class="line"><span class="comment"> *     TreeNode *right;</span></span><br><span class="line"><span class="comment"> *     TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125;</span></span><br><span class="line"><span class="comment"> * &#125;;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">maxPathSum</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> res=INT_MIN;</span><br><span class="line">        getMaxPath(root, res);</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">getMaxPath</span><span class="params">(TreeNode* root, <span class="keyword">int</span> &amp;res)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(!root) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> left = getMaxPath(root-&gt;left, res);</span><br><span class="line">        <span class="keyword">int</span> right = getMaxPath(root-&gt;right, res);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//下一级到上一级的最优解</span></span><br><span class="line">        <span class="comment">//下一级的最优解和自身相比，如果下一级没有，或者都为负数，则为自身</span></span><br><span class="line">        <span class="keyword">int</span> maxSubSubTree = max(max(left, right) + root-&gt;val, root-&gt;val);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//表示所考虑是以该结点为根的路径和 </span></span><br><span class="line">        <span class="keyword">int</span> maxSubTree = max(maxSubSubTree, root-&gt;val + left+ right);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//更新整个树中的最大路径和</span></span><br><span class="line">        res = max(maxSubTree, res);</span><br><span class="line">        <span class="keyword">return</span> maxSubSubTree; <span class="comment">//返回单个子子树最优解</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">  </span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h3 id="Leetcode-128-最长连续序列"><a href="#Leetcode-128-最长连续序列" class="headerlink" title="[Leetcode 128] 最长连续序列"></a>[Leetcode 128] <a href="https://leetcode-cn.com/problems/longest-consecutive-sequence/" target="_blank" rel="noopener">最长连续序列</a></h3><ul>
<li><p>解法：(参考<a href="https://www.bilibili.com/video/av38647879" target="_blank" rel="noopener">花花酱</a>)</p>
<p>使用hashmap来构建，将数组中的值作为键，连续元素数目作为边界值，并存储在hashmap的连续区域边界节点中。</p>
<p>节点更新有三种情况:</p>
<ol>
<li>当前元素在hashmap中没有左右邻居，那么该节点的hashmap值为1 <currnodeval, 1="">;</currnodeval,></li>
<li>当前元素在hashmap中有左或右邻居(只存在一边)，那么该节点的hashmap值为其对应左右边界的连续值加1</li>
<li>当前元素能够桥接两个连续区域，那么该节点的hashmap值可以不关心，但是其左侧的左边界值与右侧部分的右边界值对应加一。</li>
</ol>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1552818789/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190317182814.png" alt=""></p>
</li>
<li><p>代码：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">longestConsecutive</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">unordered_map</span>&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt; hashMap;</span><br><span class="line">        <span class="keyword">int</span> res = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">auto</span> item: nums)&#123;</span><br><span class="line">            <span class="keyword">if</span>(hashMap.count(item)) <span class="keyword">continue</span>;</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">auto</span> it_left = hashMap.find(item<span class="number">-1</span>);</span><br><span class="line">            <span class="keyword">auto</span> it_right = hashMap.find(item+<span class="number">1</span>);</span><br><span class="line">            <span class="comment">//获取当前元素左右的边界值，左右无邻居则对应的l,r 为0</span></span><br><span class="line">            <span class="keyword">int</span> l = it_left!=hashMap.end() ? it_left-&gt;second : <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">int</span> r = it_right!=hashMap.end() ? it_right-&gt;second : <span class="number">0</span>;</span><br><span class="line">            </span><br><span class="line">            <span class="comment">//[case3]当前元素能够桥接两个连续区域，那么该节点的hashmap值可以不关心(程序中得填充表示，默认就是跟左右边界相等就行)，但是其左侧的左边界值与右侧部分的右边界值对应加一。</span></span><br><span class="line">            <span class="keyword">if</span>(l!=<span class="number">0</span> &amp;&amp; r!=<span class="number">0</span>)&#123;</span><br><span class="line">                hashMap[item] = hashMap[item-l] = hashMap[item+r] = r + l + <span class="number">1</span>;</span><br><span class="line">            &#125;<span class="keyword">else</span> <span class="keyword">if</span>(l!=<span class="number">0</span> &amp;&amp; r == <span class="number">0</span>)&#123; <span class="comment">//[case2] 当前元素在hashmap中有左或右邻居(只存在一边)，那么该节点的hashmap值为其对应左右边界的连续值加1</span></span><br><span class="line">                hashMap[item] = hashMap[item-l] = l + <span class="number">1</span>;</span><br><span class="line">            &#125;<span class="keyword">else</span> <span class="keyword">if</span>(l==<span class="number">0</span> &amp;&amp; r!=<span class="number">0</span>)&#123; <span class="comment">//[case2] 当前元素在hashmap中有左或右邻居(只存在一边)，那么该节点的hashmap值为其对应左右边界的连续值加1</span></span><br><span class="line">                 hashMap[item] = hashMap[item+r] = r + <span class="number">1</span>;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123; <span class="comment">//[case1] 当前元素在hashmap中没有左右邻居，那么该节点的hashmap值为1 &lt;currNodeVal, 1&gt;;</span></span><br><span class="line">                hashMap[item] = <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 遍历选择边界值最大的</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">const</span> <span class="keyword">auto</span>&amp; item: hashMap)&#123;</span><br><span class="line">            res = max(res, item.second);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
</li>
</ul>
]]></content>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title>A3C、PPO、GAE笔记</title>
    <url>/post/3cc694a0.html</url>
    <content><![CDATA[<h3 id="一、-重要性采样"><a href="#一、-重要性采样" class="headerlink" title="一、 重要性采样"></a>一、 重要性采样</h3><p>TRPO和PPO主要思想的数学基础是重要性采样</p>
<ul>
<li><strong>重要性采样：</strong>$x_i $是从$p(x)$分布中采样得到的， 但是$p(x)$的值往往无法直接获得，需要通过其他分布$q(x)$进行间接采样获得。</li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbb{E}_{x\sim p}[f(x)] &=\int f(x)p(x) dx \\
&=\int f(x) \frac{p(x)}{q(x)}q(x)dx \\
&=\mathbb{E}_{x\sim q}[f(x)\frac{p(x)}{q(x)}]
\end{aligned}</script><ul>
<li><p><strong>条件</strong>：</p>
<ul>
<li>$p$分布与$q$分布需要相近，才能得到较好的效果。</li>
</ul>
</li>
<li><p><strong>用在强化学习里面</strong>:</p>
<ul>
<li>由于策略梯度原始公式中的 新策略分布难以得到，因而使用旧策略进行间接采样，以使得未知项变成可估计的已知项进行计算。</li>
</ul>
<a id="more"></a>
</li>
</ul>
<hr>
<h3 id="二、-梯度与参数更新"><a href="#二、-梯度与参数更新" class="headerlink" title="二、 梯度与参数更新"></a>二、 梯度与参数更新</h3><p><strong>1. 回报的期望：</strong>最大化全部采样轨迹上的策略回报值，$R(\tau)$ 表示某一个轨迹$\tau$的回报值</p>
<script type="math/tex; mode=display">
argmax_\theta \space \mathbb{E}[{R_\theta}]=\sum_\tau R(\tau)p_\theta(\tau)</script><p><strong>2. 回报的期望的梯度</strong>：(第三个等号用到的公式：$\nabla f(x) = f(x) \nabla \log f(x)​$)</p>
<script type="math/tex; mode=display">
\begin{aligned}
\nabla \mathbb{E}[{R_\theta}]&=\sum_\tau R(\tau) \nabla p_{\theta}(\tau) \\
&= \sum_\tau R(\tau)p_\theta(\tau)\frac{\nabla p_\theta(\tau)}{p_\theta(\tau)} \\
&= \sum_\tau R(\tau)p_\theta(\tau){\nabla \log p_\theta(\tau)}\\
&= \mathbb{E}_{\tau \sim p_\theta{(\tau)}}[R(\tau){\nabla \log p_\theta(\tau)}] \\
&≈ \frac{1}{N} \sum_{n=1}^{N}R(\tau^n)\nabla \log p_{\theta}(\tau^n) \\
&=\frac{1}{N}\sum_{n=1}^{N} \sum_{t=1}^{T_n} R(\tau^n)\nabla \log p_\theta(a_t^n|s_t^n)

\end{aligned}</script><p>式中</p>
<ul>
<li><p>$N​$表示采样了$N​$条trajectory, $T_n​$表示每条trajectory的step数量。</p>
</li>
<li><p>关于$p_{\theta}(\tau)$ </p>
<script type="math/tex; mode=display">
\begin{aligned}
p_{\theta}(\tau) &= p(s_1)p_\theta(a_1|s_1)p(s_2|s_1,a_1)p_\theta(a_2|s_2)p(s_3|s_2,a_2) \space\space...\space\space \\
&=p(s_1) \prod_{t=1}^T p_{\theta}(a_t|s_t)p(s_{t+1}|s_t, a_t)
\end{aligned}</script><p>由两部分组成一部分是来自环境的 $p_\theta(s_{t+1}|s_t, a)$， 一部分是来自agent的 $p_\theta {(a_t|s_t)}$, 其中来自环境的部分不带入计算，策略更新只考虑agent这部分。所以最后一步并没有$t+1$这部分。</p>
</li>
</ul>
<p><strong>3. 参数更新：</strong></p>
<script type="math/tex; mode=display">
\theta = \theta+\eta \nabla \bar{R_\theta}</script><hr>
<h3 id="三、-实际算法中对策略梯度的处理方法"><a href="#三、-实际算法中对策略梯度的处理方法" class="headerlink" title="三、 实际算法中对策略梯度的处理方法"></a>三、 实际算法中对策略梯度的处理方法</h3><ol>
<li><p><strong>策略梯度方法：</strong></p>
<p>加入baseline</p>
<script type="math/tex; mode=display">
\nabla \bar{R_\theta}=\frac{1}{N} \sum_{n=1}^{N}(R(\tau^n)-b)\nabla \log p_{\theta}(\tau^n) \\
b≈\mathbb{E}[R(\tau)]</script></li>
</ol>
<p>   $b$ 的加入保证reward不是恒大于0的，若reward一直大于0，则会导致未被采样的action无法得到提升，但其实该action并不是不好而是未被采样。</p>
<ol>
<li><p><strong>状态值函数估计轨迹回报：</strong></p>
<p>$R(\tau^n)-b$ 部分使用状态值函数来替代</p>
<script type="math/tex; mode=display">
q(s,a)</script></li>
<li><p><strong>优势函数估计轨迹回报：</strong></p>
<p>$R(\tau^n)-b​$ 部分用以下Advantage function来替代</p>
</li>
</ol>
<script type="math/tex; mode=display">
A(s_t,a_t)= q(s,a)-V(s)</script><ol>
<li><p><strong>TD-Error估计轨迹回报：(A3C)使用值网络估计值，引入bias减小variance</strong></p>
<p>$R(\tau^n)-b​$ 部分用以下TD-Error 代替</p>
<script type="math/tex; mode=display">
r(s_t. a_t)+v(s_{t+1})-v(s)​</script></li>
</ol>
<hr>
<h3 id="四、GAE-Generalized-Advantage-Estimation"><a href="#四、GAE-Generalized-Advantage-Estimation" class="headerlink" title="四、GAE(Generalized Advantage Estimation)"></a>四、GAE(Generalized Advantage Estimation)</h3><ol>
<li><p><strong>GAE的作用</strong></p>
<ul>
<li>GAE的意思是泛化优势估计，因而他是用来<strong>优化Advantage Function优势函数</strong>的。</li>
<li>GAE的存在是用来权衡variance和bias问题的：<ul>
<li>On-policy直接交互并用每一时刻的回报作为长期回报的估计$\sum_{t’=t}^{T} \gamma^{t’-t}r_{t’}$ 会产生较大的方差，Variance较大。</li>
<li>而通过基于优势函数的AC方法来进行回报值估计，则会产生方差较小，而Bias较大的问题。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>GAE 推导</strong></p>
<p>满足$\gamma$-just条件。(未完待续)</p>
</li>
<li><p><strong>GAE形式</strong></p>
<p>GAE的形式为多个价值估计的加权平均数。</p>
<script type="math/tex; mode=display">
TD Error=\delta_t=r_t+\gamma v(s_{t+1})-v(s_t)</script><p>运用GAE公式进行优势函数的估计：</p>
</li>
</ol>
<script type="math/tex; mode=display">
\sum_{l=0}^\infin(\gamma\lambda)^l \delta_{t+l}^V</script><p>​    为了快速估计序列中所有时刻的估计值，采用倒序计算，从t+1时刻估计t时刻：</p>
<script type="math/tex; mode=display">
\hat{A_t}^{GAE(\gamma,\lambda)}=\sum_{l=0}^{\infin}(\gamma\lambda)^l \delta_{t+l}^V=\delta_t^V+\gamma\lambda\hat{A}_{t+l}^{GAE(\gamma,\lambda)}</script><hr>
<h3 id="五、PPO关于策略梯度的目标函数"><a href="#五、PPO关于策略梯度的目标函数" class="headerlink" title="五、PPO关于策略梯度的目标函数"></a>五、PPO关于策略梯度的目标函数</h3><p>以上所述的策略梯度算法属于on-policy的算法，而<strong>PPO属于off-policy</strong>的算法</p>
<ul>
<li><p><strong>on-policy:</strong> 使用当前策略$\pi_\theta$收集数据，当参数$\theta$更新后，必须重新采样。</p>
<script type="math/tex; mode=display">
\nabla \bar{R_\theta}=\mathbb{E}_{\tau \sim p_\theta{\tau}}[R(\tau){\nabla \log p_\theta(\tau)}]</script></li>
<li><p><strong>off-policy:</strong> 可以从可重用的样本数据中获取样本来训练当前的策略$\pi _\theta​$，下式用了重要性采样。</p>
<script type="math/tex; mode=display">
\nabla \bar{R_\theta}=\mathbb{E}_{\tau \sim p_{\theta^\prime}{\tau}}[\frac{p_\theta(\tau)}{p_{\theta^\prime}(\tau)} R(\tau){\nabla \log p_\theta(\tau)}]</script></li>
</ul>
<p><strong>1. PPO目标函数</strong></p>
<p>   对于PPO而言，轨迹回报通过采用Advantage function的方式进行估计，因而其梯度更新方式为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\nabla \bar{R_\theta} &=\mathbb{E}_{(s_t,a_t)\sim\pi_\theta}[A^\theta(s_t,a_t)\nabla \log p_\theta({a_t^n|s_t^n})] \\
&=\mathbb{E}_{(s_t,a_t)\sim\pi_\theta^\prime}[\frac{p_\theta(s_t,a_t)}{p_\theta^\prime(s_t,a_t)}A^{\theta^\prime}(s_t,a_t)\nabla \log p_\theta({a_t^n|s_t^n})] \\
&=\mathbb{E}_{(s_t,a_t)\sim\pi_\theta^\prime}[\frac{p_\theta(a_t|s_t)}{p_\theta^\prime(a_t|s_t)}\frac{p_\theta(s_t)}{p_\theta^\prime(s_t)}A^{\theta^\prime}(s_t,a_t)\nabla \log p_\theta({a_t^n|s_t^n})] \\
&≈\mathbb{E}_{(s_t,a_t) \sim \pi_\theta^\prime}[\frac{\nabla p_\theta(a_t|s_t)}{p_\theta^\prime(a_t|s_t)}A^{\theta^\prime}(s_t,a_t)]
\end{aligned}</script><p>​    其中，从第二个等式用的是重要性采样，第三到第四个约等式由于$\frac{p_\theta(s_t)}{p_\theta^\prime(s_t)}​$这一项来源于重要性采样，前提假设两个分布差别不大，近似为1，且不易计算，故省略，后面的$\nabla \log p_\theta({a_t^n|s_t^n})​$ ,根据公式$\nabla f(x) = f(x) \nabla \log f(x)​$转换。     </p>
<p>​    因而，定义<strong>目标函数</strong>为：</p>
<script type="math/tex; mode=display">
J^{\theta^{\prime}} (\theta)=\mathbb{E}_{(s_t,a_t) \sim \pi_\theta^\prime}[\frac{p_\theta(a_t|s_t)}{p_\theta^\prime(a_t|s_t)}A^{\theta^\prime}(s_t,a_t)]</script><p><strong>2. PPO对于重要性采样约束的处理</strong></p>
<p>​    为了保证$p_\theta(s_t,a_t) ​$ 与 $p_\theta^\prime(s_t,a_t)​$ 分布的差别不会太大，采用以下约束：</p>
<ul>
<li><strong>TRPO</strong>： 使用约束 $KL(\theta,\theta’)&lt;\delta$，在分布上进行约束。</li>
<li><strong>PPO1</strong>(Adaptive KL)：使用$J_{PPO}^{\theta’}(\theta)=J^{\theta’}(\theta)-\beta KL(\theta,\theta’)$，在目标函数上加一个正则项进行约束，注意，这里KL散度衡量的是action之间的距离，而不是参数$\theta$与$\theta’$之间的距离。</li>
<li><strong>PPO2</strong> (Clip，论文中推荐的)：使用$J_{PPO_2}^{\theta’}(\theta)=\sum_{(s_t,a_t)}\min\{([\frac{p_\theta(a_t|s_t)}{p_\theta^\prime(a_t|s_t)}A^{\theta^\prime}(s_t,a_t)], [clip(\frac{p_\theta(a_t|s_t)}{p_\theta^\prime(a_t|s_t)},1-\epsilon,1+\epsilon)A^{\theta^\prime}(s_t,a_t)])\}​$, 来约束分布距离。</li>
</ul>
<ol>
<li><strong>使用GAE对优势函数进行优化</strong></li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_gaes</span><span class="params">(self, rewards, v_preds, v_preds_next)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    GAE</span></span><br><span class="line"><span class="string">    :param rewards: r(t)</span></span><br><span class="line"><span class="string">    :param v_preds: v(st)</span></span><br><span class="line"><span class="string">    :param v_preds_next: v(st+1)</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    deltas = [r_t + self.gamma * v_next - v <span class="keyword">for</span> r_t, v_next, v <span class="keyword">in</span> zip(rewards, v_preds_next, v_preds)]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#计算GAE(lambda = 1), 参见 ppo paper eq(11)</span></span><br><span class="line">    gaes = copy.deepcopy(deltas)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 倒序计算GAE</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> reversed(range(len(gaes) - <span class="number">1</span>)):</span><br><span class="line">        gaes[t] = gaes[t] + self.gamma * gaes[t + <span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> gaes</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="六、-PPO的目标函数"><a href="#六、-PPO的目标函数" class="headerlink" title="六、 PPO的目标函数"></a>六、 PPO的目标函数</h3><p>PPO的最终目标函数由三部分组成，可使用梯度下降求解，而不是像TRPO一样使用共轭梯度法：</p>
<ul>
<li>策略梯度目标函数： $L_t^{CLIP}(\theta)​$</li>
<li>值函数目标函数：$L_t^{VF}(\theta)=(V_\theta(s_t)-V_t^{target})^2=((r+\gamma v(s_{t+1}))-v(s_t))^2$</li>
<li>策略模型的熵: $S_{[\pi_\theta]}(s_t)=-\pi_\theta(a|s)\log\pi_\theta(a|s)​$</li>
</ul>
<p>完整的形式如下：</p>
<script type="math/tex; mode=display">
L_t^{PPO_2}(\theta)=\hat{\mathbb{E}}_t[L_t^{CLIP}(\theta)-c_1L_t^{VF}(\theta)+c_2S_{[\pi_\theta]}(s_t)]</script><p>这部分相应的代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'assign_op'</span>):</span><br><span class="line">    self.assign_ops = []</span><br><span class="line">    <span class="keyword">for</span> v_old, v <span class="keyword">in</span> zip(old_pi_trainable, pi_trainable):</span><br><span class="line">        self.assign_ops.append(tf.assign(v_old, v))</span><br><span class="line"></span><br><span class="line"><span class="comment"># inputs for train_op</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'train_inp'</span>):</span><br><span class="line">    self.actions = tf.placeholder(dtype=tf.int32, shape=[<span class="keyword">None</span>], name=<span class="string">'actions'</span>)</span><br><span class="line">    self.rewards = tf.placeholder(dtype=tf.float32, shape=[<span class="keyword">None</span>], name=<span class="string">'rewards'</span>)</span><br><span class="line">    self.v_preds_next = tf.placeholder(dtype=tf.float32, shape=[<span class="keyword">None</span>], name=<span class="string">'v_preds_next'</span>)</span><br><span class="line">    self.gaes = tf.placeholder(dtype=tf.float32, shape=[<span class="keyword">None</span>], name=<span class="string">'gaes'</span>)</span><br><span class="line"></span><br><span class="line">act_probs = self.Policy.act_probs</span><br><span class="line">act_probs_old = self.Old_Policy.act_probs</span><br><span class="line"></span><br><span class="line"><span class="comment"># agent通过新策略选择action的概率 probabilities of actions which agent took with policy</span></span><br><span class="line">act_probs = act_probs * tf.one_hot(indices=self.actions, depth=act_probs.shape[<span class="number">1</span>])</span><br><span class="line">act_probs = tf.reduce_sum(act_probs, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># agent通过旧策略选择action的概率 probabilities of actions which agent took with old policy</span></span><br><span class="line">act_probs_old = act_probs_old * tf.one_hot(indices=self.actions, depth=act_probs_old.shape[<span class="number">1</span>])</span><br><span class="line">act_probs_old = tf.reduce_sum(act_probs_old, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'PPO_loss'</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">        策略目标函数</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># ratios = tf.divide(act_probs, act_probs_old)</span></span><br><span class="line">    <span class="comment"># r_t(θ) = π/πold 为了防止除数为0，这里截取一下值，然后使用e(log减法)来代替直接除法</span></span><br><span class="line">    ratios = tf.exp(</span><br><span class="line">        tf.log(tf.clip_by_value(act_probs, <span class="number">1e-10</span>, <span class="number">1.0</span>)) - tf.log(tf.clip_by_value(act_probs_old, <span class="number">1e-10</span>, <span class="number">1.0</span>)))</span><br><span class="line">    <span class="comment"># L_CLIP 裁剪优势函数值</span></span><br><span class="line">    clipped_ratios = tf.clip_by_value(ratios, clip_value_min=<span class="number">1</span> - clip_value, clip_value_max=<span class="number">1</span> + clip_value)</span><br><span class="line">    self.loss_clip = tf.minimum(tf.multiply(self.gaes, ratios), tf.multiply(self.gaes, clipped_ratios))</span><br><span class="line">    self.loss_clip = tf.reduce_mean(self.loss_clip)</span><br><span class="line"></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">        策略模型的熵</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 计算新策略πθ的熵 S = -p log(p) 这里裁剪防止p=0</span></span><br><span class="line">    self.entropy = -tf.reduce_sum(</span><br><span class="line">        self.Policy.act_probs * tf.log(tf.clip_by_value(self.Policy.act_probs, <span class="number">1e-10</span>, <span class="number">1.0</span>)), axis=<span class="number">1</span>)</span><br><span class="line">    self.entropy = tf.reduce_mean(self.entropy, axis=<span class="number">0</span>)  <span class="comment"># mean of entropy of pi(obs)</span></span><br><span class="line"></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">        值目标函数</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># L_vf = [(r+γV(π(st+1))) - (V(π(st)))]^2</span></span><br><span class="line">    v_preds = self.Policy.v_preds</span><br><span class="line">    self.loss_vf = tf.squared_difference(self.rewards + self.gamma * self.v_preds_next, v_preds)</span><br><span class="line">    self.loss_vf = tf.reduce_mean(self.loss_vf)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># construct computation graph for loss</span></span><br><span class="line">    <span class="comment"># L(θ) = E_hat[L_CLIP(θ) - c1 L_VF(θ) + c2 S[πθ](s)]</span></span><br><span class="line">    <span class="comment"># L = 策略目标函数 + 值目标函数 + 策略模型的熵</span></span><br><span class="line">    self.loss = self.loss_clip - c_1 * self.loss_vf + c_2 * self.entropy</span><br><span class="line">    <span class="comment"># minimize -loss == maximize loss</span></span><br><span class="line">    self.loss = -self.loss</span><br><span class="line"></span><br><span class="line">optimizer = tf.train.RMSPropOptimizer(learning_rate=args.ppo_lr, epsilon=<span class="number">1e-5</span>)</span><br><span class="line">self.gradients = optimizer.compute_gradients(self.loss, var_list=pi_trainable)</span><br><span class="line">self.train_op = optimizer.minimize(self.loss, var_list=pi_trainable)</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="七、Actor-Critic"><a href="#七、Actor-Critic" class="headerlink" title="七、Actor-Critic"></a>七、Actor-Critic</h3><p>A2C、A3C等方法采用的是TD方法来替代R-b部分</p>
<ol>
<li><p>A3C</p>
<ul>
<li><p>方法:</p>
<ul>
<li>启动N个线程，Agent在N线程中同时进行环境交互收集样本；</li>
<li>收集完样本后，每一个线程将独立完成训练并得到参数更新量，异步更新到全局的模型参数中；</li>
<li>下一次训练的时候，线程的模型参数将与全局参数完成同步，使用新的参数进行下一次训练。</li>
</ul>
</li>
<li><p>目标函数:</p>
<p>使用TD-$\lambda$减小TD带来的偏差，可以在训练早期更快的提升价值模型。为了增加模型的探索性，目标函数中引入了策略的熵。</p>
</li>
</ul>
</li>
</ol>
<script type="math/tex; mode=display">
\nabla_\theta J(\theta)=\frac{1}{T}\sum_t^T\nabla_\theta \log \pi(a_t|s_t;\theta)(\sum_{i=0}^n\gamma^{i-1}r_{t+1}+v(s_{t+n})-v(s_t))+\beta\nabla_\theta H(s_t;\theta)</script><p>   <img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1553760534/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190328160651.png" alt=""></p>
<ol>
<li><p>A2C</p>
<p>与A3C不同的是参数更新全部在全局master完成，每个子线程只负责env.step()进行探索。</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>生成对抗网络(笔记)</title>
    <url>/post/d42ca19b.html</url>
    <content><![CDATA[<blockquote>
<p><strong>参考</strong></p>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/25071913?columnSlug=f00cb0979b57ab6d7f70e287b0cba55d" target="_blank" rel="noopener">令人拍案叫绝的Wasserstein GAN</a></li>
<li><a href="http://www.dataguru.cn/article-11229-1.html" target="_blank" rel="noopener">WGAN最新进展：从weight clipping到gradient penalty</a></li>
<li><a href="https://blog.csdn.net/Jasminexjf/article/details/82686953" target="_blank" rel="noopener">Ten paper: GAN-GP(Gradient Penalty)</a></li>
<li><a href="https://blog.csdn.net/zhl493722771/article/details/82781914" target="_blank" rel="noopener">深度有趣 | 16 令人拍案叫绝的WGAN</a></li>
<li><a href="https://www.cnblogs.com/bonelee/p/9166084.html" target="_blank" rel="noopener">开发者自述：我是这样学习 GAN 的</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247495668&amp;idx=1&amp;sn=e7e959b2bdd7b2763b9207ccb80fa6bc&amp;chksm=96ea3074a19db96208a51d26f7b5b4ef9c3a37a7799ec270becc77203de4294235041ede7206&amp;scene=0&amp;xtrack=1&amp;key=ea0a47f5b68e2b15d12fc7340eeaead8c766a680edb675d1e37261c54bf195211fca30c0935b743e9e6dd48a73a6ef5d12a3fc2ced96f775ec178345e1a5c665a9c23b45568dfc4dd79eda44cfbb4327&amp;ascene=1&amp;uin=MjMzNzA4OTgzMQ%3D%3D&amp;devicetype=Windows+10&amp;version=62060739&amp;lang=zh_CN&amp;pass_ticket=Ra%2BHQfQVPTFOQ2krrdR9WfmIhtZPhLL%2F4MTSGwpPVIL4A76Tszpn5qdPd%2B%2Bqxp4s" target="_blank" rel="noopener">推荐：生成对抗网络综述</a></li>
</ol>
</blockquote>
<h3 id="一、-基本概念"><a href="#一、-基本概念" class="headerlink" title="一、 基本概念"></a>一、 基本概念</h3><ol>
<li><p><strong>符号定义</strong></p>
<p>$D$ 判别模型， $G$ 生成模型</p>
<p>$x​$ 数据集中的数据分布，$z​$ 某种随机分布</p>
</li>
<li><p><strong>目标函数</strong>（详细参见第四部分）(找一系列D让其对应的V最大，然后在这些最大的V里面选一个最小的)</p>
<script type="math/tex; mode=display">
min_Gmax_DV(G,D)=\mathbb{E}_x[log(D(x))]+\mathbb{E}_z[log(1-D(G(z)))]</script><ul>
<li><p><strong>D Loss</strong> (MC采样，相当于训练二分类器$x \sim P_{data}$一类，$\hat{x} \sim G(z)$一类):</p>
<script type="math/tex; mode=display">
max_DV(G,D)=\mathbb{E}_x[log(D(x))]+\mathbb{E}_z[log(1-D(G(z)))]</script></li>
<li><p><strong>G Loss原始</strong> （MiniMax GAN<strong>[MMGAN]</strong>）(<strong>判别器越好，生成器梯度消失越严重</strong>)</p>
<script type="math/tex; mode=display">
\mathbb{E}{_{x\sim P_g}}[log(1-D(x))]</script></li>
<li><p><strong>G Loss改进</strong> (Non-saturating GAN<strong>[NSGAN]</strong> )(-log trick) (其实与原始的差别不大)</p>
<script type="math/tex; mode=display">
\mathbb{E}{_{x\sim P_g}}[-log(D(x))]</script></li>
</ul>
</li>
<li><p><strong>释义</strong></p>
<p>$G$的目标是最大化生成数据与数据集数据的似然，减小生成数据与数据集数据之间的差距（原始GAN就是JSD）。对于生成器$ G $来说，为了尽可能欺骗$ D$，所以需要最大化生成样本的判别概率 $D(G(z))$，即最小化$ log(1-D(G(z)))$，注意：$log(D(x)) $一项与生成器$ G $无关，所以可以忽略。</p>
<script type="math/tex; mode=display">
G^{*}=argmin_G(Divergence(P_G,P_{data}))=argmin_G max_DV(D,G)</script><p>$D$要解决的问题是一个二分类问题，$V(D,G)$ 为二分类问题中常见的交叉熵损失。</p>
<script type="math/tex; mode=display">
D^{*}=argmax_DV(D,G)\\
V(D,G) = \mathbb{E}_{x\sim P_{data}}[logD(x)]+\mathbb{E}_{x \sim P_G}[1-log(1-D(x))]</script></li>
</ol>
<a id="more"></a>
<h3 id="二、-原理推导"><a href="#二、-原理推导" class="headerlink" title="二、 原理推导"></a>二、 原理推导</h3><blockquote>
<p>参考: <a href="https://www.cnblogs.com/bonelee/p/9166084.html" target="_blank" rel="noopener">开发者自述：我是这样学习 GAN 的</a></p>
</blockquote>
<p>​    真实数据的分布$ P_{data}(x)$，$x$ 是一个真实数据，是一个向量，这个向量集合的分布就是 $P_{data}$。我们需要生成一些也在这个分布内的数据，如果直接就是这个分布的话，怕是做不到的。</p>
<p>​    现有的 Generator 生成的分布可以假设为 $P_G(x;θ)$，这是一个由 $ θ $ 控制的分布，$θ$ 是这个分布的参数（如果是高斯混合模型，那么 $ θ$ 就是每个高斯分布的均值和方差) 。</p>
<p>​    假设我们在真实分布中取出一些数据，${x_1, x_2, … , x_m}$，我们想要计算一个似然 $P_G(x_i; θ)$。</p>
<p>​    对于这些数据，在生成模型中的似然就是</p>
<script type="math/tex; mode=display">
L=\prod_{i=1}^mP_G(x^i;\theta)</script><ol>
<li><p><strong>GAN原理</strong></p>
<p>最大化上面这个似然，等价于让 Generator 生成那些真实数据分布的概率最大。这就变成了一个最大似然估计的问题了，我们需要找到一个 $θ^*$ 来最大化这个似然。(倒数第三行减掉的$P_{data}$项是为了凑KL，其不包含G的参数相关项，没有影响单调性。)</p>
<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201707/5966d343a395d.png?imageMogr2/format/jpg/quality/90" alt="å¼åèèªè¿°ï¼ææ¯è¿æ ·å­¦ä¹  GAN ç"></p>
<p>固定$G$,求一个最优的$D^*​$</p>
<script type="math/tex; mode=display">
\begin{aligned}
V&=\mathbb{E}_{x\sim P_{data}}[logD(x)]+\mathbb{E}_{x\sim P_G}[log(1-D(x))]\\
&=\int_x P_{data}(x)\log D(x)dx+\int_xP_G(x)\log (1-D(x))dx \\
&=\int_x[P_{data}(x)\log D(x)+P_G(x)\log(1-D(x))]dx
\end{aligned}</script><p>那么转为优化$f(D) = P_{data}(x)\log D(x)+P_G(x)\log(1-D(x))$</p>
<p>对$f(D)$求偏导:</p>
<script type="math/tex; mode=display">
\frac{df(D)}{dD} = P_{data}(x)×\frac{1}{D}+P_G(x)×\frac{1}{1-D}×(-1) = 0</script><p>可以解得:</p>
<script type="math/tex; mode=display">
D^*(x) = \frac{P_{data}(x)}{P_{data}(x)+P_G(x)}</script><p>对于一个给定的 x，得到最优的 D 如上图，范围在 (0,1) 内，把最优的 $D^{*} ​$ 带入V</p>
<script type="math/tex; mode=display">
\begin{aligned}
&max_DV(G,D)&=V(G,D^*)\\
&=\mathbb{E}_{x\sim P_{data}}[logD^*(x)]+\mathbb{E}_{x\sim P_G}[log(1-D^*(x))]\\
&=\mathbb{E}_{x\sim P_{data}}[log\frac{P_{data}(x)}{P_{data}(x)+P_G(x)}]+\mathbb{E}_{x\sim P_G}[log(1-\frac{P_{G}(x)}{P_{data}(x)+P_G(x)})] \\
&=\int_xP_{data}(x)log\frac{P_{data}(x)}{P_{data}(x)+P_G(x)}dx+\int_xP_G(x)log(1-\frac{P_{G}(x)}{P_{data}(x)+P_G(x)})dx\\
&=-2log2+\int_xP_{data}(x)log\frac{P_{data}(x)}{(P_{data}(x)+P_G(x))/2}dx+\int_xP_G(x)log(1-\frac{P_{G}(x)}{(P_{data}(x)+P_G(x))/2})dx \\
&=-2log2+KL(P_{data}(x)||\frac{P_{data}(x)+P_G(x)}{2})+KL(P_G(x)||\frac{P_{data}+P_G(x)}{2}) \\
&= -2log2+2JSD(P_{data}(x)||P_G(x))
\end{aligned}</script><p>所以最终(V(G,D)可以看成是JSD)</p>
<script type="math/tex; mode=display">
\mathbb{E}_{x\sim P_r}[\log D^*(x)] + \mathbb{E}_{x\sim P_g}[\log(1-D^*(x))] = 2JS(P_r || P_g) - 2\log 2</script><p>表示两个分布之间的差异，最小值是$ -2log2​$，最大值为 $0​$。观察上式，当 $P_G(x)=P_{data}(x)​$ 时，G 是最优的。</p>
</li>
</ol>
<h3 id="三、GAN的训练"><a href="#三、GAN的训练" class="headerlink" title="三、GAN的训练"></a>三、GAN的训练</h3><p>有了上面推导的基础之后，我们就可以开始训练 GAN 了。结合我们开头说的，两个网络交替训练，我们可以在起初有一个 $G_0$ 和 $D_0$，先用Gradient Ascent训练 $D_0$ 找到 ：</p>
<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201707/5966d6b71a9c4.png?imageMogr2/format/jpg/quality/90" alt="开发者自述：我是这样学习 GAN 的"></p>
<p>然后固定 $D_0​$ 开始训练 $G_0​$， 训练的过程都可以使用 gradient descent，以此类推，训练 $D_1,G_1,D_2,G_2,…​$</p>
<p>但是这里有个问题就是，你可能在 $D_0^*​$ 的位置取到了：</p>
<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201707/5966d72e64539.png?imageMogr2/format/jpg/quality/90" alt="开发者自述：我是这样学习 GAN 的"></p>
<p>然后更新 $G_0$ 为 $G_1​$，可能会出现：</p>
<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201707/5966d7617e528.png?imageMogr2/format/jpg/quality/90" alt="开发者自述：我是这样学习 GAN 的"></p>
<p>但是并不保证会出现一个新的点$ D_1^*$ 使得：</p>
<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201707/5966d788afdf1.png?imageMogr2/format/jpg/quality/90" alt="开发者自述：我是这样学习 GAN 的"></p>
<p>这样更新 $G$ 就没达到它原来应该要的效果，如下图所示(G变化太大导致的JS值不稳定)：</p>
<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201707/5966d79e1ea1e.png?imageMogr2/format/jpg/quality/90" alt="开发者自述：我是这样学习 GAN 的"></p>
<p>避免上述情况的方法就是更新 $G ​$的时候，不要更新 $G ​$太多。</p>
<p>知道了网络的训练顺序，我们还需要设定两个 loss function，一个是$ D$ 的 loss，一个是 $G$ 的 loss。下面是整个 GAN 的训练具体步骤：</p>
<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201707/5966d7bdc95a8.png?imageMogr2/format/jpg/quality/90" alt="å¼åèèªè¿°ï¼ææ¯è¿æ ·å­¦ä¹  GAN ç"></p>
<p>​    <strong>适当多练D</strong>，<strong>少练G</strong>以免<strong>G的变化过大，导致JS大小不稳定</strong>，在原始GAN中，<strong>D也不要练太多</strong>，原始GAN的D是一个二分类器，<strong>用sigmoid激活，如果练的太多，导致分布很难继续拟合</strong>，形象的说就是土推不动，这块儿可以用 <strong>Least square GAN</strong>来解决，参见下面这张图(来自李宏毅老师的教程。</p>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1554971348/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190411162649.png" alt=""></p>
<h3 id="四、存在的问题"><a href="#四、存在的问题" class="headerlink" title="四、存在的问题"></a>四、存在的问题</h3><ol>
<li><p><strong>G的Loss function收敛速度问题：</strong></p>
<p>  $G​$ 的 loss function $\mathbb{E}{_{x\sim P_g}}[log(1-D(x))]​$ 还是有一点小问题，下图是两个函数的图像：</p>
</li>
</ol>
<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201707/5966d7da92f9c.png?imageMogr2/format/jpg/quality/90" alt="å¼åèèªè¿°ï¼ææ¯è¿æ ·å­¦ä¹  GAN ç"></p>
<p>​    <strong>[问题原因]</strong> $log(1-D(x)) $是我们计算时 G 的 loss function，但是我们发现，在$ D(x) $接近于 0 的时候，这个函数十分平滑，梯度非常的小。这就会导致，在训练的初期，$G $想要骗过 $D$，变化十分的缓慢，而上面的函数，趋势和下面的是一样的，都是递减的。但是它的优势是在 $D(x) $接近 0 的时候，梯度很大，有利于训练，在 $D(x) $越来越大之后，梯度减小，这也很符合实际，在初期应该训练速度更快，到后期速度减慢。</p>
<p>​    <strong>[解决方案]</strong> 所以我们把$ G$ 的 loss function 修改为:</p>
<script type="math/tex; mode=display">
minimizeV=-\frac{1}{m}\sum_{i=1}^m \log(D(x^i))</script><ol>
<li><p><strong>Loss 没有变化，一直都是平的</strong></p>
<p><strong>[问题]</strong> 此时$max_DV(G,D)=0$, $JS=log2$, $P_G$和$P_{data}$由于$D$过拟合导致完全没有交集，但是实际上两个分布是有交集的，造成这个的原因是因为，我们无法真正计算期望和积分，只能使用 sample 的方法，如果训练的过拟合了，D 还是能够完全把两部分的点分开:</p>
<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201707/5966d86f91198.png?imageMogr2/format/jpg/quality/90" alt="å¼åèèªè¿°ï¼ææ¯è¿æ ·å­¦ä¹  GAN ç"></p>
<p><strong>[问题原因]</strong> 对于这个问题，我们是否应该让 $D$变得弱一点，减弱它的分类能力，但是从理论上讲，为了让它能够有效的区分真假图片，我们又希望它能够，所以这里就产生了矛盾。</p>
<p>还有可能的原因是，虽然两个分布都是高维的，但是两个分布都十分的窄，可能交集相当小，这样也会导致 JS divergence 算出来为$log2$，约等于没有交集。</p>
<p><strong>[解决方案]</strong> 解决的一些方法，有添加噪声，让两个分布变得更宽，可能可以增大它们的交集，这样 JS divergence 就可以计算，但是随着时间变化，噪声需要逐渐变小，换一种距离计算方法比如使用Wasserstein</p>
</li>
<li><p><strong>Mode Collapse</strong></p>
<p>Mode collapse的出现应该可以说是必然的，其通过不同的散度进行拟合，最优的情况很可能就是出现在某一个密度较高的分布峰上。结局mode collapse的方法可以使用ensemble的方式，我这边还尝试过先聚类后gan的方式。</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>GANs</tag>
      </tags>
  </entry>
  <entry>
    <title>Hybrid A* 路径规划</title>
    <url>/post/c0e17f81.html</url>
    <content><![CDATA[<p>参考:</p>
<p><a href="https://zhuanlan.zhihu.com/p/40776683" target="_blank" rel="noopener"> Hybrid A Star 路径规划</a></p>
<p><a href="https://github.com/karlkurzer/path_planner" target="_blank" rel="noopener">Hybrid A* Path Planner for the KTH Research Concept Vehicle</a></p>
<p><a href="https://blog.csdn.net/AdamShan/article/details/80633099" target="_blank" rel="noopener">分层有限状态机和无人车行为规划</a></p>
<p><a href="http://kth.diva-portal.org/smash/record.jsf?pid=diva2:1057261" target="_blank" rel="noopener"> [PAPER] : Path Planning in Unstructured Environments</a></p>
<h3 id="Hybrid-A-的使用场景"><a href="#Hybrid-A-的使用场景" class="headerlink" title="Hybrid A* 的使用场景"></a>Hybrid A* 的使用场景</h3><p>​    在斯坦福大学2007年参加的DARPA无人车城市挑战赛时使用的Junior，其在行为规划层提出了分层有限状态机的方式，如下图所示。</p>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1551429769/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190301164059.png" alt=""></p>
<p>​    其中，<strong>BAD_RNDF</strong>状态表示的是，当前道路与系统的路网图不同的时候，无人车将采用Hybrid A*来进行规划路径。</p>
<a id="more"></a>
<h3 id="Hybrid-A-与-A"><a href="#Hybrid-A-与-A" class="headerlink" title="Hybrid A 与 A"></a>Hybrid A<em> 与 A</em></h3><p>Hybrid A* 的主要特点是：</p>
<ul>
<li>考虑物体的实际运动方向约束，不像A*假定所有的相邻节点都可以顺利转移</li>
<li>A<em> 的物体总是出现在栅格中心，而 Hybrid A</em> 则不一定</li>
<li>Hybrid A* 是连续路径</li>
<li>Hybrid A<em> 基于A</em></li>
</ul>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1551431225/%E9%98%BF%E8%90%A8%E5%BE%B7%E5%99%A81.png" alt=""></p>
<h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程:"></a>算法流程:</h3><p>主要流程如下图所示，需要说明的是：</p>
<p>​    算法考虑了车辆的x,y坐标，偏航角。</p>
<ol>
<li><p><strong>G值更新策略如下：</strong></p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//###################################################</span></span><br><span class="line"><span class="comment">//                                      MOVEMENT COST</span></span><br><span class="line"><span class="comment">//###################################################</span></span><br><span class="line"><span class="keyword">void</span> Node3D::updateG() &#123;</span><br><span class="line">  <span class="comment">// 前向行驶</span></span><br><span class="line">  <span class="keyword">if</span> (prim &lt; <span class="number">3</span>) &#123;</span><br><span class="line">    <span class="comment">// penalize turning 当前一个节点的prim与当前节点的prim不相等时，判断发生偏转。</span></span><br><span class="line">    <span class="keyword">if</span> (pred-&gt;prim != prim) &#123;</span><br><span class="line">      <span class="comment">// 方向变化</span></span><br><span class="line">      <span class="keyword">if</span> (pred-&gt;prim &gt; <span class="number">2</span>) &#123;</span><br><span class="line">        g += dx[<span class="number">0</span>] * Constants::penaltyTurning * Constants::penaltyCOD;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        g += dx[<span class="number">0</span>] * Constants::penaltyTurning;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      g += dx[<span class="number">0</span>];</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 倒车行驶</span></span><br><span class="line">  <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// penalize turning and reversing</span></span><br><span class="line">    <span class="keyword">if</span> (pred-&gt;prim != prim) &#123;</span><br><span class="line">      <span class="comment">// penalize change of direction</span></span><br><span class="line">      <span class="keyword">if</span> (pred-&gt;prim &lt; <span class="number">3</span>) &#123;</span><br><span class="line">        g += dx[<span class="number">0</span>] * Constants::penaltyTurning * Constants::penaltyReversing * Constants::penaltyCOD;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        g += dx[<span class="number">0</span>] * Constants::penaltyTurning * Constants::penaltyReversing;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      g += dx[<span class="number">0</span>] * Constants::penaltyReversing;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ol>
<li><p><strong>H值更新策略如下：</strong></p>
<p>可以支持倒车时计算当前节点到终点的<a href="https://zhuanlan.zhihu.com/p/38940994" target="_blank" rel="noopener">Reeds-Shepp 曲线</a>，仅支持前向行驶时计算Dubins曲线；</p>
<p>H值为Reeds-Shepp曲线、Dubins曲线、曼哈顿距离三种cost解算出来的最大值。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//###################################################</span></span><br><span class="line"><span class="comment">//                                         COST TO GO</span></span><br><span class="line"><span class="comment">//###################################################</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">updateH</span><span class="params">(Node3D&amp; start, <span class="keyword">const</span> Node3D&amp; goal, Node2D* nodes2D, <span class="keyword">float</span>* dubinsLookup, <span class="keyword">int</span> width, <span class="keyword">int</span> height, CollisionDetection&amp; configurationSpace, Visualize&amp; visualization)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">float</span> dubinsCost = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">float</span> reedsSheppCost = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">float</span> twoDCost = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">float</span> twoDoffset = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// if dubins heuristic is activated calculate the shortest path</span></span><br><span class="line">  <span class="comment">// constrained without obstacles</span></span><br><span class="line">  <span class="keyword">if</span> (Constants::dubins) &#123;</span><br><span class="line">    ompl::base::<span class="function">DubinsStateSpace <span class="title">dubinsPath</span><span class="params">(Constants::r)</span></span>;</span><br><span class="line">    State* dbStart = (State*)dubinsPath.allocState();</span><br><span class="line">    State* dbEnd = (State*)dubinsPath.allocState();</span><br><span class="line">    dbStart-&gt;setXY(start.getX(), start.getY());</span><br><span class="line">    dbStart-&gt;setYaw(start.getT());</span><br><span class="line">    dbEnd-&gt;setXY(goal.getX(), goal.getY());</span><br><span class="line">    dbEnd-&gt;setYaw(goal.getT());</span><br><span class="line">    dubinsCost = dubinsPath.distance(dbStart, dbEnd);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// if reversing is active use a</span></span><br><span class="line">  <span class="keyword">if</span> (Constants::reverse &amp;&amp; !Constants::dubins) &#123;</span><br><span class="line">    <span class="comment">//    ros::Time t0 = ros::Time::now();</span></span><br><span class="line">    ompl::base::<span class="function">ReedsSheppStateSpace <span class="title">reedsSheppPath</span><span class="params">(Constants::r)</span></span>;</span><br><span class="line">    State* rsStart = (State*)reedsSheppPath.allocState();</span><br><span class="line">    State* rsEnd = (State*)reedsSheppPath.allocState();</span><br><span class="line">    rsStart-&gt;setXY(start.getX(), start.getY());</span><br><span class="line">    rsStart-&gt;setYaw(start.getT());</span><br><span class="line">    rsEnd-&gt;setXY(goal.getX(), goal.getY());</span><br><span class="line">    rsEnd-&gt;setYaw(goal.getT());</span><br><span class="line">    reedsSheppCost = reedsSheppPath.distance(rsStart, rsEnd);</span><br><span class="line">    <span class="comment">//    ros::Time t1 = ros::Time::now();</span></span><br><span class="line">    <span class="comment">//    ros::Duration d(t1 - t0);</span></span><br><span class="line">    <span class="comment">//    std::cout &lt;&lt; "calculated Reed-Sheep Heuristic in ms: " &lt;&lt; d * 1000 &lt;&lt; std::endl;</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// if twoD heuristic is activated determine shortest path</span></span><br><span class="line">  <span class="comment">// unconstrained with obstacles</span></span><br><span class="line">  <span class="keyword">if</span> (Constants::twoD &amp;&amp; !nodes2D[(<span class="keyword">int</span>)start.getY() * width + (<span class="keyword">int</span>)start.getX()].isDiscovered()) &#123;</span><br><span class="line">    <span class="comment">//    ros::Time t0 = ros::Time::now();</span></span><br><span class="line">    <span class="comment">// create a 2d start node</span></span><br><span class="line">    <span class="function">Node2D <span class="title">start2d</span><span class="params">(start.getX(), start.getY(), <span class="number">0</span>, <span class="number">0</span>, <span class="literal">nullptr</span>)</span></span>;</span><br><span class="line">    <span class="comment">// create a 2d goal node</span></span><br><span class="line">    <span class="function">Node2D <span class="title">goal2d</span><span class="params">(goal.getX(), goal.getY(), <span class="number">0</span>, <span class="number">0</span>, <span class="literal">nullptr</span>)</span></span>;</span><br><span class="line">    <span class="comment">// run 2d astar and return the cost of the cheapest path for that node</span></span><br><span class="line">    nodes2D[(<span class="keyword">int</span>)start.getY() * width + (<span class="keyword">int</span>)start.getX()].setG(aStar(goal2d, start2d, nodes2D, width, height, configurationSpace, visualization));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (Constants::twoD) &#123;</span><br><span class="line">    <span class="comment">// offset for same node in cell</span></span><br><span class="line">    twoDoffset = <span class="built_in">sqrt</span>(((start.getX() - (<span class="keyword">long</span>)start.getX()) - (goal.getX() - (<span class="keyword">long</span>)goal.getX())) * ((start.getX() - (<span class="keyword">long</span>)start.getX()) - (goal.getX() - (<span class="keyword">long</span>)goal.getX())) +</span><br><span class="line">                      ((start.getY() - (<span class="keyword">long</span>)start.getY()) - (goal.getY() - (<span class="keyword">long</span>)goal.getY())) * ((start.getY() - (<span class="keyword">long</span>)start.getY()) - (goal.getY() - (<span class="keyword">long</span>)goal.getY())));</span><br><span class="line">    twoDCost = nodes2D[(<span class="keyword">int</span>)start.getY() * width + (<span class="keyword">int</span>)start.getX()].getG() - twoDoffset;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// return the maximum of the heuristics, making the heuristic admissable</span></span><br><span class="line">  start.setH(<span class="built_in">std</span>::max(reedsSheppCost, <span class="built_in">std</span>::max(dubinsCost, twoDCost)));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1551437581/1231q23.png" alt=""></p>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p>参考：<a href="https://github.com/karlkurzer/path_planner/blob/master/include/algorithm.h" target="_blank" rel="noopener">Hybrid A* Path Planner for the KTH Research Concept Vehicle </a></p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//###################################################</span></span><br><span class="line"><span class="comment">//                                        3D A*</span></span><br><span class="line"><span class="comment">//###################################################</span></span><br><span class="line">Node3D* Algorithm::hybridAStar(Node3D&amp; start,</span><br><span class="line">                               <span class="keyword">const</span> Node3D&amp; goal,</span><br><span class="line">                               Node3D* nodes3D,</span><br><span class="line">                               Node2D* nodes2D,</span><br><span class="line">                               <span class="keyword">int</span> width,</span><br><span class="line">                               <span class="keyword">int</span> height,</span><br><span class="line">                               CollisionDetection&amp; configurationSpace,</span><br><span class="line">                               <span class="keyword">float</span>* dubinsLookup,</span><br><span class="line">                               Visualize&amp; visualization) &#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// PREDECESSOR AND SUCCESSOR INDEX</span></span><br><span class="line">  <span class="keyword">int</span> iPred, iSucc;</span><br><span class="line">  <span class="keyword">float</span> newG;</span><br><span class="line">  <span class="comment">// Number of possible directions, 3 for forward driving and an additional 3 for reversing</span></span><br><span class="line">  <span class="keyword">int</span> dir = Constants::reverse ? <span class="number">6</span> : <span class="number">3</span>;</span><br><span class="line">  <span class="comment">// Number of iterations the algorithm has run for stopping based on Constants::iterations</span></span><br><span class="line">  <span class="keyword">int</span> iterations = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// VISUALIZATION DELAY</span></span><br><span class="line">  ros::<span class="function">Duration <span class="title">d</span><span class="params">(<span class="number">0.003</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// OPEN LIST AS BOOST IMPLEMENTATION</span></span><br><span class="line">  <span class="keyword">typedef</span> boost::heap::binomial_heap&lt;Node3D*,</span><br><span class="line">          boost::heap::compare&lt;CompareNodes&gt;</span><br><span class="line">          &gt; priorityQueue;</span><br><span class="line">  priorityQueue O;  <span class="comment">// OPEN_LIST 优先队列</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">// update h value</span></span><br><span class="line">  updateH(start, goal, nodes2D, dubinsLookup, width, height, configurationSpace, visualization);</span><br><span class="line">  <span class="comment">// mark start as open</span></span><br><span class="line">  start.open();</span><br><span class="line">  <span class="comment">// push on priority queue aka open list</span></span><br><span class="line">  O.push(&amp;start);</span><br><span class="line">  iPred = start.setIdx(width, height);</span><br><span class="line">  nodes3D[iPred] = start;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// NODE POINTER</span></span><br><span class="line">  Node3D* nPred;</span><br><span class="line">  Node3D* nSucc;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// float max = 0.f;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// continue until O empty</span></span><br><span class="line">  <span class="keyword">while</span> (!O.empty()) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// pop node with lowest cost from priority queue</span></span><br><span class="line">    nPred = O.top();</span><br><span class="line">    <span class="comment">// set index</span></span><br><span class="line">    iPred = nPred-&gt;setIdx(width, height);</span><br><span class="line">    iterations++;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// RViz visualization</span></span><br><span class="line">    <span class="keyword">if</span> (Constants::visualization) &#123;</span><br><span class="line">      visualization.publishNode3DPoses(*nPred);</span><br><span class="line">      visualization.publishNode3DPose(*nPred);</span><br><span class="line">      d.sleep();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// _____________________________</span></span><br><span class="line">    <span class="comment">// LAZY DELETION of rewired node</span></span><br><span class="line">    <span class="comment">// if there exists a pointer this node has already been expanded</span></span><br><span class="line">    <span class="keyword">if</span> (nodes3D[iPred].isClosed()) &#123;</span><br><span class="line">      <span class="comment">// pop node from the open list and start with a fresh node</span></span><br><span class="line">      O.pop();</span><br><span class="line">      <span class="keyword">continue</span>;</span><br><span class="line">    &#125; </span><br><span class="line">    <span class="comment">// _________________</span></span><br><span class="line">    <span class="comment">// EXPANSION OF NODE</span></span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (nodes3D[iPred].isOpen()) &#123;</span><br><span class="line">      <span class="comment">// add node to closed list</span></span><br><span class="line">      nodes3D[iPred].close();</span><br><span class="line">      <span class="comment">// remove node from open list</span></span><br><span class="line">      O.pop();</span><br><span class="line"></span><br><span class="line">      <span class="comment">// _________</span></span><br><span class="line">      <span class="comment">// GOAL TEST 检测当前节点是否是终点或者是否超出了解算最大时间</span></span><br><span class="line">      <span class="keyword">if</span> (*nPred == goal || iterations &gt; Constants::iterations) &#123;</span><br><span class="line">        <span class="comment">// DEBUG</span></span><br><span class="line">        <span class="keyword">return</span> nPred;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// ____________________</span></span><br><span class="line">      <span class="comment">// CONTINUE WITH SEARCH</span></span><br><span class="line">      <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// _______________________</span></span><br><span class="line">        <span class="comment">// SEARCH WITH DUBINS SHOT</span></span><br><span class="line">        <span class="keyword">if</span> (Constants::dubinsShot &amp;&amp; nPred-&gt;isInRange(goal) &amp;&amp; nPred-&gt;getPrim() &lt; <span class="number">3</span>) &#123;</span><br><span class="line">          nSucc = dubinsShot(*nPred, goal, configurationSpace);</span><br><span class="line"></span><br><span class="line">          <span class="keyword">if</span> (nSucc != <span class="literal">nullptr</span> &amp;&amp; *nSucc == goal) &#123;</span><br><span class="line">            <span class="comment">//DEBUG</span></span><br><span class="line">            <span class="comment">// std::cout &lt;&lt; "max diff " &lt;&lt; max &lt;&lt; std::endl;</span></span><br><span class="line">            <span class="keyword">return</span> nSucc;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// ______________________________</span></span><br><span class="line">        <span class="comment">// SEARCH WITH FORWARD SIMULATION</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; dir; i++) &#123;</span><br><span class="line">          <span class="comment">// 创建下一个扩展节点，这里有三种可能的方向，如果可以倒车的话是6种方向</span></span><br><span class="line">          nSucc = nPred-&gt;createSuccessor(i);</span><br><span class="line">          <span class="comment">// 设置节点遍历标识</span></span><br><span class="line">          iSucc = nSucc-&gt;setIdx(width, height);</span><br><span class="line"></span><br><span class="line">          <span class="comment">// 判断扩展节点是否满足约束，能否进行遍历</span></span><br><span class="line">          <span class="keyword">if</span> (nSucc-&gt;isOnGrid(width, height) &amp;&amp; configurationSpace.isTraversable(nSucc)) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 确定新扩展的节点不在close list中，或者没有在之前遍历过</span></span><br><span class="line">            <span class="keyword">if</span> (!nodes3D[iSucc].isClosed() || iPred == iSucc) &#123;</span><br><span class="line"></span><br><span class="line">              <span class="comment">// 更新G值</span></span><br><span class="line">              nSucc-&gt;updateG();</span><br><span class="line">              newG = nSucc-&gt;getG();</span><br><span class="line"></span><br><span class="line">              <span class="comment">// 如果扩展节点不在OPEN LIST中，或者找到了更短G值的路径</span></span><br><span class="line">              <span class="keyword">if</span> (!nodes3D[iSucc].isOpen() || newG &lt; nodes3D[iSucc].getG() || iPred == iSucc) &#123;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// calculate H value</span></span><br><span class="line">                updateH(*nSucc, goal, nodes2D, dubinsLookup, width, height, configurationSpace, visualization);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// if the successor is in the same cell but the C value is larger 如果扩展节点与当前节点在同一个栅格，并且cost值更大，则略过</span></span><br><span class="line">                <span class="keyword">if</span> (iPred == iSucc &amp;&amp; nSucc-&gt;getC() &gt; nPred-&gt;getC() + Constants::tieBreaker) &#123;</span><br><span class="line">                  <span class="keyword">delete</span> nSucc;</span><br><span class="line">                  <span class="keyword">continue</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// if successor is in the same cell and the C value is lower, set predecessor to predecessor of predecessor</span></span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span> (iPred == iSucc &amp;&amp; nSucc-&gt;getC() &lt;= nPred-&gt;getC() + Constants::tieBreaker) &#123;</span><br><span class="line">                  nSucc-&gt;setPred(nPred-&gt;getPred());</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (nSucc-&gt;getPred() == nSucc) &#123;</span><br><span class="line">                  <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"looping"</span>;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// put successor on open list</span></span><br><span class="line">                nSucc-&gt;open();</span><br><span class="line">                nodes3D[iSucc] = *nSucc;</span><br><span class="line">                O.push(&amp;nodes3D[iSucc]);</span><br><span class="line">                <span class="keyword">delete</span> nSucc;</span><br><span class="line">              &#125; <span class="keyword">else</span> &#123; <span class="keyword">delete</span> nSucc; &#125;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123; <span class="keyword">delete</span> nSucc; &#125;</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123; <span class="keyword">delete</span> nSucc; &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (O.empty()) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Rviz实验效果"><a href="#Rviz实验效果" class="headerlink" title="Rviz实验效果"></a>Rviz实验效果</h3><p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1551437757/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190301185544.png" alt=""></p>
]]></content>
      <tags>
        <tag>路径规划算法</tag>
      </tags>
  </entry>
  <entry>
    <title>RNN反向传播推导</title>
    <url>/post/cf8dbd9a.html</url>
    <content><![CDATA[<p>参考：</p>
<ul>
<li><p><a href="http://www.cnblogs.com/pinard/p/6509630.html" target="_blank" rel="noopener">循环神经网络(RNN)模型与前向反向传播算法</a></p>
</li>
<li><p><a href="https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/blob/master/A-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/B-%E4%B8%93%E9%A2%98-RNN.md" target="_blank" rel="noopener">RNN面试总结</a></p>
</li>
</ul>
<h3 id="RNN正向传播推导"><a href="#RNN正向传播推导" class="headerlink" title="RNN正向传播推导"></a>RNN正向传播推导</h3><p>RNN原理图：</p>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1550906908/%E7%BB%98%E5%9B%BE2.png" alt="x"><br><a id="more"></a></p>
<ul>
<li><strong>变量说明</strong><ul>
<li>$W,U,V​$为权重矩阵，在整个RNN网络中是<strong>共享</strong>的。</li>
<li>$h^t​$代表在序列$t​$时模型的隐藏状态。$h^t​$由$x^t​$和$h^{t-1}​$<strong>共同</strong>决定。</li>
<li>$o^{t}$代表在序列$t$时模型的输出。$o^t$<strong>只由</strong>模型当前的隐藏状态$h^{t}$决定。</li>
<li>$x^t$代表在序列$t$时训练样本的<strong>输入</strong>。</li>
<li>$L_t​$代表在序列$t​$时模型的<strong>损失函数</strong>。</li>
<li>$y^t$代表在序列$t​$时训练样本序列的<strong>真实输出</strong>。</li>
</ul>
</li>
<li><strong>前向公式</strong></li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned}
&h^t=\sigma(Wh^{t-1}+Ux^{t}+B) \\
&o^t=Vh^t+C\\
&\hat{y}^t=\sigma_{softmax}(o^t)
\end{aligned}</script><h3 id="RNN反向传播推导"><a href="#RNN反向传播推导" class="headerlink" title="RNN反向传播推导"></a>RNN反向传播推导</h3><script type="math/tex; mode=display">
L=\sum_TL_t</script><ul>
<li><p><strong>$L$对$C$的梯度：</strong></p>
<script type="math/tex; mode=display">
\frac{\partial{L}}{\partial{C}} =\sum_T\frac{\partial{L_t}}{\partial{o^t}} \cdot \frac{\partial{o^t}}{\partial{C}}=\sum_T(\hat{y}^t-y^t)</script></li>
<li><p><strong>$L$ 对V的梯度：</strong></p>
</li>
</ul>
<script type="math/tex; mode=display">
\frac{\partial{L}}{\partial{V}}=\sum_T\frac{\partial{L_t}}{\partial o^t}\cdot\frac{\partial o^t}{\partial V}=\sum_T(\hat{y}^t-y^t)(h^t)^T</script><ul>
<li><strong>$L$ 对$h^t$的梯度：</strong></li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned}
\delta^{t}&=\frac{\partial L}{\partial h^{t}} \\
&=\frac{\partial L}{\partial o^{t}} \frac{\partial o^{t}}{\partial h^{t}} + \frac{\partial L}{\partial h^{t+1}}\frac{\partial h^{t+1}}{\partial h^{t}} \\
&=V^T(\hat{y}^{(t)} - y^{(t)}) + W^T\delta^{(t+1)}diag(1-(h^{(t+1)})^2)
\end{aligned}</script><p>​    其中：</p>
<p>​    (1)</p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial o^{t}} \frac{\partial o^{t}}{\partial h^{t}} = V^T(\hat{y}^{t} - y^{t})</script><p>​    (2)</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial L}{\partial h^{t+1}}\frac{\partial h^{t+1}}{\partial h^{t}}&=\delta^{t+1} \frac{\partial h^{t+1}}{\partial h^{t}}\\
&=W^T\delta^{t+1}diag(1-(h^{t+1})^2)
\end{aligned}</script><p>​    详细推导：<br><img src="http://img2018.cnblogs.com/blog/1220309/201812/1220309-20181221151409029-188623430.png" alt="img"></p>
<ul>
<li><p><strong>$L​$ 对$W​$的梯度：</strong></p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial W} =  \sum\limits_{t=1}^{\tau}\frac{\partial L}{\partial h^{t}} \frac{\partial h^{t}}{\partial W} = \sum\limits_{t=1}^{\tau}diag(1-(h^{t})^2)\delta^{t}(h^{t-1})^T</script></li>
<li><p><strong>$L$ 对$B$的梯度：</strong></p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial B}= \sum\limits_{t=1}^{\tau}\frac{\partial L}{\partial h^{t}} \frac{\partial h^{t}}{\partial b} = \sum\limits_{t=1}^{\tau}diag(1-(h^{(t)})^2)\delta^{t}</script></li>
<li><p><strong>$L$ 对$U$的梯度：</strong></p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial U} = \sum\limits_{t=1}^{\tau}\frac{\partial L}{\partial h^{t}} \frac{\partial h^{t}}{\partial U} = \sum\limits_{t=1}^{\tau}diag(1-(h^{t})^2)\delta^{t}(x^{t})^T</script></li>
</ul>
]]></content>
      <tags>
        <tag>循环神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>参数初始化与正则化</title>
    <url>/post/7a6400a0.html</url>
    <content><![CDATA[<blockquote>
<p>参考文献：</p>
<p><a href="http://www.cnblogs.com/zhangchaoyang/articles/6581518.html#gv" target="_blank" rel="noopener"><a href="https://www.cnblogs.com/zhangchaoyang/articles/6581518.html" target="_blank" rel="noopener">神经网络调优</a></a></p>
<p><a href="https://blog.csdn.net/VictoriaW/article/details/73000632" target="_blank" rel="noopener">深度学习之参数初始化（一）——Xavier初始化</a></p>
</blockquote>
<h3 id="一、参数初始化"><a href="#一、参数初始化" class="headerlink" title="一、参数初始化"></a>一、参数初始化</h3><p><strong>参数初始化的作用：</strong></p>
<ul>
<li>目的是为了让神经网络在训练过程中学习到有用的信息</li>
<li><p>参数梯度不应该全部为0</p>
<ul>
<li>各层激活值不会出现饱和现象</li>
<li>各层激活值不为0</li>
</ul>
<a id="more"></a>
</li>
</ul>
<h4 id="1-1-标准初始化方法"><a href="#1-1-标准初始化方法" class="headerlink" title="1.1 标准初始化方法"></a>1.1 <strong>标准初始化方法</strong></h4><ul>
<li><p>特点：</p>
<ul>
<li>隐层的状态的均值为0，<strong>方差为常量$\frac{1}{3}$</strong>，<strong>和网络的层数无关</strong></li>
<li>标准初始化只适用于满足<strong>Glorot假设</strong>的激活函数，比如tanh。</li>
</ul>
</li>
<li><p>推导：</p>
<p>符合输入参数的均匀分布，$n$是输入层神经元个数。</p>
</li>
</ul>
<script type="math/tex; mode=display">
W_{ij} \sim U[-\frac{1}{\sqrt n}, \frac{1}{\sqrt n}]</script><p>​    权重值的方差为（均匀分布 $D(x)=(b-a)²/12$）:</p>
<script type="math/tex; mode=display">
Var(W_{ij})=\frac{1}{3n}</script><p>​    现在把输入$X$的每一维度$x$看做一个随机变量，并且假设$E(x)=0$, $Var(x)=1$。假设$w$和$x$相互独立，则隐层状态的方差为 :</p>
<script type="math/tex; mode=display">
\begin{align}
Var(z_k)= & Var(\sum_{i=0}^{n}W_{ki}x_i) \\
= &\sum_{i=0}^{n}Var(W_{ki})Var(x_i) \\
= & \sum_{i=0}^{n}Var(W_{ki}) \\
= & \sum_{i=0}^{n}\frac{1}{3n} \\
= &\frac{1}{3}
\end{align}</script><p>​    可以看出标准初始化方法得到一个非常好的特性：隐层的状态的均值为0，方差为常量$\frac{1}{3}$，和网络的层数无关，这意味着对于sigmoid函数来说，自变量落在有梯度的范围内。</p>
<h4 id="1-2-Glorot条件"><a href="#1-2-Glorot条件" class="headerlink" title="1.2 Glorot条件"></a>1.2 <strong>Glorot条件</strong></h4><p>上述参数梯度不应该全部为0的条件只能保证网络能学到东西，而Glorot认为，优秀的初始化应该使得<strong>各层的激活值</strong>和<strong>状态梯度</strong>的<strong>方差</strong>在传播过程中的方差保持一致。</p>
<ul>
<li><strong>条件：</strong><ul>
<li><strong>输入假设</strong><ul>
<li>输入的每个特征方差一样：Var(x)</li>
</ul>
</li>
<li><strong>激活函数假设：</strong><ul>
<li>激活函数$f(x)$对称：这样就可以假设每层的输入均值都是0</li>
<li>$f\prime(0)=1$</li>
<li>初始时，状态值落在激活函数的线性区域：$f\prime(s_k^i)\approx 1$</li>
</ul>
</li>
</ul>
</li>
<li><strong>符合该条件的特征</strong>：<ul>
<li><strong>激活值方差</strong>和<strong>层数相关</strong>，<strong>反向传播的梯度方差</strong>和<strong>层数是有关</strong>系的，而<strong>参数梯度的方差</strong>和<strong>层数无关</strong></li>
<li>相关推导过程: <a href="https://blog.csdn.net/VictoriaW/article/details/73000632" target="_blank" rel="noopener"><strong>Xavier初始化推导</strong></a></li>
</ul>
</li>
</ul>
<h4 id="1-3-Xavier初始化"><a href="#1-3-Xavier初始化" class="headerlink" title="1.3 Xavier初始化"></a>1.3 Xavier初始化</h4><p>为了保证前向传播和反向传播时每一层的方差一致，可以将Glorot条件转换成($n_i$为第$i$层的神经元个数)：</p>
<script type="math/tex; mode=display">
\forall i, n_iVar(W^{i+1})=1</script><script type="math/tex; mode=display">
\forall i, n_{i+1}Var(W^{i+1})=1</script><p>输入与输出的个数往往不相等，于是为了均衡考量，根据由Glorot条件，得到方差设定符合：</p>
<script type="math/tex; mode=display">
\forall i, Var(W^{i+1})=\frac{2}{n_i+n_{i+1}}</script><p>该方差对应的均匀分布即为Xavier初始化分布：</p>
<script type="math/tex; mode=display">
W\sim U\left[ -\frac{\sqrt{6}}{\sqrt{n_i + n_{i+1}}},\frac{\sqrt{6}}{\sqrt{n_i + n_{i+1}}} \right]</script><ul>
<li><strong>特点:</strong><ul>
<li>激活值的方差和层数无关，反向传播梯度的方差和层数无关</li>
</ul>
</li>
</ul>
<hr>
<h3 id="二、正则化"><a href="#二、正则化" class="headerlink" title="二、正则化"></a>二、正则化</h3><p>正则化的目的是对网络参数进行惩罚，特别是阶数较高项$x^n$ $ (网络表达式： w_1x^n+w_2x^{n-1}+…+w_n)$的系数参数，因为如果该项系数较大，是的拟合曲线复杂，会导致网络过拟合。引入正则化，则不会因为$x$的较小变动导致输出的较大变化，对噪声的容忍程度比较好。</p>
<p>没有正则化：($\eta$是学习率)</p>
<script type="math/tex; mode=display">
\begin{equation}w\to w'=w-\eta\frac{\partial C_0}{\partial w}\end{equation}</script><h4 id="2-1-L1正则-（带有特征选择能力）"><a href="#2-1-L1正则-（带有特征选择能力）" class="headerlink" title="2.1 L1正则 （带有特征选择能力）"></a>2.1 L1正则 （带有特征选择能力）</h4><p>$C$是正则化后的损失函数，$C_0$是正则化前的损失函数。</p>
<script type="math/tex; mode=display">
\begin{aligned}
&C=C_0+\frac{\lambda}{n}\sum_{i=1}^{n}|w_i| \\
\end{aligned}</script><script type="math/tex; mode=display">
\frac{\partial C}{\partial w}=\frac{\partial C_0}{\partial w} + \frac{\lambda}{n}sgn(w)</script><script type="math/tex; mode=display">
sgn(w)=\begin{equation}
            \left\{
            \begin{aligned}
                1 ,w≥1\\
                0, w=0\\
            \end{aligned}
            \right.
        \end{equation} \\</script><p>那么权重更新为：</p>
<script type="math/tex; mode=display">
w \to w-\frac{\eta\lambda}{n}sgn(w)-\eta\frac{\partial C_0}{\partial w}=w\pm\frac{\eta\lambda}{n}-\eta\frac{\partial C_0}{\partial w}</script><ul>
<li><p>L1正则的作用是使$w​$在每一次迭代时都变化一个常数：$\frac{\eta\lambda}{n}​$。</p>
</li>
<li><p>当$w​$本身比较小时，L1正则比L2正则衰减得更厉害。L1正则的效果是使不重要的$w​$几乎衰减为0。</p>
</li>
<li>因而L1具有一定的稀疏性，并有一定的特征选择的功能。</li>
</ul>
<h4 id="2-2-L2正则-与权重衰减不完全等价"><a href="#2-2-L2正则-与权重衰减不完全等价" class="headerlink" title="2.2 L2正则 (与权重衰减不完全等价)"></a>2.2 L2正则 (与权重衰减不完全等价)</h4><script type="math/tex; mode=display">
C=C_0+\frac{\lambda}{n}\sum_{i=1}^n{w_i^2}</script><p>$C_0$是正则化之前的损失函数，$λ$是正则项系数，$n$是参数$w$的个数。最小化损失函数$C$的同时也会使$\sum_{i=1}^n{w_i^2}​$尽可能小。</p>
<p>加上正则项后梯度变为:</p>
<script type="math/tex; mode=display">
\frac{\partial C}{\partial w}=\frac{\partial C_0}{\partial w}+\frac{\lambda}{2n}w</script><script type="math/tex; mode=display">
\begin{equation}w\to w'=w-\eta\frac{\partial C}{\partial w}=\left(1-\frac{\eta\lambda}{n}\right)w-\eta\frac{\partial C_0}{\partial w}\end{equation}</script><ul>
<li>L2正则项的作用是使$w$在每次迭代时都变小了$\frac{\eta\lambda}{n}$倍。</li>
<li>如果要使这个倍率不变，那么当神经元个数增多（即$n$变大）时，正则项系数$λ$也应该相应调大。</li>
</ul>
<h3 id="2-3-L2-正则-vs-权值衰减"><a href="#2-3-L2-正则-vs-权值衰减" class="headerlink" title="2.3 L2 正则 vs 权值衰减"></a>2.3 L2 正则 vs 权值衰减</h3><p>​    L2正则化是在目标函数中直接加上一个正则项，直接修改了我们的优化目标。   </p>
<p>​    权值衰减是在训练的每一步结束的时候，对网络中的参数值直接裁剪一定的比例，优化目标的式子是不变的。        </p>
<p>​    在使用朴素的梯度下降法时二者是同一个东西，因为此时L2正则化的正则项对梯度的影响就是每次使得权值衰减一定的比例。             </p>
<p>​    但是在使用一些其他优化方法的时候，就不一样了。比如说使用Adam方法时，每个参数的学习率会随着时间变化。这时如果使用L2正则化，正则项的效果也会随之变化；而如果使用权值衰减，那就与当前的学习率无关了，每次衰减的比例是固定的。</p>
]]></content>
      <tags>
        <tag>深度学习基础</tag>
      </tags>
  </entry>
  <entry>
    <title>反向传播公式推导</title>
    <url>/post/9e924caa.html</url>
    <content><![CDATA[<p><a href="https://blog.csdn.net/u014313009/article/details/51039334" target="_blank" rel="noopener">参考博客：反向传播算法（过程及公式推导）</a></p>
<h3 id="1-符号定义"><a href="#1-符号定义" class="headerlink" title="1. 符号定义"></a>1. 符号定义</h3><ul>
<li>$w_{jk}^L$ 表示第$L-1$层的第$j$个神经元到第$L$层的第$k$个神经元映射的权值。</li>
<li>$b_k^L$ 表示第$L$层的第$k$个神经元的偏置量。</li>
<li>$z_k^L=\sum_j w_{jk}^L a_j^{L-1}+b_k^L​$ 表示第$L​$层的未经激活函数的输出。</li>
<li>$a_k^{L}=\sigma(z_j^L)$ 表示第$L$层经过sigmoid函数后的输出。</li>
</ul>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1550404145/%E7%BB%98%E5%9B%BE1.png" alt=""></p>
<a id="more"></a>
<h3 id="2-损失函数定义"><a href="#2-损失函数定义" class="headerlink" title="2.  损失函数定义"></a>2.  损失函数定义</h3><p><strong>二次代价函数</strong>：($x$代表输入的样本，$y(x)$代表标签值)</p>
<script type="math/tex; mode=display">
C=\frac{1}{2n}\sum_x||y(x)-a^L(x)||^2</script><p>当只关注某一个样本$x_i​$的时候，有：</p>
<script type="math/tex; mode=display">
C=\frac{1}{2}(y-a)^2</script><h3 id="3-反向传播推导"><a href="#3-反向传播推导" class="headerlink" title="3. 反向传播推导"></a>3. 反向传播推导</h3><ul>
<li><p><strong>计算最后一层神经网络产生的错误</strong></p>
<p>对于<strong>每一个$L$层的神经元</strong>有：</p>
</li>
</ul>
<script type="math/tex; mode=display">
\delta^L_k=\frac{\partial{C}}{\partial{z_k^L}}=\frac{\partial{C}}{\partial{a_{k}^L}}\cdot\frac{\partial{a_k^L}}{\partial{z_{k}^{L}}}</script><p>​    则<strong>整个$L​$层</strong>可以用矩阵的Hadamard积(矩阵行行对应相乘)来进行计算：</p>
<script type="math/tex; mode=display">
\delta^L=\nabla_aC\odot\sigma^\prime(z^L)</script><ul>
<li><p><strong>反向传播</strong></p>
<p>计算每一层的每个神经元产生的误差，推广到每一层的每个神经元有：</p>
<script type="math/tex; mode=display">
\begin{align}
\delta_k^L=\frac{\partial C}{\partial z_k^L}&=\sum_m\frac{\partial C}{\partial z_m^{L+1}}\cdot\frac{\partial{z_m^{L+1}}}{\partial{a_{k}^{L}}}\cdot\frac{\partial a_k^L}{\partial z_k^L}\\
&=\sum_m{\delta_m^{L+1}}\cdot\frac{w_{km}^{L+1}a_k^{L}+b_m^{L+1}}{\partial{a_k^L}}\cdot\sigma'(z_k^L)\\
&=\sum_m\delta_m^{L+1}\cdot w_{km}^{L+1}\cdot\sigma'(z_k^L)
\end{align}</script><p>推广到整个一层有：</p>
<script type="math/tex; mode=display">
\delta^L=((W^{L+1})^T\cdot \delta^{L+1})\odot \sigma'(z^L)</script></li>
<li><p><strong>计算权重的梯度</strong></p>
<script type="math/tex; mode=display">
\frac{\partial C}{\partial w_{jk}^L}=\frac{\partial C}{\partial{z_k^L}}\cdot \frac{\partial{z_k^L}}{\partial{w_{jk}^L}}=\delta_k^L\cdot\frac{\partial{(w_{jk}^La_j^{L-1}+b_k^L)}}{\partial w_{jk}^L}=a_j^{L-1}\cdot\delta_k^L</script></li>
<li><p><strong>计算偏置的梯度</strong></p>
<script type="math/tex; mode=display">
\frac{\partial C}{\part b_k^L}=\frac{\partial{C}}{\partial z_{k}^L} \cdot \frac{\partial z_k^L}{\partial b_k^L}=\delta_k^L\cdot\frac{\partial{(w_{jk}^La_j^{L-1}+b_k^L)}}{\partial b_{k}^L}=\delta_k^L</script></li>
</ul>
<h3 id="4-总结"><a href="#4-总结" class="headerlink" title="4.总结"></a>4.总结</h3><p>反向传播四个公式：</p>
<ol>
<li><p>输出层误差：</p>
<script type="math/tex; mode=display">
\delta^L=\nabla_aC\odot\sigma^\prime(z^L)</script></li>
<li><p>反向传播每一层误差：</p>
<script type="math/tex; mode=display">
\delta^L=((W^{L+1})^T\cdot \delta^{L+1})\odot \sigma'(z^L)</script></li>
<li><p>权重梯度：</p>
<script type="math/tex; mode=display">
\frac{\partial C}{\partial w_{jk}^L}=a_j^{L-1}\cdot\delta_k^L</script></li>
<li><p>偏置梯度：</p>
<script type="math/tex; mode=display">
\frac{\partial C}{\partial b_k^L}=\delta_k^L</script></li>
</ol>
]]></content>
      <tags>
        <tag>深度学习基础</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习常见Loss推导</title>
    <url>/post/19fc17a4.html</url>
    <content><![CDATA[<h3 id="1-Softmax-Loss-推导"><a href="#1-Softmax-Loss-推导" class="headerlink" title="1. Softmax Loss 推导"></a>1. Softmax Loss 推导</h3><blockquote>
<p>(1) $N$为类别数</p>
<p>(2) $a$为输出向量，$a_j$为向量$a$的第$j$个值</p>
</blockquote>
<p>参考：<a href="https://blog.csdn.net/u014380165/article/details/79632950" target="_blank" rel="noopener">卷积神经网络系列之softmax loss对输入的求导推导</a></p>
<hr>
<h4 id="1-1-Softmax"><a href="#1-1-Softmax" class="headerlink" title="1.1 Softmax"></a>1.1 <strong>Softmax</strong></h4><script type="math/tex; mode=display">
S_i=\frac{e^{a_i}}{\sum_{j}e^{a_j}}</script><p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1562836327/5236230-12cd299a8d571d1e.png" alt="x"></p>
<a id="more"></a>
<h4 id="1-2-Cross-entropy-Loss"><a href="#1-2-Cross-entropy-Loss" class="headerlink" title="1.2 Cross-entropy Loss"></a>1.2 <strong>Cross-entropy Loss</strong></h4><script type="math/tex; mode=display">
L=\sum_{j}-y_ilnP_j</script><h4 id="1-3-Softmax-Loss"><a href="#1-3-Softmax-Loss" class="headerlink" title="1.3 Softmax Loss"></a>1.3 <strong>Softmax Loss</strong></h4><ul>
<li>当Cross-entropy的$P_j=S_i$  ，即Softmax输出的时候。</li>
</ul>
<script type="math/tex; mode=display">
L_{softmax}=\sum_{j}-y_ilnS_i</script><h4 id="1-4-Softmax对Softmax输入的导数"><a href="#1-4-Softmax对Softmax输入的导数" class="headerlink" title="1.4 Softmax对Softmax输入的导数"></a>1.4 <strong>Softmax对Softmax输入的导数</strong></h4><ul>
<li>$S_i$对$a_j$求导：</li>
</ul>
<script type="math/tex; mode=display">
\frac{\partial{S_i}}{\partial{a_j}}=\frac{\partial{\frac{e^{a_i}}{\sum_ke^{a_k}}}}{\partial a_j}</script><ul>
<li><p>这里求导有两种情况</p>
<ul>
<li><p>1）当$i=j$时：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial{S_i}}{\partial{a_j}} 
&=\frac{\partial{S_i}}{\partial{a_i}} \\
&=\frac{\frac{\partial{e^{a_i}}}{\partial{a_i}}\sum_ke^{a_k}-\frac{\sum_ke^{a_k}}{\partial{a_i}}e^{a_i}}{(\sum_ke^{a_k})^2} \\ 
&=\frac{e^{a_i}\sum_ke^{a_k}-e^{a_i}e^{a_i}}{\sum_k{e^{2a_k}}} \\ 
&=\frac{e^{a_i}}{\sum_k{e^{a_k}}}-\frac{e^{a_i}}{\sum_k{e^{a_k}}}\cdot{\frac{e^{a_i}}{\sum_k{e^{a_k}}}} \\ 
&=S_i-S_i^2\\
&=S_i(1-S_i)
\end{aligned}</script></li>
<li><p>2）当$i≠j$时：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial{S_i}}{\partial{a_j}} 
&=\frac{\frac{\partial{e^{a_i}}}{\partial{a_j}}\sum_ke^{a_k}-\frac{\sum_ke^{a_k}}{\partial{a_j}}e^{a_i}}{(\sum_ke^{a_k})^2} \\ 
&=\frac{-e^{a_j}e^{a_i}}{\sum_k{e^{2a_k}}} \\
&=-S_jS_i
\end{aligned}</script></li>
</ul>
</li>
</ul>
<h4 id="1-5-Softmax-Loss对softmax输入的导数"><a href="#1-5-Softmax-Loss对softmax输入的导数" class="headerlink" title="1.5 Softmax Loss对softmax输入的导数"></a>1.5 Softmax Loss对softmax输入的导数</h4><ul>
<li>第③个等号就用到了上面$S_i$对$a_j$求导的结论，第三个等号结果的左半部分是$i=k$的时候$S_i$对$a_j$求导的导数，右半部分是$i≠k$的时候S_i$对​$a_j$求导的导数。</li>
<li>第⑥、⑦个等号是将$y_iS_i​$合并到∑里面。最后一个等号的成立是建立在假设$∑y_k=1​$的前提下，这个对于常见的单标签分类任务而言都是成立的。 </li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial{L_{softmax}}}{\partial{a_k}}&=-\sum_ky_k\frac{\partial{lnS_k}}{\partial{a_i}} \\
&=-\sum_ky_k\frac{1}{S_k}\frac{\partial{S_k}}{\partial{a_i}} \\
&=-y_i(1-S_i)-\sum_{k≠i}y_k\frac{1}{S_k}({-S_kS_i}) \\
&=-y_i(1-S_i)+\sum_{k≠i}y_kS_i \\
&=-y_i+y_iS_i+\sum_{k≠i}y_kS_i \\
&=\sum_ky_kS_i-y_i \\
&=S_i-y_i


\end{aligned}</script><h4 id="1-6-总结"><a href="#1-6-总结" class="headerlink" title="1.6 总结"></a>1.6 <strong>总结</strong></h4><p>因此假设一个5分类任务，经过Softmax层后得到的概率向量$S$是$[0.1,0.2,0.25,0.4,0.05]$，真实标签$y$是$[0,0,1,0,0]$，那么损失回传时该层得到的梯度就是$p-y=[0.1,0.2,-0.75,0.4,0.05]$。这个梯度就指导网络在下一次forward的时候更新该层的权重参数。</p>
]]></content>
      <tags>
        <tag>深度学习基础</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习优化器</title>
    <url>/post/1eda57ac.html</url>
    <content><![CDATA[<p>参考资料：<a href="https://blog.csdn.net/u012759136/article/details/52302426" target="_blank" rel="noopener">深度学习最全优化方法总结比较（SGD，Adagrad，Adadelta，Adam，Adamax，Nadam）</a></p>
<h3 id="1-随机梯度下降"><a href="#1-随机梯度下降" class="headerlink" title="1. 随机梯度下降"></a>1. 随机梯度下降</h3><ul>
<li><strong>批梯度下降(Gradient Descent)</strong></li>
<li><strong>随机批梯度下降(Stotastic Gradient Descent)</strong><ul>
<li>每次梯度计算只使用一个随机样本(可能是噪声样本)<ul>
<li>避免在类似的样本上进行冗余计算</li>
<li>增加了跳出当前局部最小值的可能</li>
<li>可以通过减小学习率，来使其能够与GD有相同的收敛速度</li>
</ul>
</li>
</ul>
</li>
<li><strong>小批量随机梯度下降(Mini batch SGD)</strong><ul>
<li>每次梯度计算使用小批量的样本<ul>
<li>梯度计算比单样本计算更加稳定</li>
<li>便于使用矩阵计算</li>
<li>适当的batch size训练效率高</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_decent</span><span class="params">(_x_data, _y_data, _b, _w, _iteration, _lr)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Gradient Decent</span></span><br><span class="line"><span class="string">    :param _x_data:</span></span><br><span class="line"><span class="string">    :param _y_data:</span></span><br><span class="line"><span class="string">    :param _b:</span></span><br><span class="line"><span class="string">    :param _w:</span></span><br><span class="line"><span class="string">    :param _iteration:</span></span><br><span class="line"><span class="string">    :param _lr:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    _b_history = [_b]</span><br><span class="line">    _w_history = [_w]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> _i <span class="keyword">in</span> range(_iteration):</span><br><span class="line">        b_grad = <span class="number">0.0</span></span><br><span class="line">        w_grad = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> _n <span class="keyword">in</span> range(len(x_data)):</span><br><span class="line">            b_grad = b_grad - <span class="number">2.0</span> * (y_data[_n] - _b - _w * _x_data[_n]) * <span class="number">1.0</span></span><br><span class="line">            w_grad = w_grad - <span class="number">2.0</span> * (y_data[_n] - _b - _w * _x_data[_n]) * _x_data[_n]</span><br><span class="line"></span><br><span class="line">        _b = _b - _lr * b_grad</span><br><span class="line">        _w = _w - _lr * w_grad</span><br><span class="line"></span><br><span class="line">        _b_history.append(_b)</span><br><span class="line">        _w_history.append(_w)</span><br><span class="line">    <span class="keyword">return</span> _b_history, _w_history</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<hr>
<h3 id="2-随机梯度下降的困难"><a href="#2-随机梯度下降的困难" class="headerlink" title="2. 随机梯度下降的困难"></a>2. 随机梯度下降的困难</h3><ul>
<li>局部梯度的反方向不一定是函数整体的下降方向。</li>
<li>学习率衰减法，难以根据数据进行自适应。</li>
<li>对不同的参数采取不同的学习率(数据稀疏，不平衡)。</li>
<li>容易困在局部最小点，甚至是鞍点。</li>
</ul>
<hr>
<h3 id="3-动量方法-Momentum"><a href="#3-动量方法-Momentum" class="headerlink" title="3. 动量方法(Momentum)"></a>3. 动量方法(Momentum)</h3><ul>
<li><p><strong>目的:</strong> 解决随机梯度的局部梯度的反方向不一定是函数整体的下降方向问题。</p>
</li>
<li><p><strong>方法：</strong></p>
<ul>
<li><p><strong>动量法：(Momentum)</strong>（适用于隧道型曲面）</p>
<ul>
<li><p>方法：</p>
<p>每次更新都吸收上一次更新的余势。使得主体方向得到了更好的保留，使得效果被不断的放大。</p>
<script type="math/tex; mode=display">
v_t = \gamma v_{t-1}+\eta\nabla_{\theta}J(\theta) \\
\theta_t=\theta_{t-1}-v_t</script></li>
<li><p>缺点：</p>
<p>在前期下降比较快，收敛速度较好，但到最优值附近时容易由于动量过大而导致优化过度。</p>
</li>
</ul>
</li>
<li><p><strong>改进动量法：(Nesterov)</strong></p>
<ul>
<li>方法：利用主体的下降方向，<strong>预判下一步优化的位置</strong>，根据预判的位置计算优化的梯度。<script type="math/tex; mode=display">
v_t = \gamma v_{t-1} + \eta \nabla_{\theta}J(\theta-\gamma v_{t-1}) \\
\theta=\theta-v_t</script>momentum首先计算一个梯度(短的蓝色向量)，然后在加速更新梯度的方向进行一个大的跳跃(长的蓝色向量)，nesterov项首先在之前加速的梯度方向进行一个大的跳跃(棕色向量)，计算梯度然后进行校正(绿色梯向量)<br><img src="https://img-blog.csdn.net/20160824170803993" alt=""></li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="4-自适应梯度方法-Ada"><a href="#4-自适应梯度方法-Ada" class="headerlink" title="4. 自适应梯度方法(Ada)"></a>4. 自适应梯度方法(Ada)</h3><ul>
<li><p><strong>目的：</strong></p>
<ul>
<li>解决学习率衰减法，难以根据数据进行自适应的问题。</li>
<li>更新频繁的参数使用较小的学习率。</li>
<li>更新较少的参数使用较大的学习率。</li>
</ul>
</li>
<li><p><strong>方法：</strong></p>
<ul>
<li><p><strong>Adagrad方法：</strong></p>
<ul>
<li><p><strong>思路：</strong>Adagrad对每个参数的历史梯度更新进行叠加，并以此作为下一次更新的惩罚系数。（约束学习率）</p>
</li>
<li><p><strong>算法：</strong></p>
<ul>
<li>梯度：$g_{t,i}=\nabla_{\theta}J(\theta_i)$</li>
<li>梯度历史矩阵: $G_t$是对角阵，其中$G_{t,ii}=\sum_{k}g_{k,i}^2$</li>
<li>参数更新：（历史梯度大，则$\eta$项越小）</li>
</ul>
<script type="math/tex; mode=display">
\theta_{t+1,i}=\theta_{t,i}-\frac{\eta}{\sqrt{G_{t,ii}+\epsilon}}\cdot g_{t, i}</script></li>
<li><p><strong>存在的问题：</strong></p>
<ul>
<li><ol>
<li>随着训练的进行，学习率衰减过快。</li>
</ol>
</li>
<li><ol>
<li>梯度与参数单位不匹配</li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adagrad</span><span class="params">(_x_data, _y_data, _b, _w, _iteration, _lr)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Adagrad: auto adapt the learning rate.</span></span><br><span class="line"><span class="string">    root mean square</span></span><br><span class="line"><span class="string">    :param _x_data:</span></span><br><span class="line"><span class="string">    :param _y_data:</span></span><br><span class="line"><span class="string">    :param _b:</span></span><br><span class="line"><span class="string">    :param _w:</span></span><br><span class="line"><span class="string">    :param _iteration:</span></span><br><span class="line"><span class="string">    :param _lr:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    _b_history = [_b]</span><br><span class="line">    _w_history = [_w]</span><br><span class="line">    lr_b = <span class="number">0</span></span><br><span class="line">    lr_w = <span class="number">0</span></span><br><span class="line">    epsilon = <span class="number">0.01</span></span><br><span class="line">    <span class="keyword">for</span> _i <span class="keyword">in</span> range(_iteration):</span><br><span class="line">        b_grad = <span class="number">0.0</span></span><br><span class="line">        w_grad = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> _n <span class="keyword">in</span> range(len(x_data)):</span><br><span class="line">            b_grad = b_grad - <span class="number">2.0</span> * (y_data[_n] - _b - _w * _x_data[_n]) * <span class="number">1.0</span></span><br><span class="line">            w_grad = w_grad - <span class="number">2.0</span> * (y_data[_n] - _b - _w * _x_data[_n]) * _x_data[_n]</span><br><span class="line"></span><br><span class="line">        lr_b = lr_b + b_grad ** <span class="number">2</span></span><br><span class="line">        lr_w = lr_w + w_grad ** <span class="number">2</span></span><br><span class="line">        _b = _b - _lr / (np.sqrt(lr_b) + epsilon) * b_grad</span><br><span class="line">        _w = _w - _lr / (np.sqrt(lr_w) + epsilon) * w_grad</span><br><span class="line"></span><br><span class="line">        _b_history.append(_b)</span><br><span class="line">        _w_history.append(_w)</span><br><span class="line">    <span class="keyword">return</span> _b_history, _w_history</span><br></pre></td></tr></table></figure>
<ul>
<li><p><strong>RMSprop（Adadelta方法第一版）：</strong></p>
<ul>
<li><p><strong>目的：</strong>解决随着训练的进行，学习率衰减过快。</p>
</li>
<li><p><strong>思路：</strong>使用<strong>梯度平方的移动平均</strong>来取代全部的历史平方和。</p>
</li>
<li><p><strong>算法：</strong></p>
<ul>
<li><p>梯度：$g_{t,i}=\nabla_{\theta}J(\theta_i)$</p>
</li>
<li><p>移动平均: $\mathbb{E}_{t}[g^2]=\gamma \mathbb{E}_{t-1}[g^2] + (1-\gamma) g_{t}^2$</p>
</li>
<li><p>参数更新：（更新系数分母换了）</p>
<script type="math/tex; mode=display">
\theta_{t+1, i} = \theta_{t,i} - \frac{\eta}{\sqrt{\mathbb{E}_{t,ii}+\epsilon}} \cdot g_{t,i}</script></li>
</ul>
</li>
<li><p><strong>特点：</strong></p>
<ul>
<li>其实RMSprop依然依赖于全局学习率</li>
<li>RMSprop算是Adagrad的一种发展，和Adadelta的变体，效果趋于二者之间</li>
<li>适合处理非平稳目标</li>
<li>对于RNN效果很好</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong>Adadelta方法第二版:</strong></p>
<ul>
<li><p><strong>目的：</strong>梯度与参数单位不匹配</p>
</li>
<li><p><strong>思路</strong>：使用参数更新的移动平均来取代学习率$\eta$</p>
</li>
<li><p><strong>算法</strong>：</p>
<ul>
<li>参数更新: （学习率换成参数的移动平均自适应）<script type="math/tex; mode=display">
\theta_{t+1, i} = \theta_{t,i} - \frac{\sqrt{\mathbb{E}_{t-1}[\Delta \theta]}}{\sqrt{\mathbb{E}_{t,ii}+\epsilon}} \cdot g_{t,i}</script></li>
</ul>
</li>
<li><p><strong>特点：</strong></p>
<ul>
<li>训练初中期，加速效果不错，很快</li>
<li>训练后期，反复在局部最小值附近抖动</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="5-动量-自适应方法-Adam"><a href="#5-动量-自适应方法-Adam" class="headerlink" title="5. 动量+自适应方法(Adam)"></a>5. 动量+自适应方法(Adam)</h3><ul>
<li><p><strong>Adam （带动量项的RMSprop）</strong></p>
<ul>
<li><p><strong>思路</strong>：在Adadelta的梯度平方和(二阶矩)的基础上引入动量方法的的一阶矩(梯度)</p>
</li>
<li><p><strong>算法</strong>：</p>
<ul>
<li>一阶矩(动量)： $m_t=\beta_1 m_{t-1}+(1-\beta_1)g_t$ （保持下降速度）</li>
<li>二阶矩(Adadelta)：$v_t=\beta_2 v_{t-1}+(1-\beta_2)g_t^2$ （保持参数自适应）</li>
<li>参数更新：</li>
</ul>
<script type="math/tex; mode=display">
\theta_{t+1}=\theta_{t} - \frac{\eta}{\sqrt{v_{t}}+\epsilon}m_t</script></li>
<li><p><strong>特点</strong>：</p>
<ul>
<li>经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。</li>
<li>适用于大数据集和高维空间</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>NAdam</strong> (引入Nesterov)</p>
<ul>
<li>对学习率有了更强的约束</li>
</ul>
</li>
</ul>
<hr>
<h3 id="6-小结"><a href="#6-小结" class="headerlink" title="6. 小结"></a>6. 小结</h3><ul>
<li>对于稀疏数据，尽量使用学习率可自适应的优化方法，不用手动调节，而且最好采用默认值</li>
<li>SGD通常训练时间更长，容易陷入鞍点，但是在好的初始化和学习率调度方案的情况下，结果更可靠</li>
<li>如果在意更快的收敛，并且需要训练较深较复杂的网络时，推荐使用学习率自适应的优化方法。</li>
<li>Adadelta，RMSprop，Adam是比较相近的算法，在相似的情况下表现差不多。</li>
<li>在想使用带动量的RMSprop，或者Adam的地方，大多可以使用Nadam取得更好的效果</li>
</ul>
<p><img src="https://img-blog.csdn.net/20160824161755284" alt="x"></p>
<p>鞍点：</p>
<p><img src="https://img-blog.csdn.net/20160824161815758" alt="x"></p>
]]></content>
      <tags>
        <tag>深度学习基础</tag>
      </tags>
  </entry>
  <entry>
    <title>刷题-搜索</title>
    <url>/post/d42edc1.html</url>
    <content><![CDATA[<h3 id="1-Leetcode-200-岛屿的个数"><a href="#1-Leetcode-200-岛屿的个数" class="headerlink" title="1. [Leetcode 200] 岛屿的个数"></a>1. [Leetcode 200] <a href="https://leetcode-cn.com/problems/number-of-islands/" target="_blank" rel="noopener">岛屿的个数</a></h3><ul>
<li><p>解法： </p>
<ul>
<li>DFS：一次遍历，遇到1则进行深搜，遍历过的1设为2，遇到非1或者越界则深搜停止，统计深搜的次数，即为岛屿数量。(这里采用)</li>
<li>BFS: 循环遍历每个点，如果该点是0，则跳过，如果是1，岛屿数目加1，并加入队列，并将该点改为0，避免重复访问，然后进行广搜，取出队首元素，搜索该点周围四个点，如果有1就加入队列，并将1改为0，否则就跳过，当队列空时，一块岛屿搜索完毕，进入下一块搜索。(代码可参考：<a href="https://blog.csdn.net/qq_41822647/article/details/85780488" target="_blank" rel="noopener">岛屿的个数 BFS</a>)  </li>
<li><a href="https://www.bilibili.com/video/av38498175/?p=1" target="_blank" rel="noopener">并查集</a>: 将二维数组重新编号，从<code>0</code>开始，从左到右，从上到下，直到<code>n*m-1</code>（其中<code>n</code>为行数，<code>m</code>为列数），对于位置<code>(i,j)</code>则编号为<code>i*m+j</code>，那么相邻（左右，上下）的为同一个值，则认为他们相通。那么最终只要统计一下<code>father[i]==i</code>且对应值为<code>1</code>的个数即可。(代码可参考: <a href="https://blog.csdn.net/hi_baymax/article/details/82585480" target="_blank" rel="noopener">岛屿的个数 Disjoint Set</a>)</li>
</ul>
</li>
<li><p>代码：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">numIslands</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">char</span>&gt;&gt;&amp; grid)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(grid.empty()) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> res = <span class="number">0</span>;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; grid.size(); i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; grid[<span class="number">0</span>].size(); j++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(grid[i][j] == <span class="string">'1'</span>)&#123;</span><br><span class="line">                    dfs(grid, i, j);</span><br><span class="line">                    res++;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">char</span>&gt;&gt;&amp; grid, <span class="keyword">int</span> row, <span class="keyword">int</span> col)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(row &lt; <span class="number">0</span> || col &lt; <span class="number">0</span> || row &gt;= grid.size() || col &gt;= grid[<span class="number">0</span>].size() || grid[row][col] == <span class="string">'0'</span> || grid[row][col] == <span class="string">'2'</span>)</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span>(grid[row][col] == <span class="string">'1'</span>) grid[row][col] = <span class="string">'2'</span>;</span><br><span class="line">        </span><br><span class="line">        dfs(grid, row<span class="number">-1</span>, col);</span><br><span class="line">        dfs(grid, row+<span class="number">1</span>, col);</span><br><span class="line">        dfs(grid, row, col<span class="number">-1</span>);</span><br><span class="line">        dfs(grid, row, col+<span class="number">1</span>);</span><br><span class="line">                </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">   </span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="2-Leetcode-77-组合"><a href="#2-Leetcode-77-组合" class="headerlink" title="2. [Leetcode 77] 组合"></a>2. [Leetcode 77] <a href="https://leetcode-cn.com/problems/combinations/" target="_blank" rel="noopener">组合</a></h3><ul>
<li><p>模板题：重点在于dfs中的startPos=i+1</p>
</li>
<li><p>代码：</p>
<pre><code><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; combine(<span class="keyword">int</span> n, <span class="keyword">int</span> k) &#123;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; res;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; inRes;</span><br><span class="line">        <span class="keyword">if</span>(n&lt;=<span class="number">0</span> || k&lt;=<span class="number">0</span>) <span class="keyword">return</span> &#123;&#125;;</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        dfs(res, inRes, n, k, <span class="number">1</span>);</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; &amp;res, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;inRes, <span class="keyword">int</span> n, <span class="keyword">int</span> k, <span class="keyword">int</span> startPos)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(inRes.size()==k)&#123;</span><br><span class="line">            res.emplace_back(inRes);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">            </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = startPos; i&lt;=n; i++)&#123;</span><br><span class="line">            inRes.emplace_back(i);</span><br><span class="line">            dfs(res, inRes, n, k, i+<span class="number">1</span>); <span class="comment">//重点在于i+1</span></span><br><span class="line">            inRes.pop_back();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
</code></pre></li>
</ul>
<h3 id="3-Leetcode-39-组合总和-可重复选取数字"><a href="#3-Leetcode-39-组合总和-可重复选取数字" class="headerlink" title="3. [Leetcode 39]  组合总和(可重复选取数字)"></a>3. [Leetcode 39] <a href="https://leetcode-cn.com/problems/combination-sum/" target="_blank" rel="noopener"> 组合总和(可重复选取数字)</a></h3><ul>
<li>基于模板加一些改动即可，由于可重复选择数字，将startPos=i+1 改成startPos=i，判停条件满足目标和就可以了,(前提是有序数组，先拍下序)</li>
<li>代码：</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; combinationSum(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; candidates, <span class="keyword">int</span> target) &#123;</span><br><span class="line">        <span class="keyword">if</span>(candidates.size() &lt;= <span class="number">0</span>) <span class="keyword">return</span> &#123;&#125;;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; res;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; tmp;</span><br><span class="line">        sort(candidates.begin(), candidates.end()); <span class="comment">//保证有序</span></span><br><span class="line">        dfs(res, target, tmp, candidates, <span class="number">0</span>);</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; &amp;res,<span class="keyword">int</span> target, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; tmp, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; candidates, <span class="keyword">int</span> startPos)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">auto</span> i : tmp) </span><br><span class="line">            sum += i;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (sum &gt; target)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (sum == target) <span class="comment">//求和判停</span></span><br><span class="line">        &#123;</span><br><span class="line">            res.emplace_back(tmp);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = startPos; i &lt; candidates.size(); i++)</span><br><span class="line">        &#123;</span><br><span class="line">            tmp.emplace_back(candidates[i]);</span><br><span class="line">            dfs(res, target, tmp, candidates, i);</span><br><span class="line">            tmp.pop_back();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h3 id="4-Leetcode-40-组合总和-II-不重复"><a href="#4-Leetcode-40-组合总和-II-不重复" class="headerlink" title="4. [Leetcode 40] 组合总和 II (不重复)"></a>4. [Leetcode 40] <a href="https://leetcode-cn.com/problems/combination-sum-ii/" target="_blank" rel="noopener">组合总和 II (不重复)</a></h3><ul>
<li><p>解法：在上一题的基础上使用Set进行去重</p>
</li>
<li><p>代码:</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; combinationSum2(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; candidates, <span class="keyword">int</span> target) &#123;</span><br><span class="line">        <span class="keyword">if</span> (candidates.size() &lt;= <span class="number">0</span>) <span class="keyword">return</span> &#123;&#125;;</span><br><span class="line">        <span class="built_in">set</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; res;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; curr;</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        sort(candidates.begin(), candidates.end());</span><br><span class="line">        dfs(res, target, <span class="number">0</span>, curr, candidates);</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;(res.begin(), res.end());</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="built_in">set</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; &amp;res, <span class="keyword">int</span> target, <span class="keyword">int</span> s, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; curr, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; candidates)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(target == <span class="number">0</span>) </span><br><span class="line">        &#123;</span><br><span class="line">            res.insert(curr);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = s; i &lt; candidates.size(); i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(target - candidates[i] &lt; <span class="number">0</span>) <span class="keyword">break</span>;</span><br><span class="line">            curr.emplace_back(candidates[i]);</span><br><span class="line">            dfs(res, target - candidates[i], i+<span class="number">1</span>, curr, candidates);</span><br><span class="line">            curr.pop_back();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="5-Leetcode-216-组合总和-III"><a href="#5-Leetcode-216-组合总和-III" class="headerlink" title="5.[Leetcode 216] 组合总和 III"></a>5.[Leetcode 216] <a href="https://leetcode-cn.com/problems/combination-sum-iii/" target="_blank" rel="noopener">组合总和 III</a></h3><ul>
<li>解法:</li>
<li>代码</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; combinationSum3(<span class="keyword">int</span> k, <span class="keyword">int</span> n) &#123;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; nums;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; res;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; curr;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">9</span>; i++)</span><br><span class="line">            nums.emplace_back(i+<span class="number">1</span>);</span><br><span class="line">        dfs(res, k, n, <span class="number">0</span>, <span class="number">0</span>, curr, nums);</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; &amp;res, <span class="keyword">int</span> k, <span class="keyword">int</span> target, <span class="keyword">int</span> s, <span class="keyword">int</span> depth, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; curr, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(target == <span class="number">0</span> &amp;&amp; depth == k)</span><br><span class="line">        &#123;</span><br><span class="line">            res.emplace_back(curr);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = s; i &lt; nums.size(); i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(target - nums[i] &lt; <span class="number">0</span>) <span class="keyword">break</span>;</span><br><span class="line">            curr.emplace_back(nums[i]);</span><br><span class="line">            dfs(res, k, target - nums[i], i+<span class="number">1</span> , depth+<span class="number">1</span> , curr, nums);</span><br><span class="line">            curr.pop_back();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h3 id="6-Leetcode-131-分割回文串"><a href="#6-Leetcode-131-分割回文串" class="headerlink" title="6.[Leetcode 131] 分割回文串"></a>6.[Leetcode 131] <a href="https://leetcode-cn.com/problems/palindrome-partitioning/" target="_blank" rel="noopener">分割回文串</a></h3><ul>
<li><p>解法:</p>
<p>按解组合的方式解，解的过程中判断回文串，组合的元素集合是按照从子串的第一个元素开始，到第i个元素截断作为一个新子串进行回文判断，</p>
</li>
<li><p>代码：</p>
</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&gt; partition(<span class="built_in">string</span> s) &#123;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="built_in">string</span> &gt;&gt; res;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; tmp;</span><br><span class="line">        </span><br><span class="line">        dfs(res, tmp, <span class="number">0</span>, s.length()<span class="number">-1</span>, s);</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span> &lt;<span class="built_in">string</span>&gt; &gt;&amp; res, <span class="built_in">vector</span>&lt;<span class="built_in">string</span> &gt;&amp; tmp, <span class="keyword">int</span> startPos, <span class="keyword">int</span> endPos, <span class="built_in">string</span> &amp;s)</span></span>&#123;</span><br><span class="line">        <span class="comment">//当原字符串都遍历完成后，结束一次搜索。</span></span><br><span class="line">        <span class="keyword">if</span>(startPos &gt; endPos)&#123;</span><br><span class="line">            res.emplace_back(tmp);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//遍历子串，从0~1，0~2 ... 到0~n, 递归的起始位置为截断后的第一位</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i =<span class="number">1</span>; i &lt;= endPos-startPos+<span class="number">1</span>; i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(isPalindrome(s.substr(startPos, i)))&#123;</span><br><span class="line">                tmp.emplace_back(s.substr(startPos, i));</span><br><span class="line">                <span class="comment">//递归的起始位置为截断后的第一位</span></span><br><span class="line">                dfs(res, tmp, startPos+i, endPos, s);</span><br><span class="line">                tmp.pop_back();</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 判断回文串，这里用的是可解决大小写字母，数字的，过滤其他符号。</span></span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">isPalindrome</span><span class="params">(<span class="built_in">string</span> s)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> pStart = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> pEnd = s.length()<span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">while</span>(pStart &lt; pEnd)&#123;</span><br><span class="line">            <span class="keyword">while</span> (!checkChar(s[pStart]) &amp;&amp; pStart &lt; pEnd)</span><br><span class="line">				pStart++;</span><br><span class="line">			<span class="keyword">while</span> (!checkChar(s[pEnd]) &amp;&amp; pStart &lt; pEnd)</span><br><span class="line">				pEnd--;</span><br><span class="line"></span><br><span class="line">			toLower(s[pStart]);</span><br><span class="line">			toLower(s[pEnd]);</span><br><span class="line">           </span><br><span class="line">            <span class="keyword">if</span>(s[pStart] == s[pEnd])&#123;</span><br><span class="line">                pStart ++;</span><br><span class="line">                pEnd--;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">               </span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">toLower</span><span class="params">(<span class="keyword">char</span>&amp; s)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(s &gt;= <span class="string">'A'</span> &amp;&amp; s &lt;= <span class="string">'Z'</span>)</span><br><span class="line">            s = s+<span class="number">32</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">	<span class="function"><span class="keyword">bool</span> <span class="title">checkChar</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span>&amp; s)</span></span>&#123;</span><br><span class="line">		<span class="keyword">if</span> ((s &gt;= <span class="string">'a'</span> &amp;&amp; s &lt;= <span class="string">'z'</span>) || (s &gt;= <span class="string">'0'</span> &amp;&amp; s &lt;= <span class="string">'9'</span>) || (s &gt;= <span class="string">'A'</span> &amp;&amp; s &lt;= <span class="string">'Z'</span>))</span><br><span class="line">			<span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h3 id="7-机器人运动范围"><a href="#7-机器人运动范围" class="headerlink" title="7.机器人运动范围"></a>7.<a href="https://www.nowcoder.com/practice/6e5207314b5241fb83f2329e89fdecc8?tpId=13&amp;tqId=11219&amp;tPage=4&amp;rp=1&amp;ru=%2Fta%2Fcoding-interviews&amp;qru=%2Fta%2Fcoding-interviews%2Fquestion-ranking" target="_blank" rel="noopener">机器人运动范围</a></h3><ul>
<li><p>规规矩矩没啥说的, 主要注意边界条件</p>
</li>
<li><p>代码:</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">movingCount</span><span class="params">(<span class="keyword">int</span> threshold, <span class="keyword">int</span> rows, <span class="keyword">int</span> cols)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> res = <span class="number">0</span>;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; visited (rows, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;(cols, <span class="number">0</span>));</span><br><span class="line">        Dfs(threshold, rows, cols, <span class="number">0</span>, <span class="number">0</span>, res, visited);</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">Dfs</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span>&amp; threshold, <span class="keyword">const</span> <span class="keyword">int</span>&amp; rows, <span class="keyword">const</span> <span class="keyword">int</span>&amp; cols, <span class="keyword">int</span> rowIdx, <span class="keyword">int</span> colIdx, <span class="keyword">int</span>&amp; res, <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; visited)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (JudgeOverThreshold(threshold, rowIdx, colIdx) || JudgeOutside(rows, cols, rowIdx, colIdx) || visited[rowIdx][colIdx] )</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        visited[rowIdx][colIdx] = <span class="number">1</span>;</span><br><span class="line">        res ++;</span><br><span class="line">        Dfs(threshold, rows, cols, rowIdx+<span class="number">1</span>, colIdx, res, visited);</span><br><span class="line">        Dfs(threshold, rows, cols, rowIdx, colIdx+<span class="number">1</span>, res, visited);</span><br><span class="line">        Dfs(threshold, rows, cols, rowIdx<span class="number">-1</span>, colIdx, res, visited);</span><br><span class="line">        Dfs(threshold, rows, cols, rowIdx, colIdx<span class="number">-1</span>, res, visited);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">JudgeOverThreshold</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span>&amp; threshold, <span class="keyword">int</span> rowIdx, <span class="keyword">int</span> colIdx)</span></span>&#123;</span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span>(rowIdx)&#123;</span><br><span class="line">            sum += rowIdx%<span class="number">10</span>;</span><br><span class="line">            rowIdx /= <span class="number">10</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span>(colIdx)&#123;</span><br><span class="line">            sum += colIdx%<span class="number">10</span>;</span><br><span class="line">            colIdx /= <span class="number">10</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(sum &gt; threshold) <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">JudgeOutside</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span>&amp; rows, <span class="keyword">const</span> <span class="keyword">int</span>&amp; cols, <span class="keyword">const</span> <span class="keyword">int</span>&amp; rowIdx, <span class="keyword">const</span> <span class="keyword">int</span>&amp; colIdx)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(rowIdx &lt; <span class="number">0</span> || rowIdx &gt; rows<span class="number">-1</span> || colIdx &lt; <span class="number">0</span>  || colIdx &gt; cols<span class="number">-1</span>) <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
</li>
</ul>
]]></content>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>/post/0.html</url>
    <content><![CDATA[<p>title: “[刷题]链表专题”<br>tags: 数据结构与算法<br>abbrlink: 6db30a46<br>date: 2019-02-12 21:23:22</p>
<h3 id="Leetcode-141-142-环形链表，环形链表-II-（剑指Offer面试题23）"><a href="#Leetcode-141-142-环形链表，环形链表-II-（剑指Offer面试题23）" class="headerlink" title="[Leetcode 141, 142] 环形链表，环形链表 II （剑指Offer面试题23）"></a>[Leetcode 141, 142] <a href="https://leetcode-cn.com/problems/linked-list-cycle/" target="_blank" rel="noopener">环形链表</a>，<a href="https://leetcode-cn.com/problems/linked-list-cycle-ii/" target="_blank" rel="noopener">环形链表 II</a> （剑指Offer面试题23）</h3><ul>
<li><p>解法：(链表遍历，快慢指针)</p>
<ul>
<li>判环：快慢指针，快指针+2， 慢指针+1，若快指针在到达链表尾(不带环的才有链表尾)之前与慢指针相遇，则有环。</li>
<li>找入口：快慢指针第一次相遇节点与头结点之间的节点数与环中节点数相同，两个指针一个从链表头开始，一个从相遇节点开始遍历，两个指针再次相遇节点为入口节点。</li>
</ul>
<a id="more"></a>
</li>
<li><p>代码：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"> * struct ListNode &#123;</span></span><br><span class="line"><span class="comment"> *     int val;</span></span><br><span class="line"><span class="comment"> *     ListNode *next;</span></span><br><span class="line"><span class="comment"> *     ListNode(int x) : val(x), next(NULL) &#123;&#125;</span></span><br><span class="line"><span class="comment"> * &#125;;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 找入口</span></span><br><span class="line">    <span class="function">ListNode *<span class="title">detectCycle</span><span class="params">(ListNode *head)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(!JudgeCycle(head)) <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">        </span><br><span class="line">        ListNode *pFirstptr = head;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//第一次相遇节点位置就是环内节点数</span></span><br><span class="line">        <span class="keyword">while</span>(pFirstptr != pMeetNode)&#123;</span><br><span class="line">            pFirstptr = pFirstptr -&gt; next;</span><br><span class="line">            pMeetNode = pMeetNode -&gt; next;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> pFirstptr;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 判环</span></span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">JudgeCycle</span><span class="params">(ListNode *head)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(head == <span class="literal">nullptr</span>) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        </span><br><span class="line">        ListNode *pFastptr = head;</span><br><span class="line">        ListNode *pSlowptr = head;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span>(pFastptr -&gt; next &amp;&amp; pFastptr -&gt; next -&gt; next)&#123;</span><br><span class="line">            pFastptr = pFastptr -&gt; next -&gt; next;</span><br><span class="line">            pSlowptr = pSlowptr -&gt; next;</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span>(pFastptr == pSlowptr)&#123;</span><br><span class="line">                pMeetNode = pFastptr;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    ListNode *pMeetNode;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Leetcode-206-反转链表-剑指Offer面试题24"><a href="#Leetcode-206-反转链表-剑指Offer面试题24" class="headerlink" title="[Leetcode 206] 反转链表 (剑指Offer面试题24)"></a>[Leetcode 206] <a href="https://leetcode-cn.com/problems/reverse-linked-list/" target="_blank" rel="noopener">反转链表</a> (剑指Offer面试题24)</h3><ul>
<li>解法： </li>
</ul>
<ul>
<li><p>代码：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">ListNode* <span class="title">ReverseList</span><span class="params">(ListNode* pHead)</span> </span>&#123;</span><br><span class="line">        ListNode* h = <span class="literal">NULL</span>;</span><br><span class="line">        ListNode* p = pHead;</span><br><span class="line">        <span class="keyword">while</span>(p)&#123;</span><br><span class="line">            ListNode* tmp = p -&gt; next;</span><br><span class="line">            p -&gt; next = h;</span><br><span class="line">            h = p;</span><br><span class="line">            p = tmp;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> h;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Leetcode-21-合并两个有序链表-剑指Offer面试题-25"><a href="#Leetcode-21-合并两个有序链表-剑指Offer面试题-25" class="headerlink" title="[Leetcode 21] 合并两个有序链表 (剑指Offer面试题 25)"></a>[Leetcode 21] <a href="https://leetcode-cn.com/problems/merge-two-sorted-lists/" target="_blank" rel="noopener">合并两个有序链表</a> (剑指Offer面试题 25)</h3><ul>
<li><p>解法：</p>
<ul>
<li><ol>
<li>优先队列重排    (可参考 提交记录99%代码)</li>
</ol>
</li>
<li><ol>
<li>比较插入列表 （这里使用）</li>
</ol>
</li>
</ul>
</li>
<li><p>代码：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"> * struct ListNode &#123;</span></span><br><span class="line"><span class="comment"> *     int val;</span></span><br><span class="line"><span class="comment"> *     ListNode *next;</span></span><br><span class="line"><span class="comment"> *     ListNode(int x) : val(x), next(NULL) &#123;&#125;</span></span><br><span class="line"><span class="comment"> * &#125;;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">ListNode* <span class="title">mergeTwoLists</span><span class="params">(ListNode* l1, ListNode* l2)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(l1 == <span class="literal">nullptr</span> &amp;&amp; l2 == <span class="literal">nullptr</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span>(l1 == <span class="literal">nullptr</span> &amp;&amp; l2 != <span class="literal">nullptr</span>)</span><br><span class="line">            <span class="keyword">return</span> l2;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span>(l2 == <span class="literal">nullptr</span> &amp;&amp; l1 != <span class="literal">nullptr</span>)</span><br><span class="line">            <span class="keyword">return</span> l1;</span><br><span class="line">        </span><br><span class="line">        ListNode* res;</span><br><span class="line">        ListNode* pMergeNode;</span><br><span class="line">        <span class="keyword">if</span>(l1-&gt;val &lt; l2-&gt;val)&#123;</span><br><span class="line">            pMergeNode = l1;</span><br><span class="line">            l1 = l1-&gt;next;</span><br><span class="line">        &#125;   </span><br><span class="line">        <span class="keyword">else</span>&#123;</span><br><span class="line">            pMergeNode = l2;</span><br><span class="line">            l2 = l2-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        res = pMergeNode;</span><br><span class="line">        <span class="keyword">while</span>(<span class="literal">true</span>)&#123;</span><br><span class="line">            <span class="keyword">if</span>(l1 == <span class="literal">nullptr</span> &amp;&amp; l2 != <span class="literal">nullptr</span>)&#123;</span><br><span class="line">                pMergeNode-&gt;next = l2;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(l2 == <span class="literal">nullptr</span> &amp;&amp; l1 != <span class="literal">nullptr</span>)&#123;</span><br><span class="line">                pMergeNode-&gt;next = l1;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(l1 == <span class="literal">nullptr</span> &amp;&amp; l2 == <span class="literal">nullptr</span>)&#123;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span>(l1-&gt;val &lt; l2-&gt;val)&#123;</span><br><span class="line">                pMergeNode-&gt;next = l1;</span><br><span class="line">                l1 = l1-&gt;next;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                pMergeNode-&gt;next = l2;</span><br><span class="line">                l2 = l2-&gt;next;</span><br><span class="line">            &#125;</span><br><span class="line">            pMergeNode = pMergeNode-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Leetcode23-合并K个排序链表"><a href="#Leetcode23-合并K个排序链表" class="headerlink" title="[Leetcode23] 合并K个排序链表"></a>[Leetcode23] <a href="https://leetcode-cn.com/problems/merge-k-sorted-lists/" target="_blank" rel="noopener">合并K个排序链表</a></h3><ul>
<li><p>解法：</p>
<ul>
<li>优先队列或列表遍历插入，优先队列效率相对高一些，这里列出的是列表遍历比较插入的代码，优先队列可参考提交记录99%代码。</li>
</ul>
</li>
<li><p>代码：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"> * struct ListNode &#123;</span></span><br><span class="line"><span class="comment"> *     int val;</span></span><br><span class="line"><span class="comment"> *     ListNode *next;</span></span><br><span class="line"><span class="comment"> *     ListNode(int x) : val(x), next(NULL) &#123;&#125;</span></span><br><span class="line"><span class="comment"> * &#125;;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">ListNode* <span class="title">mergeKLists</span><span class="params">(<span class="built_in">vector</span>&lt;ListNode*&gt;&amp; lists)</span> </span>&#123;</span><br><span class="line">        ListNode* res;</span><br><span class="line">        ListNode* pMerge = <span class="keyword">new</span> ListNode(<span class="number">0</span>);</span><br><span class="line">        res = pMerge;</span><br><span class="line">        <span class="keyword">while</span>(!isAllNodeEnd(lists))&#123;</span><br><span class="line">            <span class="keyword">int</span> tmp = INT_MAX;</span><br><span class="line">            <span class="keyword">int</span> index = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; lists.size() ; i++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(lists[i] != <span class="literal">nullptr</span>)&#123;</span><br><span class="line">                    <span class="keyword">if</span>(lists[i]-&gt;val &lt; tmp)&#123;</span><br><span class="line">                        tmp = lists[i]-&gt;val;</span><br><span class="line">                        index = i;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">            pMerge-&gt;next = lists[index];</span><br><span class="line">            pMerge = pMerge-&gt;next;</span><br><span class="line">            lists[index] = lists[index]-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res-&gt;next;</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">isAllNodeEnd</span><span class="params">(<span class="built_in">vector</span>&lt;ListNode*&gt;&amp; lists)</span></span>&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">auto</span> item : lists)&#123;</span><br><span class="line">            <span class="keyword">if</span>(item != <span class="literal">nullptr</span>)</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
</li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>[刷题]递归回溯专题</title>
    <url>/post/8cb158ac.html</url>
    <content><![CDATA[<h3 id="1-剑指Offer-面试题17-打印从1到最大的n位数"><a href="#1-剑指Offer-面试题17-打印从1到最大的n位数" class="headerlink" title="1. [剑指Offer 面试题17] 打印从1到最大的n位数"></a>1. [剑指Offer 面试题17] 打印从1到最大的n位数</h3><ul>
<li>题目：输入数字n，按顺序打印出从1到最大的n位十进制数。比如输入3， 打印出1,2，…，999。</li>
<li>解法：<ul>
<li>全排列解，这里主要用这个方式，有需要在输出时，对高位0进行截断。</li>
<li>大数加法解</li>
</ul>
</li>
<li>代码：<a id="more"></a>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Overview17</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">	Overview17()&#123;&#125;</span><br><span class="line">	~Overview17()&#123;&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">void</span> <span class="title">PrintAllNumber</span><span class="params">(<span class="keyword">int</span> n)</span></span>&#123;</span><br><span class="line">		<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; curr; <span class="comment">// 递归当前数字</span></span><br><span class="line">		<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; res; <span class="comment">//全部数字列表</span></span><br><span class="line"></span><br><span class="line">		dfs(res, <span class="number">0</span>, n, curr);</span><br><span class="line">		</span><br><span class="line">        <span class="comment">// 打印数字</span></span><br><span class="line">		<span class="keyword">for</span> (<span class="keyword">auto</span> item : res)&#123;</span><br><span class="line">			<span class="keyword">for</span> (<span class="keyword">auto</span> it : item)</span><br><span class="line">			&#123;</span><br><span class="line">				<span class="built_in">cout</span> &lt;&lt; it;</span><br><span class="line">			&#125;</span><br><span class="line">			<span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; &amp; res, <span class="keyword">int</span> depth, <span class="keyword">int</span> n, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; curr)</span></span>&#123;</span><br><span class="line">		<span class="keyword">if</span> (depth &gt;= n)&#123;</span><br><span class="line">			res.emplace_back(curr);</span><br><span class="line">			<span class="keyword">return</span>;</span><br><span class="line">		&#125;</span><br><span class="line">			</span><br><span class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++)&#123;</span><br><span class="line">			curr.emplace_back(i);</span><br><span class="line">			dfs(res, depth + <span class="number">1</span>, n, curr);</span><br><span class="line">			curr.pop_back();</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	Overview17 ocs;</span><br><span class="line">	ocs.PrintAllNumber(<span class="number">2</span>);</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="2-Leetcode-46-全排列"><a href="#2-Leetcode-46-全排列" class="headerlink" title="2. [Leetcode 46] 全排列"></a>2. [Leetcode 46] <a href="https://leetcode-cn.com/problems/permutations/" target="_blank" rel="noopener">全排列</a></h3><ul>
<li>解法：递归，注意去重</li>
<li>代码：</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">	<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; permute(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums) &#123;</span><br><span class="line">		<span class="keyword">if</span> (nums.size() == <span class="number">0</span>) <span class="keyword">return</span>&#123;&#125;;</span><br><span class="line">		<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; res;</span><br><span class="line">		<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; curr_vec;</span><br><span class="line">		<span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt; used;</span><br><span class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nums.size(); i++)</span><br><span class="line">		&#123;</span><br><span class="line">			used.push_back(<span class="literal">false</span>);</span><br><span class="line">		&#125;</span><br><span class="line">		dfs(res, nums, <span class="number">0</span>, curr_vec, used);</span><br><span class="line">		<span class="keyword">return</span> res;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; res, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> depth, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; curr_vec, <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt; used)</span></span></span><br><span class="line"><span class="function">	</span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">if</span> (depth &gt;= nums.size())</span><br><span class="line">		&#123;</span><br><span class="line">			res.emplace_back(curr_vec);</span><br><span class="line">			<span class="keyword">return</span>;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nums.size(); i++)</span><br><span class="line">		&#123;</span><br><span class="line">			<span class="keyword">if</span> (used[i]) <span class="keyword">continue</span>;</span><br><span class="line">			used[i] = <span class="literal">true</span>;</span><br><span class="line">			curr_vec.emplace_back(nums[i]);</span><br><span class="line">			dfs(res, nums, depth + <span class="number">1</span>, curr_vec, used);</span><br><span class="line">			curr_vec.pop_back();</span><br><span class="line">			used[i] = <span class="literal">false</span>;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h3 id="3-Leetcode-47-全排列-II"><a href="#3-Leetcode-47-全排列-II" class="headerlink" title="3. [Leetcode 47] 全排列 II"></a>3. [Leetcode 47] <a href="https://leetcode-cn.com/problems/permutations-ii/" target="_blank" rel="noopener">全排列 II</a></h3><ul>
<li>解法: 递归，有可重复数字，不要递归重了。对数列进行排序，让相同的数字连在一起，如果当前位已经使用过，或者该位的前一位与该位相同并且前一位没有被使用过，这时两个排列会相同，因而略过。</li>
<li>代码:</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">	<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; permuteUnique(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums) &#123;</span><br><span class="line">		<span class="keyword">if</span> (nums.size() &lt;= <span class="number">0</span>) <span class="keyword">return</span>&#123;&#125;;</span><br><span class="line"></span><br><span class="line">		sort(nums.begin(), nums.end());</span><br><span class="line">		</span><br><span class="line">		<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; res;</span><br><span class="line">		<span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt; used;</span><br><span class="line">		<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; curr;</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">for</span> (<span class="keyword">auto</span> item : nums)</span><br><span class="line">		&#123;</span><br><span class="line">			used.push_back(<span class="literal">false</span>);</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		dfs(res, nums, curr, used, <span class="number">0</span>);</span><br><span class="line">		<span class="keyword">return</span> res;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; res, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; curr, <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt; used, <span class="keyword">int</span> depth)</span></span></span><br><span class="line"><span class="function">	</span>&#123;</span><br><span class="line">		<span class="keyword">if</span> (depth &gt;= nums.size())</span><br><span class="line">		&#123;</span><br><span class="line">			res.emplace_back(curr);</span><br><span class="line">			<span class="keyword">return</span>;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nums.size(); i++)</span><br><span class="line">		&#123;</span><br><span class="line">			<span class="keyword">if</span> (used[i]) <span class="keyword">continue</span>; </span><br><span class="line">			<span class="keyword">if</span> (i &gt; <span class="number">0</span> &amp;&amp; nums[i] == nums[i - <span class="number">1</span>] &amp;&amp; !used[i<span class="number">-1</span>]) <span class="keyword">continue</span>;</span><br><span class="line"></span><br><span class="line">			used[i] = <span class="literal">true</span>;</span><br><span class="line">			curr.emplace_back(nums[i]);</span><br><span class="line">			dfs(res, nums, curr, used, depth + <span class="number">1</span>);</span><br><span class="line">			curr.pop_back();</span><br><span class="line">			used[i] = <span class="literal">false</span>;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title>刷题-栈和队列</title>
    <url>/post/db3d6a7.html</url>
    <content><![CDATA[<h3 id="1-Leetcode-232-用栈实现队列-（剑指OFFER面试题-9）"><a href="#1-Leetcode-232-用栈实现队列-（剑指OFFER面试题-9）" class="headerlink" title="1. [Leetcode 232] 用栈实现队列 （剑指OFFER面试题 9）"></a>1. [Leetcode 232] <a href="https://leetcode-cn.com/problems/implement-queue-using-stacks/" target="_blank" rel="noopener">用栈实现队列</a> （剑指OFFER面试题 9）</h3><ul>
<li><strong>解法</strong>：一个栈(push栈)用于接收push，一个栈(pop栈)用于top(peek)和pop<ol>
<li>当pop栈为空，且push栈不为空时，将push栈的元素转移到pop栈中</li>
<li>当pop栈不为空时，将pop栈的数据pop出去</li>
<li>push操作只在push栈进行</li>
</ol>
</li>
<li><strong>注意</strong>：<ul>
<li>leetcode上可以不进行异常处理，能a过，但是面试时候最好还是加上空栈的异常处理。</li>
<li>泛型支持。</li>
<li>线程安全。<a id="more"></a>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyQueue</span> &#123;</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">     <span class="built_in">stack</span>&lt;<span class="keyword">int</span>&gt;  spush;</span><br><span class="line">     <span class="built_in">stack</span>&lt;<span class="keyword">int</span>&gt; spop;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">/** Initialize your data structure here. */</span></span><br><span class="line">    MyQueue() &#123;</span><br><span class="line">       </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** Push element x to the back of queue. */</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">push</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">        spush.push(x);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/** Removes the element from in front of queue and returns that element. */</span></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">pop</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    	<span class="keyword">if</span> (spop.empty())&#123;</span><br><span class="line">        	<span class="keyword">while</span> (!spush.empty()) &#123;</span><br><span class="line">				spop.push(spush.top());</span><br><span class="line">				spush.pop();</span><br><span class="line">        	&#125;</span><br><span class="line">     	&#125;</span><br><span class="line">        <span class="keyword">int</span> res = spop.top();</span><br><span class="line">		spop.pop();</span><br><span class="line">		<span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/** Get the front element. */</span></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">peek</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    	<span class="keyword">if</span> (spop.empty())&#123;</span><br><span class="line">      		<span class="keyword">while</span> (!spush.empty()) &#123;</span><br><span class="line">				spop.push(spush.top());</span><br><span class="line">				spush.pop();</span><br><span class="line">      		&#125;  </span><br><span class="line">    	&#125;</span><br><span class="line">    	<span class="keyword">return</span> spop.top();</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/** Returns whether the queue is empty. */</span></span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">empty</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> spush.empty()&amp;&amp;spop.empty();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h3 id="2-Leetcode-155-最小栈-剑指Offer面试题30"><a href="#2-Leetcode-155-最小栈-剑指Offer面试题30" class="headerlink" title="2.[Leetcode 155] 最小栈 (剑指Offer面试题30)"></a>2.[Leetcode 155] 最小栈 (剑指Offer面试题30)</h3><ul>
<li><p>解法：双栈实现，一个栈用于正常的入栈出栈，一个栈用于记录最小值</p>
</li>
<li><p>代码：</p>
</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MinStack</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">/** initialize your data structure here. */</span></span><br><span class="line">    MinStack() &#123;</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">push</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">        mainStack.push(x);</span><br><span class="line">        <span class="keyword">if</span>(subStack.empty() || x &lt;= subStack.top())&#123;</span><br><span class="line">            subStack.push(x);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">pop</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> tmp = mainStack.top();</span><br><span class="line">        mainStack.pop();</span><br><span class="line">        <span class="keyword">if</span>(tmp == subStack.top())&#123;</span><br><span class="line">            subStack.pop();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">top</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> res = mainStack.top();</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">getMin</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">//if(subStack.empty()) return 0;</span></span><br><span class="line">        <span class="keyword">return</span> subStack.top();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="built_in">stack</span>&lt;<span class="keyword">int</span>&gt; mainStack;</span><br><span class="line">    <span class="built_in">stack</span>&lt;<span class="keyword">int</span>&gt; subStack;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Your MinStack object will be instantiated and called as such:</span></span><br><span class="line"><span class="comment"> * MinStack obj = new MinStack();</span></span><br><span class="line"><span class="comment"> * obj.push(x);</span></span><br><span class="line"><span class="comment"> * obj.pop();</span></span><br><span class="line"><span class="comment"> * int param_3 = obj.top();</span></span><br><span class="line"><span class="comment"> * int param_4 = obj.getMin();</span></span><br><span class="line"><span class="comment"> */</span></span><br></pre></td></tr></table></figure>
<h3 id="3-Leetcode-946-验证栈序列-（剑指Offer面试题31）"><a href="#3-Leetcode-946-验证栈序列-（剑指Offer面试题31）" class="headerlink" title="3. [Leetcode 946] 验证栈序列 （剑指Offer面试题31）"></a>3. [Leetcode 946] <a href="https://leetcode-cn.com/problems/validate-stack-sequences/" target="_blank" rel="noopener">验证栈序列</a> （剑指Offer面试题31）</h3><ul>
<li>解法：双栈模拟，或者通过判断输入栈栈顶元素是否与验证栈当前指针所指元素相等。</li>
<li>代码：</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">validateStackSequences</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; pushed, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; popped)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">stack</span> &lt;<span class="keyword">int</span>&gt;s;</span><br><span class="line">        <span class="keyword">int</span> count=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;pushed.size();i++)&#123;</span><br><span class="line">                s.push(pushed[i]);</span><br><span class="line">            <span class="keyword">while</span>(!s.empty()&amp;&amp;s.top()==popped[count])&#123;</span><br><span class="line">                s.pop();</span><br><span class="line">                count++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(!s.empty())</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title>[刷题]二叉树专题</title>
    <url>/post/76e180f5.html</url>
    <content><![CDATA[<blockquote>
<p>参考 <a href="https://www.toutiao.com/a6669724491066638859/?tt_from=weixin&amp;utm_campaign=client_share&amp;wxshare_count=1&amp;timestamp=1552926051&amp;app=news_article&amp;utm_source=weixin&amp;utm_medium=toutiao_android&amp;group_id=6669724491066638859" target="_blank" rel="noopener">几道和「二叉树」有关的算法面试题</a></p>
</blockquote>
<h3 id="1-Leetcode-105-从前序与中序遍历序列构造二叉树-剑指OFFER面试题-7"><a href="#1-Leetcode-105-从前序与中序遍历序列构造二叉树-剑指OFFER面试题-7" class="headerlink" title="1. [Leetcode 105] 从前序与中序遍历序列构造二叉树 (剑指OFFER面试题 7)"></a>1. [Leetcode 105] <a href="https://leetcode-cn.com/problems/construct-binary-tree-from-preorder-and-inorder-traversal/" target="_blank" rel="noopener">从前序与中序遍历序列构造二叉树</a> (剑指OFFER面试题 7)</h3><ul>
<li><strong>解法：</strong>递归，由先序序列确定子树根节点，由中序序列确定当前节点下的子树范围。</li>
<li><strong>主要参数(递归函数)：</strong>先序序列的起点idx与终点idx， 中序序列的起点idx与终点idx。</li>
<li><strong>递归停止条件：</strong> 先序序列或中序序列的起点idx &gt; 终点idx。</li>
<li><strong>递归参数更新方法：</strong><ul>
<li><strong>左子树更新:</strong> <ul>
<li>先序起点idx+1。 <code>preL + 1</code></li>
<li>先序终点为中序确定左子树节点数量num + 先序起点idx。 <code>preL + num</code></li>
<li>中序起点为之前递归层中的中序起点。 <code>inL</code></li>
<li>中序终点为根节点在中序序列中的idx - 1。 <code>inRoot - 1</code></li>
</ul>
</li>
<li><strong>右子树更新:</strong><ul>
<li>先序起点idx+1 + 中序确定左子树节点数量num。 <code>preL + num + 1</code></li>
<li>先序终点为之前递归层中的先序终点。 <code>preR</code></li>
<li>中序起点为根节点在中序序列中的idx + 1。 <code>inRoot + 1</code></li>
<li>中序终点为为之前递归层中的中序终点。 <code>inR</code><a id="more"></a>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"> * struct TreeNode &#123;</span></span><br><span class="line"><span class="comment"> *     int val;</span></span><br><span class="line"><span class="comment"> *     TreeNode *left;</span></span><br><span class="line"><span class="comment"> *     TreeNode *right;</span></span><br><span class="line"><span class="comment"> *     TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125;</span></span><br><span class="line"><span class="comment"> * &#125;;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">TreeNode* <span class="title">buildTree</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; preorder, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; inorder)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(preorder.empty()|| inorder.empty())</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> TreeIterate(<span class="number">0</span>, preorder.size() - <span class="number">1</span>, <span class="number">0</span>, inorder.size() - <span class="number">1</span>, preorder, inorder);</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function">TreeNode* <span class="title">TreeIterate</span><span class="params">(<span class="keyword">int</span> preL, <span class="keyword">int</span> preR, <span class="keyword">int</span> inL, <span class="keyword">int</span> inR, <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; preorder, <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; inorder)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(preL &gt; preR || inL &gt; inR)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">int</span> inRoot = <span class="number">0</span>; <span class="comment">//中序遍历根节点位置</span></span><br><span class="line">        <span class="comment">// vector&lt;int&gt;::iterator inRootIter =find(inorder.begin(), inorder.end(), preorder[preL]); </span></span><br><span class="line">        <span class="comment">// int inRoot std::distance(std::begin(inorder), inRootIter);</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; inorder.size(); i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(preorder[preL] == inorder[i])&#123;</span><br><span class="line">                inRoot = i;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> num = inRoot - inL; <span class="comment">//中序确定左子树节点数量</span></span><br><span class="line">        </span><br><span class="line">        TreeNode* currNode = <span class="keyword">new</span> TreeNode(preorder[preL]);</span><br><span class="line">        currNode -&gt; left = TreeIterate(preL + <span class="number">1</span>, preL + num, inL, inRoot - <span class="number">1</span> , preorder, inorder);</span><br><span class="line">        currNode -&gt; right = TreeIterate(preL + num + <span class="number">1</span>, preR, inRoot + <span class="number">1</span>, inR, preorder, inorder);</span><br><span class="line">        <span class="keyword">return</span> currNode;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2-Leetcode-572-另一个树的子树-剑指Offer面试题-26"><a href="#2-Leetcode-572-另一个树的子树-剑指Offer面试题-26" class="headerlink" title="2. [Leetcode 572] 另一个树的子树 (剑指Offer面试题 26)"></a>2. [Leetcode 572] <a href="https://leetcode-cn.com/problems/subtree-of-another-tree/" target="_blank" rel="noopener">另一个树的子树</a> (剑指Offer面试题 26)</h3><ul>
<li><p>解法：递归寻根相同，根相同后判断子树是否相同。</p>
</li>
<li><p>注意：停止条件，空指针处理</p>
</li>
<li><p>代码：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">isSame</span><span class="params">(struct TreeNode* s, struct TreeNode* t)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(!s &amp;&amp; !t) <span class="comment">// 两个子树叶子都是空</span></span><br><span class="line">             <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">if</span>(!s || !t) <span class="comment">// 有一个不是空</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">if</span>(s-&gt;val != t-&gt;val)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">return</span> isSame(s-&gt;left,t-&gt;left) &amp;&amp; isSame(s-&gt;right,t-&gt;right);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">isSubtree</span><span class="params">(struct TreeNode* s, struct TreeNode* t)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(!s) <span class="comment">// 主树为空</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">if</span>((s-&gt;val == t-&gt;val) &amp;&amp; isSame(s,t))</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">return</span> isSubtree(s-&gt;left,t) || isSubtree(s-&gt;right,t);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="3-LeetCode-144-二叉树的前序遍历"><a href="#3-LeetCode-144-二叉树的前序遍历" class="headerlink" title="3. [LeetCode 144] 二叉树的前序遍历"></a>3. [LeetCode 144] <a href="https://leetcode-cn.com/problems/binary-tree-preorder-traversal/" target="_blank" rel="noopener">二叉树的前序遍历</a></h3><ul>
<li><p>解法：</p>
<ul>
<li><p>递归，先输出再递归</p>
</li>
<li><p>循环，用栈模拟，节点右子节点入栈，左节点直接输出。</p>
<p><img src="http://p3.pstatp.com/large/pgc-image/140566d645a64fc2943e824de5e24c23" alt="å éåãäºåæ ãæå³çç®æ³é¢è¯é¢"></p>
</li>
</ul>
</li>
<li><p>代码：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">/////====递归解</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; preorderTraversal(TreeNode* root) &#123;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; res;</span><br><span class="line">        <span class="keyword">if</span>(root == <span class="literal">nullptr</span>) <span class="keyword">return</span> &#123;&#125;;</span><br><span class="line">     </span><br><span class="line">        Recurrence(res, root);</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">Recurrence</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;res, TreeNode* root)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(root == <span class="literal">nullptr</span>) <span class="keyword">return</span>;</span><br><span class="line">        res.emplace_back(root-&gt;val);</span><br><span class="line">        Recurence(res, root-&gt;left);</span><br><span class="line">        Recurence(res, root-&gt;right);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">////====循环解 前序：根左右，stack(根 右 左)</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; preorderTraversal(TreeNode* root) &#123;</span><br><span class="line">        <span class="keyword">if</span>(!root) <span class="keyword">return</span> &#123;&#125;;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; res;</span><br><span class="line">        <span class="built_in">stack</span>&lt;TreeNode* &gt; transStack;</span><br><span class="line">        transStack.push(root);</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span>(!transStack.empty())&#123;</span><br><span class="line">            TreeNode* curr = transStack.top();</span><br><span class="line">            transStack.pop();</span><br><span class="line">            res.emplace_back(curr-&gt;val);</span><br><span class="line">            <span class="keyword">if</span>(curr-&gt;right)</span><br><span class="line">                transStack.push(curr-&gt;right);</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span>(curr-&gt;left)</span><br><span class="line">                transStack.push(curr-&gt;left);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="4-LeetCode-94-二叉树的中序遍历"><a href="#4-LeetCode-94-二叉树的中序遍历" class="headerlink" title="4. [LeetCode 94] 二叉树的中序遍历"></a>4. [LeetCode 94] <a href="https://leetcode-cn.com/problems/binary-tree-inorder-traversal/" target="_blank" rel="noopener">二叉树的中序遍历</a></h3><ul>
<li><p>解法：</p>
<ul>
<li><p>递归：遍历左节点后输出值</p>
</li>
<li><p>循环：</p>
<p><img src="http://p1.pstatp.com/large/pgc-image/0114df24d124462aa63e19b688d37e5f" alt="å éåãäºåæ ãæå³çç®æ³é¢è¯é¢"></p>
</li>
</ul>
</li>
<li><p>代码：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; inorderTraversal(TreeNode* root) &#123;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; res;</span><br><span class="line">        <span class="keyword">if</span>(root == <span class="literal">nullptr</span>) <span class="keyword">return</span> &#123;&#125;;</span><br><span class="line">        Recurrence(res, root);</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">Recurrence</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; res, TreeNode* root)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(root == <span class="literal">nullptr</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span>;  </span><br><span class="line">        &#125; </span><br><span class="line">        </span><br><span class="line">        Recurrence(res, root-&gt;left);</span><br><span class="line">        res.emplace_back(root-&gt;val);</span><br><span class="line">        Recurrence(res, root-&gt;right);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">//循环解(当前节点不是空就一直往栈里push左孩子，如果当前节点为空，则出栈，当前节点设为出栈节点的右节点)</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; inorderTraversal(TreeNode* root) &#123;</span><br><span class="line">        <span class="keyword">if</span>(root == <span class="literal">nullptr</span>) <span class="keyword">return</span> &#123;&#125;;</span><br><span class="line">        </span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; res;</span><br><span class="line">        <span class="built_in">stack</span>&lt;TreeNode* &gt; transStack;</span><br><span class="line">        TreeNode* curr = root;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(curr||!transStack.empty())&#123;</span><br><span class="line">            <span class="keyword">if</span> (curr) &#123;</span><br><span class="line">                transStack.push(curr);</span><br><span class="line">                curr = curr-&gt;left;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                curr = transStack.top();</span><br><span class="line">                transStack.pop();</span><br><span class="line">                res.emplace_back(curr-&gt;val);</span><br><span class="line">                curr = curr-&gt;right;</span><br><span class="line">            &#125;          </span><br><span class="line">        &#125;</span><br><span class="line">      </span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="5-Leetcode-145-二叉树的后序遍历"><a href="#5-Leetcode-145-二叉树的后序遍历" class="headerlink" title="5. [Leetcode 145]  二叉树的后序遍历"></a>5. [Leetcode 145] <a href="https://leetcode-cn.com/problems/binary-tree-postorder-traversal/" target="_blank" rel="noopener"> 二叉树的后序遍历</a></h3><ul>
<li><p>解法：递归/循环，循环解，先左插，再右插，遇到空节点出栈，出栈元素插在结果数组的最前面。</p>
<p><img src="http://p3.pstatp.com/large/pgc-image/98cd6c7fca964ea19f5420264211d0ea" alt="å éåãäºåæ ãæå³çç®æ³é¢è¯é¢"></p>
</li>
<li><p>代码:</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 数组</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; postorderTraversal(TreeNode* root) &#123;</span><br><span class="line">        <span class="keyword">if</span>(!root) <span class="keyword">return</span> &#123;&#125;;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; res;</span><br><span class="line">        <span class="built_in">stack</span>&lt;TreeNode*&gt; transStack;</span><br><span class="line">        TreeNode* curr = root;</span><br><span class="line">        transStack.push(root);</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span>(!transStack.empty())&#123;</span><br><span class="line">            curr = transStack.top();</span><br><span class="line">            transStack.pop();</span><br><span class="line">            <span class="keyword">if</span>(curr-&gt;left) transStack.push(curr-&gt;left);</span><br><span class="line">            <span class="keyword">if</span>(curr-&gt;right) transStack.push(curr-&gt;right);</span><br><span class="line">            res.insert(res.begin(), curr-&gt;val);</span><br><span class="line">            </span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 链表转换 （Leetcode上的时间表现一样）</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; postorderTraversal(TreeNode* root) &#123;</span><br><span class="line">        <span class="keyword">if</span>(!root) <span class="keyword">return</span> &#123;&#125;;</span><br><span class="line">        </span><br><span class="line">        <span class="built_in">list</span>&lt;<span class="keyword">int</span> &gt; resList;</span><br><span class="line">        <span class="built_in">stack</span>&lt;TreeNode*&gt; transStack;</span><br><span class="line">        TreeNode* curr = root;</span><br><span class="line">        transStack.push(root);</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span>(!transStack.empty())&#123;</span><br><span class="line">            curr = transStack.top();</span><br><span class="line">            transStack.pop();</span><br><span class="line">            <span class="keyword">if</span>(curr-&gt;left) transStack.push(curr-&gt;left);</span><br><span class="line">            <span class="keyword">if</span>(curr-&gt;right) transStack.push(curr-&gt;right);</span><br><span class="line">            resList.insert(resList.begin(), curr-&gt;val);</span><br><span class="line">            </span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; res(resList.begin(), resList.end());</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="5-LeetCode-226-翻转二叉树-剑指Offer面试题27"><a href="#5-LeetCode-226-翻转二叉树-剑指Offer面试题27" class="headerlink" title="5. [LeetCode 226] 翻转二叉树 (剑指Offer面试题27)"></a>5. [LeetCode 226] <a href="https://leetcode-cn.com/problems/invert-binary-tree/" target="_blank" rel="noopener">翻转二叉树</a> (剑指Offer面试题27)</h3><ul>
<li><p>解法：递归换序</p>
</li>
<li><p>代码：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">TreeNode* <span class="title">invertTree</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(root == <span class="literal">nullptr</span>) <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">        </span><br><span class="line">        TreeNode* left = invertTree(root-&gt;left);</span><br><span class="line">        TreeNode* right = invertTree(root-&gt;right);</span><br><span class="line">        </span><br><span class="line">        root-&gt;right = left;</span><br><span class="line">        root-&gt;left = right;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> root;</span><br><span class="line">    &#125;   </span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="6-LeetCode-101-对称二叉树-剑指Offer面试题28"><a href="#6-LeetCode-101-对称二叉树-剑指Offer面试题28" class="headerlink" title="6.[LeetCode 101] 对称二叉树 (剑指Offer面试题28)"></a>6.[LeetCode 101] <a href="https://leetcode-cn.com/problems/symmetric-tree/" target="_blank" rel="noopener">对称二叉树</a> (剑指Offer面试题28)</h3><ul>
<li><p>解法1： 递归，</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">isSymmetric</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> isSymmetrical(root, root);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">isSymmetrical</span><span class="params">(TreeNode* pRoot1, TreeNode* pRoot2)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(pRoot1==<span class="literal">nullptr</span> &amp;&amp; pRoot2==<span class="literal">nullptr</span>) <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">if</span>(pRoot1==<span class="literal">nullptr</span> || pRoot2 == <span class="literal">nullptr</span>) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">if</span>(pRoot1-&gt;val != pRoot2-&gt;val) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">return</span> isSymmetrical(pRoot1-&gt;left, pRoot2-&gt;right) &amp;&amp; isSymmetrical(pRoot1-&gt;right, pRoot2-&gt;left);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
</li>
<li><p>解法2：BFS循环</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">isSymmetric</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(!root) <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        <span class="built_in">stack</span>&lt;TreeNode*&gt; Q;</span><br><span class="line">        Q.push(root-&gt;left);</span><br><span class="line">        Q.push(root-&gt;right);</span><br><span class="line">        <span class="keyword">while</span>(!Q.empty())&#123;</span><br><span class="line">            TreeNode* r1=Q.top();Q.pop();</span><br><span class="line">            TreeNode* r2=Q.top();Q.pop();</span><br><span class="line">            <span class="keyword">if</span>(!r1 &amp;&amp; !r2) <span class="keyword">continue</span>;</span><br><span class="line">            <span class="keyword">if</span>(r1 &amp;&amp; r2)&#123;</span><br><span class="line">                <span class="keyword">if</span>(r1-&gt;val != r2-&gt;val) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">                Q.push(r1-&gt;left);Q.push(r2-&gt;right);</span><br><span class="line">                Q.push(r1-&gt;right);Q.push(r2-&gt;left);</span><br><span class="line">            &#125;<span class="keyword">else</span> <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="7-Leetcode-102-二叉树的层次遍历-剑指Offer面试题32"><a href="#7-Leetcode-102-二叉树的层次遍历-剑指Offer面试题32" class="headerlink" title="7. [Leetcode 102] 二叉树的层次遍历 (剑指Offer面试题32)"></a>7. [Leetcode 102] <a href="https://leetcode-cn.com/problems/binary-tree-level-order-traversal/" target="_blank" rel="noopener">二叉树的层次遍历</a> (剑指Offer面试题32)</h3><ul>
<li><p><strong>思路：</strong>用一个队列<code>nodeQ</code>维护二叉树BFS顺序，由于题目中需要分层记录，所以需要维护每一层的数据vector<code>layerRes</code>以及每一层可扩展节点队列<code>layerQ</code>。</p>
</li>
<li><p><strong>代码：</strong></p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; levelOrder(TreeNode* root) &#123;</span><br><span class="line">        <span class="keyword">if</span>(root == <span class="literal">nullptr</span> ) <span class="keyword">return</span> &#123;&#125;;</span><br><span class="line">        <span class="built_in">queue</span>&lt;TreeNode* &gt; nodeQ;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; res;</span><br><span class="line">        nodeQ.push(root);</span><br><span class="line">        <span class="keyword">while</span>(!nodeQ.empty())&#123;</span><br><span class="line">            <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; layerRes;</span><br><span class="line">            <span class="keyword">int</span> qSize = nodeQ.size();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span>(qSize--)&#123;</span><br><span class="line">                TreeNode* currNode = nodeQ.front();</span><br><span class="line">                nodeQ.pop();</span><br><span class="line">                layerRes.emplace_back(currNode-&gt;val);</span><br><span class="line">                <span class="keyword">if</span>(currNode-&gt;left)</span><br><span class="line">                    nodeQ.push(currNode-&gt;left);</span><br><span class="line">                <span class="keyword">if</span>(currNode-&gt;right)</span><br><span class="line">                    nodeQ.push(currNode-&gt;right);</span><br><span class="line">            &#125;</span><br><span class="line">          </span><br><span class="line">            res.emplace_back(layerRes);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="8-Leetcode-230-二叉搜索树中第K小的元素"><a href="#8-Leetcode-230-二叉搜索树中第K小的元素" class="headerlink" title="8. [Leetcode 230] 二叉搜索树中第K小的元素"></a>8. [Leetcode 230] <a href="https://leetcode-cn.com/problems/kth-smallest-element-in-a-bst/" target="_blank" rel="noopener">二叉搜索树中第K小的元素</a></h3><ul>
<li><p><strong>解法:</strong> 循环模拟中序遍历</p>
</li>
<li><p><strong>代码:</strong></p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">kthSmallest</span><span class="params">(TreeNode* root, <span class="keyword">int</span> k)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">stack</span>&lt;TreeNode*&gt; transStack;</span><br><span class="line">        <span class="keyword">while</span>(<span class="number">1</span>)&#123;</span><br><span class="line">            <span class="keyword">if</span>(root != <span class="literal">NULL</span>)&#123;</span><br><span class="line">                transStack.push(root);</span><br><span class="line">                root = root-&gt;left;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">if</span>(--k == <span class="number">0</span>)&#123;</span><br><span class="line">                    <span class="keyword">return</span> transStack.top()-&gt;val;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    root = (transStack.top())-&gt;right;</span><br><span class="line">                    transStack.pop();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
</li>
</ul>
]]></content>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title>干货收集</title>
    <url>/post/b8a74939.html</url>
    <content><![CDATA[<blockquote>
<p>2017~2019年度我在Github上面收藏的一些优质干货Repo<br>其中有一些很荣幸作为Repo的贡献者，在issue中也结识了不少好友，共同学习</p>
</blockquote>
<h3 id="计算机软件工程类"><a href="#计算机软件工程类" class="headerlink" title="计算机软件工程类"></a>计算机软件工程类</h3><ul>
<li><a href="https://github.com/justjavac/free-programming-books-zh_CN" target="_blank" rel="noopener">免费的计算机编程类中文书籍</a>   (44638 star)</li>
<li><a href="https://github.com/AlfredTheBest/Design-Pattern" target="_blank" rel="noopener">设计模式-包教不包会</a><br><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1548490937/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190126162112.png" alt="x"></li>
<li><a href="https://github.com/judasn/IntelliJ-IDEA-Tutorial" target="_blank" rel="noopener">IntelliJ IDEA 简体中文专题教程</a><br><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1548490887/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190126162111.png" alt="x"><a id="more"></a></li>
<li><a href="https://github.com/stanzhai/be-a-professional-programmer" target="_blank" rel="noopener">成为专业程序员路上用到的各种优秀资料、神器及框架</a><br><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1548491108/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190126162454.png" alt="x"></li>
<li><a href="https://github.com/jacksu/utils4s" target="_blank" rel="noopener">scala、spark使用过程中，各种测试用例以及相关资料整理</a><br><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1548491208/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190126162631.png" alt="x"></li>
</ul>
<h3 id="刷题类"><a href="#刷题类" class="headerlink" title="刷题类"></a>刷题类</h3><ul>
<li><a href="https://github.com/zhkmxx9302013/leetcode" target="_blank" rel="noopener">Leetcode刷题(正在完善，可贡献)</a><br><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1548491429/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190126163016.png" alt="x"></li>
<li><a href="https://github.com/zhkmxx9302013/algorithm" target="_blank" rel="noopener">algorithm</a></li>
<li><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1548491743/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190126163528.png" alt="x"></li>
<li></li>
</ul>
<h3 id="深度学习类"><a href="#深度学习类" class="headerlink" title="深度学习类"></a>深度学习类</h3><ul>
<li><a href="https://github.com/exacity/deeplearningbook-chinese" target="_blank" rel="noopener">deeplearningbook-chinese</a><br><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1548491627/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190126163333.png" alt="x"></li>
</ul>
<blockquote>
<p>随着整理，持续更新</p>
</blockquote>
]]></content>
      <tags>
        <tag>干货分享</tag>
      </tags>
  </entry>
  <entry>
    <title>[强化学习笔记专题(二)]Nature DQN</title>
    <url>/post/22518.html</url>
    <content><![CDATA[<h2 id="DQN-Nature"><a href="#DQN-Nature" class="headerlink" title="DQN (Nature)"></a>DQN (Nature)</h2><p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1548320185/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190124165548.png" alt="1"></p>
<h2 id="一、-算法流程："><a href="#一、-算法流程：" class="headerlink" title="一、 算法流程："></a>一、 算法流程：</h2><ol>
<li><h5 id="定义可配置参数"><a href="#定义可配置参数" class="headerlink" title="定义可配置参数"></a>定义可配置参数</h5><ul>
<li>episode 数量 M</li>
<li>最大仿真时间 T，$\epsilon-greedy$参数$\epsilon_{low}$,$\epsilon_{high}$</li>
<li>batch size $N​$</li>
<li>折扣率 $\gamma$，学习率 $\alpha$等优化器参数</li>
<li>Soft update 频率 $C​$<a id="more"></a></li>
</ul>
</li>
<li><h5 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h5><ul>
<li>初始化 replay buffer 大小N</li>
<li>初始化 Q 网络 $Q​$ ，使用随机权值 $\theta​$</li>
<li>初始化 TargetQ 网络 $\hat{Q}$ 权值 $\theta^-$，使用 Q 网络的权值 $\theta$</li>
</ul>
</li>
<li><h5 id="DQN-一个Episode的流程"><a href="#DQN-一个Episode的流程" class="headerlink" title="DQN 一个Episode的流程"></a>DQN 一个Episode的流程</h5><ul>
<li>使用 $\epsilon-greedy$ 策略 选择一个 action $a_t$</li>
<li>执行当前 action $a_t$， 获取下一个状态 $s_{t+1}$ 和 reward $r_{t}$ </li>
<li>将当前状态$s_t$赋值为下一个状态 $s_{t+1}$</li>
<li>将五元组$\langle s_t,a_t,r_t,s_{t+1},done \rangle $存入 replay buffer $D$</li>
<li>训练Q网络$Q​$:<ul>
<li><strong>[Pre-condition]</strong>训练网络的前提是 <strong>replay buffer 的已有五元组数量大于 batch size </strong> $N$</li>
<li>从 replay buffer $D​$中随机选取 batch size $N​$条数据$\langle s_j,a_j,r_j,s_{j+1},done\rangle​$ $D_{selected}​$</li>
<li>计算目标Q值$y​$， $y​$是一个向量，$\{y_j \in y |j\in[0,N]\} ​$，大小为 batch size $N​$<ul>
<li>当 $D_{selected}​$[j] 中 $done=True​$ 时，即终局状态，此时 $y_j=r_j​$</li>
<li>当 $D_{selected}$[j] 中 $done=False$ 时，即非终局状态，此时$y_i=r_j+\gamma max_{a’}\hat{Q}(s_{j+1},a’;\theta^-)$， 注意这里是用的 TargetQ 网络进行的</li>
</ul>
</li>
<li>使用优化器进行梯度下降，损失函数是(一个batch里面)  $(y-Q(s,a;\theta))^2​$，注意这里使用的是Q网络进行，来让<strong>计算出来的目标Q值</strong>与<strong>当前Q网络输出的Q值</strong>进行<strong>MSE</strong></li>
<li>每 $C$ 次 episode，soft update 一次 target net 参数，$\theta^- = \theta$</li>
</ul>
</li>
</ul>
</li>
<li><h5 id="不断迭代Episode流程-M-次"><a href="#不断迭代Episode流程-M-次" class="headerlink" title="不断迭代Episode流程$M$次"></a>不断迭代Episode流程$M$次</h5></li>
</ol>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1548321569/dqn_loss.png" alt="img"></p>
<h2 id="二、对应代码"><a href="#二、对应代码" class="headerlink" title="二、对应代码"></a>二、对应代码</h2><p>完整代码地址： <a href="https://coding.net/u/zhkmxx930/p/DQN_Nature/git/blob/master/NatureDQN.py" target="_blank" rel="noopener">Nature DQN</a></p>
<ol>
<li><h5 id="初始化-1"><a href="#初始化-1" class="headerlink" title="初始化"></a>初始化</h5><ul>
<li>初始化 replay buffer 大小N</li>
<li>初始化 Q 网络 $Q$ ，使用随机权值$\theta$</li>
<li>初始化 TargetQ 网络 $\hat{Q}$ 权值 $\theta^-$，使用 Q 网络的权值 $\theta$</li>
</ul>
</li>
</ol>
<pre><code><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_Q_network</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Q net 网络定义</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 输入状态 placeholder</span></span><br><span class="line">        self.state_input = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>, self.state_dim])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Q 网络结构 两层全连接</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'current_net'</span>):</span><br><span class="line">            W1 = self.weight_variable([self.state_dim, <span class="number">100</span>])</span><br><span class="line">            b1 = self.bias_variable([<span class="number">100</span>])</span><br><span class="line">            W2 = self.weight_variable([<span class="number">100</span>, self.action_dim])</span><br><span class="line">            b2 = self.bias_variable([self.action_dim])</span><br><span class="line">            h_layer = tf.nn.relu(tf.matmul(self.state_input, W1) + b1)</span><br><span class="line">            <span class="comment"># Q Value</span></span><br><span class="line">            self.Q_value = tf.matmul(h_layer, W2) + b2</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Target Net 结构与 Q相同，可以用tf的reuse实现</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'target_net'</span>):</span><br><span class="line">            W1t = self.weight_variable([self.state_dim, <span class="number">100</span>])</span><br><span class="line">            b1t = self.bias_variable([<span class="number">100</span>])</span><br><span class="line">            W2t = self.weight_variable([<span class="number">100</span>, self.action_dim])</span><br><span class="line">            b2t = self.bias_variable([self.action_dim])</span><br><span class="line">            h_layer_t = tf.nn.relu(tf.matmul(self.state_input, W1t) + b1t)</span><br><span class="line">            <span class="comment"># target Q Value</span></span><br><span class="line">            self.target_Q_value = tf.matmul(h_layer_t, W2t) + b2t</span><br><span class="line"></span><br><span class="line">        t_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=<span class="string">'target_net'</span>)</span><br><span class="line">        e_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=<span class="string">'current_net'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># soft update 更新 target net</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'soft_replacement'</span>):</span><br><span class="line">            self.target_replace_op = [tf.assign(t, e) <span class="keyword">for</span> t, e <span class="keyword">in</span> zip(t_params, e_params)]</span><br><span class="line"></span><br><span class="line"><span class="comment">#===============================================================#</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(self, shape)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        初始化网络权值(随机, truncated_normal)</span></span><br><span class="line"><span class="string">        :param shape:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        initial = tf.truncated_normal(shape)</span><br><span class="line">        <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"><span class="comment">#===============================================================#</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(self, shape)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        初始化bias(const)</span></span><br><span class="line"><span class="string">        :param shape:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        initial = tf.constant(<span class="number">0.01</span>, shape=shape)</span><br><span class="line">        <span class="keyword">return</span> tf.Variable(initial)</span><br></pre></td></tr></table></figure>
</code></pre><ol>
<li><p>$\epsilon-greedy$ 策略 定义，这里对$\epsilon$进行一个随时间步的迁移而减小的策略，使其动作选择的不确定性逐渐减小。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">egreedy_action</span><span class="params">(self, state)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    epsilon-greedy策略</span></span><br><span class="line"><span class="string">    :param state:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    Q_value = self.Q_value.eval(feed_dict=&#123;</span><br><span class="line">        self.state_input: [state]</span><br><span class="line">    &#125;)[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> random.random() &lt;= self.epsilon:</span><br><span class="line">        self.epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / <span class="number">10000</span></span><br><span class="line">        <span class="keyword">return</span> random.randint(<span class="number">0</span>, self.action_dim - <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        self.epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / <span class="number">10000</span></span><br><span class="line">        <span class="keyword">return</span> np.argmax(Q_value)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ol>
<li><p>Replay buffer的定义</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">perceive</span><span class="params">(self, state, action, reward, next_state, done)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Replay buffer</span></span><br><span class="line"><span class="string">    :param state:</span></span><br><span class="line"><span class="string">    :param action:</span></span><br><span class="line"><span class="string">    :param reward:</span></span><br><span class="line"><span class="string">    :param next_state:</span></span><br><span class="line"><span class="string">    :param done:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 对action 进行one-hot存储，方便网络进行处理</span></span><br><span class="line">    <span class="comment"># [0,0,0,0,1,0,0,0,0] action=5</span></span><br><span class="line">    one_hot_action = np.zeros(self.action_dim)</span><br><span class="line">    one_hot_action[action] = <span class="number">1</span></span><br><span class="line">   </span><br><span class="line">    <span class="comment"># 存入replay_buffer</span></span><br><span class="line">    <span class="comment"># self.replay_buffer = deque()</span></span><br><span class="line">    self.replay_buffer.append((state, one_hot_action, reward, next_state, done))</span><br><span class="line">   </span><br><span class="line">    <span class="comment"># 溢出出队</span></span><br><span class="line">    <span class="keyword">if</span> len(self.replay_buffer) &gt; REPLAY_SIZE:</span><br><span class="line">        self.replay_buffer.popleft()</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 可进行训练条件</span></span><br><span class="line">    <span class="keyword">if</span> len(self.replay_buffer) &gt; BATCH_SIZE:</span><br><span class="line">        self.train_Q_network()</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ol>
<li><p>Q网络训练</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_Q_network</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Q网络训练</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    self.time_step += <span class="number">1</span></span><br><span class="line">    <span class="comment"># 从 replay buffer D中随机选取 batch size N条数据&lt;s_j,a_j,r_j,s_j+1,done&gt;   D_selected</span></span><br><span class="line">    minibatch = random.sample(self.replay_buffer, BATCH_SIZE)</span><br><span class="line">    state_batch = [data[<span class="number">0</span>] <span class="keyword">for</span> data <span class="keyword">in</span> minibatch]</span><br><span class="line">    action_batch = [data[<span class="number">1</span>] <span class="keyword">for</span> data <span class="keyword">in</span> minibatch]</span><br><span class="line">    reward_batch = [data[<span class="number">2</span>] <span class="keyword">for</span> data <span class="keyword">in</span> minibatch]</span><br><span class="line">    next_state_batch = [data[<span class="number">3</span>] <span class="keyword">for</span> data <span class="keyword">in</span> minibatch]</span><br><span class="line">   </span><br><span class="line">    <span class="comment"># 计算目标Q值y</span></span><br><span class="line">    y_batch = []</span><br><span class="line">    Q_value_batch = self.target_Q_value.eval(feed_dict=&#123;self.state_input: next_state_batch&#125;)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, BATCH_SIZE):</span><br><span class="line">        done = minibatch[i][<span class="number">4</span>]</span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            y_batch.append(reward_batch[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            y_batch.append(reward_batch[i] + GAMMA * np.max(Q_value_batch[i]))</span><br><span class="line">   </span><br><span class="line">    self.optimizer.run(feed_dict=&#123;</span><br><span class="line">        self.y_input: y_batch,</span><br><span class="line">        self.action_input: action_batch,</span><br><span class="line">        self.state_input: state_batch</span><br><span class="line">    &#125;)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ol>
<li><p>Soft update</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_target_q_network</span><span class="params">(self, episode)</span>:</span></span><br><span class="line">   <span class="comment"># 更新 target Q netowrk</span></span><br><span class="line">   <span class="keyword">if</span> episode % REPLACE_TARGET_FREQ == <span class="number">0</span>:</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="三、实验结果"><a href="#三、实验结果" class="headerlink" title="三、实验结果"></a>三、实验结果</h2><p>环境 cart-pole-v0 (期望回报是200)</p>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1548385957/1.png" alt=""><br><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1548385957/2.png" alt=""></p>
<h2 id="四、DQN参考论文流程："><a href="#四、DQN参考论文流程：" class="headerlink" title="四、DQN参考论文流程："></a>四、DQN参考论文流程：</h2><p><img src="http://intheworld.win/wordpress/wp-content/uploads/2018/07/dqn.jpg" alt="img"></p>
<h2 id="五、Double-DQN"><a href="#五、Double-DQN" class="headerlink" title="五、Double DQN"></a>五、Double DQN</h2><p><strong>DQN存在的问题是Q function容易过拟合</strong>，根据状态 $s_{t+1}$ 选择动作 $a_{t+1}$ 的过程,以及估计 $Q(s_{t+1},a_{t+1})​$ 使用的同一个Q net网络参数，这可能导致选择过高的估计值，从而导致过于乐观的值估计。为了避免这种情况的出现，可以对选择和衡量进行解耦，从而就有了使用 Double DQN 来解决这一问题。</p>
<p>Double DQN与DQN的区别仅在于$y$的求解方式不同，Double DQN根据Q网络参数来选择动作$a_{t+1}$,再用Target Q网络参数来衡量$Q(s_{t+1},a_{t+1})$的值。</p>
<script type="math/tex; mode=display">
Y_t^{DQN}=R_{t+1}+\gamma \hat{Q}(S_{t+1},argmax_a\hat{Q}(S_{t+1},a;\theta_t^-);\theta_t^-)\\
Y_t^{DDQN}=R_{t+1}+\gamma \hat{Q}(S_{t+1},argmax_aQ(S_{t+1},a;\theta_t);\theta_t^-)</script><p>反映在代码上，就是训练的时候选择Q的时候有点变动：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_Q_network</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Q网络训练</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    self.time_step += <span class="number">1</span></span><br><span class="line">    <span class="comment"># 从 replay buffer D中随机选取 batch size N条数据&lt;s_j,a_j,r_j,s_j+1,done&gt;$  D_selected</span></span><br><span class="line">    minibatch = random.sample(self.replay_buffer, BATCH_SIZE)</span><br><span class="line">    state_batch = [data[<span class="number">0</span>] <span class="keyword">for</span> data <span class="keyword">in</span> minibatch]</span><br><span class="line">    action_batch = [data[<span class="number">1</span>] <span class="keyword">for</span> data <span class="keyword">in</span> minibatch]</span><br><span class="line">    reward_batch = [data[<span class="number">2</span>] <span class="keyword">for</span> data <span class="keyword">in</span> minibatch]</span><br><span class="line">    next_state_batch = [data[<span class="number">3</span>] <span class="keyword">for</span> data <span class="keyword">in</span> minibatch]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算目标Q值y</span></span><br><span class="line">    y_batch = []</span><br><span class="line">    QTarget_value_batch = self.target_Q_value.eval(feed_dict=&#123;self.state_input: next_state_batch&#125;)</span><br><span class="line">    Q_value_batch = self.Q_value.eval(feed_dict=&#123;self.state_input: next_state_batch&#125;)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, BATCH_SIZE):</span><br><span class="line">        done = minibatch[i][<span class="number">4</span>]</span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            y_batch.append(reward_batch[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment">#################用target Q(Q)#######################</span></span><br><span class="line">            <span class="keyword">if</span> DOUBLE_DQN:</span><br><span class="line">                selected_q_next = QTarget_value_batch[i][np.argmax(Q_value_batch[i])]</span><br><span class="line">            <span class="comment">#################用target Q(target Q)################</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                selected_q_next = np.max(QTarget_value_batch[i])</span><br><span class="line"></span><br><span class="line">            y_batch.append(reward_batch[i] + GAMMA * selected_q_next)</span><br><span class="line"></span><br><span class="line">    self.optimizer.run(feed_dict=&#123;</span><br><span class="line">        self.y_input: y_batch,</span><br><span class="line">        self.action_input: action_batch,</span><br><span class="line">        self.state_input: state_batch</span><br><span class="line">    &#125;)</span><br></pre></td></tr></table></figure>
<h2 id="六、DQN，DDQN实验结果对比"><a href="#六、DQN，DDQN实验结果对比" class="headerlink" title="六、DQN，DDQN实验结果对比"></a>六、DQN，DDQN实验结果对比</h2><p>可以看到DoubleDQN的表现比 DQN稳定</p>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1548386089/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190125111426.png" alt=""></p>
<h2 id="七、-Dueling-DDQN"><a href="#七、-Dueling-DDQN" class="headerlink" title="七、 Dueling DDQN"></a>七、 Dueling DDQN</h2><h2 id="八、-Dueling-DQN-DDQN-DQN对比"><a href="#八、-Dueling-DQN-DDQN-DQN对比" class="headerlink" title="八、 Dueling DQN, DDQN, DQN对比"></a>八、 Dueling DQN, DDQN, DQN对比</h2><p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1548387003/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190125112753.png" alt=""></p>
]]></content>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>[强化学习论文] (HDQN) Integrating Temporal Abstraction and Intrinsic Motivation</title>
    <url>/post/10845.html</url>
    <content><![CDATA[<h2 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h2><ul>
<li><strong>题目</strong>: Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation</li>
<li><strong>作者</strong>: Tejas D. Kulkarni, Karthik R. Narasimhan, Ardavan Saeedi, Joshua B. Tenenbaum</li>
<li><strong>论文</strong>: <a href="https://arxiv.org/abs/1604.06057" target="_blank" rel="noopener">https://arxiv.org/abs/1604.06057</a></li>
<li><strong>年份</strong>: 2016</li>
<li><p><strong>参考:</strong> <a href="https://github.com/aleju/papers/blob/master/neural-nets/Hierarchical_Deep_Reinforcement_Learning.md" target="_blank" rel="noopener">https://github.com/aleju/papers/blob/master/neural-nets/Hierarchical_Deep_Reinforcement_Learning.md</a></p>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1548211066/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190123103711.png" alt="ti"></p>
</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><h3 id="1-主要贡献"><a href="#1-主要贡献" class="headerlink" title="1.主要贡献"></a>1.主要贡献</h3><ul>
<li>提出了一种分层强化学习方法</li>
<li>该方法使用了长期目标(long-term goal)指导短期动作(short-term choice)的选择</li>
</ul>
<a id="more"></a>
<h3 id="2-主要方法"><a href="#2-主要方法" class="headerlink" title="2.主要方法"></a>2.主要方法</h3><ul>
<li>两个重要组件<ul>
<li><strong>Meta-controller</strong><ul>
<li>负责生成长期目标long term goal;</li>
<li>通过训练Meta-controller使其能够根据当前state来选取目标goal，使得extrinsic reward最大;</li>
<li>当且仅当底层执行器Controller完成一个episode或者达成某个Meta-controller产生的goal的时候，Meta-controller再去产生新的目标goal。</li>
</ul>
</li>
<li><strong>Controller</strong><ul>
<li>从环境中获取当前state，并从Meta-controller中获取当前目标goal;</li>
<li>基于当前goal和当前的state，来选取最大化intrinsic reward期望的action，这里与传统的rf相同只是增加了目标goal，这里通过估计action-value function ( $Q_1(s_t, a_t;\theta_1,g_t)$ )来做;</li>
<li>Reward 是 intrinsic的，在agent内部，这个intrinsic reward由Critic网络产生，当且仅当当前的目标达到时，才会产生intrinsic reward。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="https://pic4.zhimg.com/v2-16726ce2c7aa3e1ee41d79656324501f_r.jpg" alt="preview"></p>
<ul>
<li>在蒙特祖玛的复仇(Montezuma’s Revenge)上实验<ul>
<li>目标Goal是一些手动设置的特定游戏object，比如钥匙，在实验中，将设置一个与游戏屏幕大小相等的遮罩层，当且仅当目标object的位置上的二进制位是1，其他像素上的二进制值为0；</li>
<li>Meta-controller通过Q function选择要到达的下一个Goal;</li>
<li>Controller根据Q function选择能够到达Goal的action, 其不断迭代选择action，直到其完成一个episode或到达Goal；</li>
<li>每当达到目标Goal时，Critic都会向Controller提供内在奖励(intrinsic reward);</li>
<li>CNN用于Meta-controller和Controller，在架构上类似于Atari-DQN论文(shallow CNNs);</li>
<li>使用两个Replay buffer，一个用于Meta-controller(大小为40k)，一个用于Controller(大小为1M);</li>
<li>Meta-controller和Controller两者都遵循epsilon-greedy。Epsilon从1.0开始，减小至0.1;</li>
<li>折扣因子γ为0.9;</li>
<li>使用SGD优化。</li>
</ul>
</li>
</ul>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1548210666/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190123102201.png" alt="1"></p>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1548210834/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190123103336.png" alt="2"></p>
]]></content>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>强化学习专题笔记(一) 强化学习基础</title>
    <url>/post/17172.html</url>
    <content><![CDATA[<h2 id="一、长期回报"><a href="#一、长期回报" class="headerlink" title="一、长期回报"></a>一、长期回报</h2><p>对于问题的简化，采用理想的MDP，简化问题到具有马尔科夫性，对于马尔科夫决策过程而言，在理想状态下，每一个行动都要为最终的目标<strong>最大化长期回报</strong> 而努力。</p>
<script type="math/tex; mode=display">
\max\sum_{t}{r_t}</script><p>但是很多情况下，仿真的时间维度较大，步数较多，甚至可以无限循环下去，这样的情况下我们需要引入一个可以和收敛的无穷数列，来替代我们最原始的长期回报公式。即<strong>对未来的回报乘以一个折扣率，使得长期回报变得更有意义</strong>：</p>
<script type="math/tex; mode=display">
\sum_{t=0}{\gamma^tr_t}    （\gamma < 1）</script><p>由此我们引出<strong>长期回报</strong>的概念，即从当前状态开始对之后的所有回报，运用上式进行累加的折扣率计算：</p>
<script type="math/tex; mode=display">
Ret_t=\sum_{k=0}\gamma^kr_{t+k+1}</script><p>但是长期回报需要知道未来的行动情况，我们需要对上式进行一个合理的估计，因而我们定义了<strong>策略的价值</strong>。<br><a id="more"></a></p>
<h2 id="二、值函数"><a href="#二、值函数" class="headerlink" title="二、值函数"></a>二、值函数</h2><p>由于环境的原因，MDP中的状态转移概率有时候并不能够确定，因而需要基于<strong>状态转移</strong>来<strong>估计长期回报</strong>的<strong>期望</strong>。τ是从某个状态s出发，根据策略与状态转移概率采样得到的序列(trajectory)。那么价值函数可以表示为：</p>
<script type="math/tex; mode=display">
v_{\pi}{(s_t)} = \mathbb{E}_{s,a\simτ}[\sum_{k=0}\gamma^kr_{t+k+1}]
=\sum_{\tau}{p(\tau)}{\sum_{k=0}^{\infin}{\gamma^k}{r_{t+k+1}}}</script><p>根据MDP模型的形式，值函数一般分为两种：</p>
<ul>
<li><strong>状态值函数</strong>  $v_{\pi}{(s)}​$:               已知当前状态s，按照某种策略行动产生的长期回报期望；</li>
<li><strong>状态-动作值函数 </strong> $q_{\pi}{(s,a)}​$:  已知当前状态s及动作a，按照某种策略行动产生的长期回报期望。</li>
</ul>
<p>由于符合马尔科夫性，我们可以将值函数的形式<strong>进行马尔科夫展开</strong>,其中${\pi(a_t|s_t)}$表示，在$s_t$状态下选择策略$\pi$的概率，策略$\pi$将产生行动$a_t$，$p(s_{t+1}|s_t,a_t)$表示在策略$\pi$的情况下，从$s_t，a_t$到达$s_{t+1}$的概率。</p>
<script type="math/tex; mode=display">
v_{\pi}{(s_t)}=\sum_{(s_t,a_t,...)\sim\tau}{\pi(a_t|s_t)}p(s_{t+1}|s_t,a_t)...{\sum_{k=0}^{\infin}{\gamma^k}{r_{t+k+1}}}</script><h2 id="三、贝尔曼方程"><a href="#三、贝尔曼方程" class="headerlink" title="三、贝尔曼方程"></a>三、贝尔曼方程</h2><ol>
<li><strong>状态值函数的贝尔曼方程</strong></li>
</ol>
<p>通过代换消元，可以将上式整理为<strong>状态值函数的贝尔曼方程</strong>：</p>
<script type="math/tex; mode=display">
v_{\pi}(s_t)=\sum_{a_t}\pi(a_t|s_t)\sum_{s_{t+1}}{p(s_{t+1}|s_t,a_t)[r_{t+1}+\gamma v_{\pi}(s_{t+1})]}</script><p>更直观一点可以将贝尔曼方程描述为一种DP的形式，即当前状态$s$下，选择策略$\pi$的长期回报期望。</p>
<script type="math/tex; mode=display">
v_{\pi}(s_t)=\sum_{a_t,s_t+1}\pi{(a_t|s_t)}\mathbb{E}[r_{t+1} + \gamma v_{\pi}(s_{t+1})]</script><p>按Sutton的书表示：</p>
<script type="math/tex; mode=display">
v_π(s)=\mathbb{E}_π[r_{t+1}+γv_π(s_{t+1})|s_t=s]</script><ol>
<li><strong>状态-动作值函数的贝尔曼方程</strong></li>
</ol>
<p>类似地，可以定义<strong>状态-动作值函数的贝尔曼方程</strong>：</p>
<script type="math/tex; mode=display">
q_{\pi}(s_t,a_t)=\sum_{s_{t+1}}p(s_{t+1}|s_t,a_t)\sum_{a_{t+1}}p(a_{t+1}|s_{t+1})[r_{t+1}+\gamma q_\pi{(s_{t+1},a_{t+1})}]</script><p>按Sutton的书表示:</p>
<script type="math/tex; mode=display">
q_\pi(s, a)=\mathbb{E}[r_{t+1} +\gamma q_{\pi}(s_{t+1}, a_{t+1})|s_{t}=s,a_{t}=a]</script><ol>
<li><strong>Bellman optimality equation</strong></li>
</ol>
<script type="math/tex; mode=display">
v_*(s)=\mathbb{E}[r_{t+1} +\gamma max_\pi{v(s_{t+1})|s_t=s}]</script><script type="math/tex; mode=display">
q_*(s,a)=\mathbb{E}[r_{t+1} +\gamma max_{a_{t+1}}{q(s_{t+1},a_{t+1})|s_t=s,a_t=a}]</script><h2 id="四、Monte-Carlo与Time-Difference"><a href="#四、Monte-Carlo与Time-Difference" class="headerlink" title="四、Monte-Carlo与Time Difference"></a>四、Monte-Carlo与Time Difference</h2><ul>
<li>MC 方差较大，需要较深的探索获取回报</li>
<li>TD  方差较小，偏差较大，可设定探索深度(1-step, n-step), Q-Learning, SARSA都属于TD</li>
</ul>
<p>【参考】<a href="https://zhuanlan.zhihu.com/p/25239682" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/25239682</a></p>
<p>Monte-Carlo method适用于“情节式任务”（情节任务序列有终点，与“情节式任务”相对应的是“连续型任务”）。$Q(s,a)$就是整个序列的期望回报。MC增量更新中的<strong>Monte-Carlo error</strong> ($R-Q(s_t,a_t)$)：</p>
<script type="math/tex; mode=display">
Q(s_t,a_t)\Leftarrow Q(s_t,a_t) + \alpha(R-Q(s_t,a_t))</script><p>TD（Time Difference） method，是MC和DP 方法的一个结合。相比MC方法，TD除了能够适用于连续任务外，和MC的差异从下图可以清楚看到。MC需要回退整个序列更新Q值，而TD只需要回退1步或n步更新Q值。因为MC需要等待序列结束才能训练，而TD没有这个限制，因此TD收敛速度明显比MC快，目前的主要算法都是基于TD。下图是TD和MC的回退图，很显然MC回退的更深。</p>
<p><img src="https://ws1.sinaimg.cn/large/006gCbCHly1fx0o33ydhkj30yi07umym.jpg" alt=""></p>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1553434786/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190324213854.png" alt="img"></p>
<p><strong>1-step TD error: ($r_{t+1}+\gamma Q(s_{t+1}a_{t+1}-Q(s_t,a_t)​$)：</strong></p>
<script type="math/tex; mode=display">
Q(s_t,a_t) \Leftarrow Q(s_t,a_t)+\alpha(r_{t+1}+\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t))</script><p><strong>n-steps TD error：</strong></p>
<p><img src="https://www.zhihu.com/equation?tex=R%5E%7B%28n%29%7D%3Dr_%7Bt%2B1%7D%2B%5Cgamma+r_%7Bt%2B2%7D%2B...%2B%5Cgamma+%5E%7Bn-2%7Dr_%7Bt%2Bn-1%7D%2B%5Cgamma+%5E%7Bn-1%7DQ%28s_%7Bt%2Bn%7D%2C+a_%7Bt%2Bn%7D%29++" alt=""></p>
<p><strong>TD(λ) error:</strong></p>
<p><img src="https://www.zhihu.com/equation?tex=R%5E%7B%28%5Clambda+%29%7D%3D%281-%5Clambda+%29%5Csum_%7Bn%7D%5E%7Bi%3D1%7D%7B%5Clambda+%5E%7Bi%7DR%5E%7B%28i%29%7D%7D+" alt=""></p>
<p>事实上，MC error可以视为一个情节任务的max-step TD error。另外，一般来说，在TD error中，n越大，用到的真实回报信息更多，收敛也会越快。</p>
<hr>
<h3 id="五、推荐文章"><a href="#五、推荐文章" class="headerlink" title="五、推荐文章"></a>五、推荐文章</h3><p><a href="https://www.cnblogs.com/pinard/p/9492980.html" target="_blank" rel="noopener">[强化学习（四）用蒙特卡罗法（MC）求解]</a></p>
<p><a href="https://www.cnblogs.com/pinard/p/9529828.html" target="_blank" rel="noopener">强化学习（五）用时序差分法（TD）求解</a></p>
]]></content>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>MPC控制笔记(一)</title>
    <url>/post/10696.html</url>
    <content><![CDATA[<p>笔记参考1：<a href="https://www.youtube.com/watch?v=8U0xiOkDcmw&amp;list=PLn8PRpmsu08ozoeoXgxPSBKLyd4YEHww8" target="_blank" rel="noopener">Understanding Model Predictive Control(Youtube 带自动生成字幕)</a><br>笔记参考2：<a href="https://www.bilibili.com/video/av36438909?from=search&amp;seid=18128345108889923736" target="_blank" rel="noopener">Understanding Model Predictive Control(B站 生肉)</a></p>
<h3 id="一、什么是MPC模型预测控制"><a href="#一、什么是MPC模型预测控制" class="headerlink" title="一、什么是MPC模型预测控制"></a>一、什么是MPC模型预测控制</h3><p>MPC(Model Predict Control)是一种反馈控制(feedback control)算法, 使用模型来预测过程的未来输出。</p>
<p>举例：<br>    <strong>[场景]</strong> 车道保持<br>    <strong>[已知模型]</strong> 车辆模型，比如速度控制， 转向控制对应的偏航量<br>    <strong>[预测]</strong> 根据已知模型和所选的控制策略(action)，进行轨迹预测<br>    <strong>[优化]</strong> 通过优化控制策略，来尽可能的拟合预测的轨迹。</p>
<p>如下图所示为一个MIMO系统u1,u2输入与y1,y2输出相互影响。如果使用PID控制的话，每一个子系统单独设计一个PID控制器，两个相互影响的子系统没有任何的交联，使得系统难以设计，如果像图二一样设计一个较大的系统，则参数较多难以实现，而使用MPC控制器的话可以较好的解决两种问题，综合相互间的影响来设计参数。<br><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1547791941/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190118113340.png" alt="MIMO"><br><a id="more"></a><br><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1547792593/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190118141701.png" alt="MIMO BIG"><br><strong>MPC控制</strong><br><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1547793059/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190118143048.png" alt="MIMO MPC"></p>
<p>此外MPC控制还可以方便的支持各种约束条件，具备一定的预测能力(有点像前馈feedforward控制)。</p>
<h3 id="二、MPC的特点"><a href="#二、MPC的特点" class="headerlink" title="二、MPC的特点"></a>二、MPC的特点</h3><p>综合上述，总结一下MPC的特点：</p>
<ul>
<li>支持MIMO系统，便于构建输入输出间的相互影响参数</li>
<li>支持方便添加约束条件</li>
<li>具有预测能力、</li>
<li>需要较好的处理器和较大的内存，因为需要大量的在线优化，存储大量的变量</li>
</ul>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1547793561/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190118143857.png" alt="Onlineopt"></p>
<h3 id="三、MPC参数选择"><a href="#三、MPC参数选择" class="headerlink" title="三、MPC参数选择"></a>三、MPC参数选择</h3><p>选择一个好的参数不仅影响MPC控制的性能，而且还会影响到MPC每一个timestep内进行在线优化的计算复杂度。这里将会给出关于控制器采样周期、预测及控制范围(prediction and control)、约束及权重。</p>
<h4 id="采样周期的选择"><a href="#采样周期的选择" class="headerlink" title="采样周期的选择"></a>采样周期的选择</h4><p>采样周期过大，则系统反应过慢导致难以及时进行修正控制，而采样周期过小，则会导致系统产生大量的在线优化计算，给系统带来较大的开销。因而建议采样周期设计采用<strong>开环响应时间(10~90%上升时间)的十分之一或二十分之一</strong>：<br><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1547794741/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190118145715.png" alt="timestep"></p>
<h4 id="预测范围-prediction-horizon-的选择"><a href="#预测范围-prediction-horizon-的选择" class="headerlink" title="预测范围(prediction horizon)的选择"></a>预测范围(prediction horizon)的选择</h4><p>预测范围指的是一次优化后预测未来输出的时间步的个数。建议范围：在开环响应时间内采样20-30个样本的范围<br><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1547795210/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190118150514.png" alt="predhorizon"></p>
<h4 id="控制范围-control-horizon-的选择"><a href="#控制范围-control-horizon-的选择" class="headerlink" title="控制范围(control horizon)的选择"></a>控制范围(control horizon)的选择</h4><p>如下图 [k, k+m]范围为控制范围，之后的红色部分称为 held constant，其中控制范围是要通过优化器来进行优化的参数动作。<br><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1547795524/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190118151151.png" alt="ctrlhorizon"></p>
<p>过小的控制范围，可能无法做到较好的控制，而较大的控制范围，比如与预测范围相等，则会导致只有前一部分的控制范围才会有较好的效果，而后一部分的控制范围则收效甚微，而且将带来大量的计算开销。<br><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1547795818/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190118151646.png" alt="consrtgr"></p>
<p>建议控制范围应该在预测范围的10~20%之间,最小值为2~3个timestep时间步<br><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1547796146/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190118152212.png" alt="sujest"></p>
<h4 id="约束"><a href="#约束" class="headerlink" title="约束"></a>约束</h4><p>对于约束，一般分为Hard约束和Soft约束，Hard约束是不可违背必须遵守的，在控制系统中，输入输出都可能会有约束限制，但是在设计时不建议将输入输出都给予Hard约束，因为这两部的约束有可能是有重叠的，导致优化器会产生不可行解。<br>建议输出采用较小容忍度的Soft约束，而输入的话建议输入和输入参数变化率二者之间不要同时为Hard约束，可以一个Hard一个Soft。<br><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1547796758/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190118153216.png" alt="costraint"></p>
<h3 id="四、Linear-MPC-Adaptive-MPC-与-Gain-Scheduled-MPC"><a href="#四、Linear-MPC-Adaptive-MPC-与-Gain-Scheduled-MPC" class="headerlink" title="四、Linear MPC (Adaptive MPC 与 Gain-Scheduled MPC)"></a>四、Linear MPC (Adaptive MPC 与 Gain-Scheduled MPC)</h3><p>Linear MPC适用于:<br><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1547799817/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190118162326.png" alt=""><br>对于非线性系统而言,需要在不同的operating point处进行线性化处理如下图。<br><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1547798225/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190118155646.png" alt="op"></p>
<h4 id="Adaptive-MPC"><a href="#Adaptive-MPC" class="headerlink" title="Adaptive MPC"></a>Adaptive MPC</h4><p>在 Adaptive MPC中，当operating condition发生变化的时候，需要进行一次近似线性化计算，在每个时间步中，使用其近似线性模型来更新内部的平台模型(plant model，比如飞控模型，自行车模型等)。<br>在 Adaptive MPC中，在不同的operating point条件下，其优化问题的结构保持不变，即状态数量，约束数量不会随着operating condition而改变。<br><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1547798140/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190118155524.png" alt="adaptivempc"></p>
<h4 id="Gain-Scheduled-MPC"><a href="#Gain-Scheduled-MPC" class="headerlink" title="Gain-Scheduled MPC"></a>Gain-Scheduled MPC</h4><p>在 Gain-Scheduled MPC中，在不同的operating point条件下，其优化问题的结构会发生变化，需要为每一个operating point构建一个MPC控制器，且相互之间独立，其状态数量约束数量也可能不同。<br>在 Gain-Scheduled MPC模式下，需要设计调度算法来切换不同的MPC模型。<br><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1547799440/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190118161704.png" alt="GMPC"></p>
<h4 id="二者选型"><a href="#二者选型" class="headerlink" title="二者选型"></a>二者选型</h4><p><strong>[Adaptive MPC]</strong> 当能够构建平台(如飞行器，自动车等)的runtime线性模型，且在不同的operating point下优化问题的结构不变。</p>
<p><strong>[Gain-Scheduled MPC]</strong> 当能够构建平台(如飞行器，自动车等)的runtime线性模型，且在不同的operating point下优化问题的结构发生变化。</p>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1547800226/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190118163015.png" alt=""></p>
<h3 id="五、Non-Linear-MPC-Adaptive-MPC-与-Gain-Scheduled-MPC"><a href="#五、Non-Linear-MPC-Adaptive-MPC-与-Gain-Scheduled-MPC" class="headerlink" title="五、Non-Linear MPC (Adaptive MPC 与 Gain-Scheduled MPC)"></a>五、Non-Linear MPC (Adaptive MPC 与 Gain-Scheduled MPC)</h3><p>Non-Linear MPC适用于(相对强大，能够提供更准确的预测能力，与决策支持，但是非线性优化的计算开销较大)<br><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1547799922/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190118162446.png" alt=""></p>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1547800315/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190118163146.png" alt=""></p>
]]></content>
      <tags>
        <tag>MPC</tag>
      </tags>
  </entry>
  <entry>
    <title>Generative Adversarial Imitation Learning(GAIL) 论文阅读笔记</title>
    <url>/post/21152.html</url>
    <content><![CDATA[<h2 id="GAIL优点"><a href="#GAIL优点" class="headerlink" title="GAIL优点"></a>GAIL优点</h2><p>相较于IRL，可以省略很多中间步骤，比如通过IRL来学习Reward系统，再通过Reward系统来进行RL学习policy，GAIL可以直接通过expert trajectory 来直接学习policy。<br><a id="more"></a></p>
<h2 id="IRL"><a href="#IRL" class="headerlink" title="IRL"></a>IRL</h2><p>假定cost function的集合为$C $, $\pi_E$为专家策略(一系列采集来的专家策略样本)。IRL的目标是maximum causal entropy IRL<br><img src="https://leanote.com/api/file/getImage?fileId=5c307660ab64414182000bc2" alt="title"><br>其中<img src="https://leanote.com/api/file/getImage?fileId=5c307836ab64414182000c1c" alt="title">是策略π的γ-discounted causal entropy，对于每一个cost function $c \in C$都有对于专家策略的cost最小，而其他策略的cost都相对较大。式(1)中包含了一个RL过程，实现了cost function到可以最小化期望cost误差的高熵策略的映射：<br><img src="https://leanote.com/api/file/getImage?fileId=5c307771ab64414182000bf7" alt="title"></p>
]]></content>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>无外网情况下RPM方式安装MySQL5.6</title>
    <url>/post/20206.html</url>
    <content><![CDATA[<p>RPM方式安装MySQL5.6<br>a. 检查MySQL及相关RPM包，是否安装，如果有安装，则移除（rpm –e 名称）<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# rpm -qa | grep -i mysql</span><br><span class="line">mysql-libs-5.1.66-2.el6_3.x86_64</span><br><span class="line">[root@localhost ~]# yum -y remove mysql-libs*</span><br></pre></td></tr></table></figure></p>
<p>b. 下载Linux对应的RPM包，如：CentOS6.4_64对应的RPM包，如下：这里给出我下载好的包，官网下载巨慢。。有可能还需要一个perl库的依赖，这里一并给出。链接: <a href="http://pan.baidu.com/s/1skFrEK9" target="_blank" rel="noopener">http://pan.baidu.com/s/1skFrEK9</a> 密码: apza<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost rpm]# ll</span><br><span class="line">total 74364</span><br><span class="line">-rw-r--r--. 1 root root 18442536 Dec 11 20:19 MySQL-client-5.6.15-1.el6.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 root root  3340660 Dec 11 20:06 MySQL-devel-5.6.15-1.el6.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 root root 54360600 Dec 11 20:03 MySQL-server-5.6.15-1.el6.x86_64.rpm</span><br></pre></td></tr></table></figure></p>
<a id="more"></a>
<p>c. 安装MySQL(如有必要需要安装<code>perl-libs-5.10.1-141.el6_7.1.x86_64</code>)<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost rpm]# rpm -ivh MySQL-server-5.6.15-1.el6.x86_64.rpm</span><br><span class="line">[root@localhost rpm]# rpm -ivh MySQL-devel-5.6.15-1.el6.x86_64.rpm</span><br><span class="line">[root@localhost rpm]# rpm -ivh MySQL-client-5.6.15-1.el6.x86_64.rpm</span><br><span class="line"><span class="meta">#</span><span class="bash">修改配置文件位置</span></span><br><span class="line">[root@localhost rpm]# cp /usr/share/mysql/my-default.cnf /etc/my.cnf</span><br></pre></td></tr></table></figure></p>
<p>d. 在<code>my.cnf</code>文件中的<code>[mysqld]</code>下设置这一行：<code>datadir = /usr/local/mysql/var</code></p>
<p>e. 初始化MySQL及设置密码<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost rpm]# /usr/bin/mysql_install_db</span><br><span class="line">[root@localhost rpm]# service mysql start</span><br><span class="line">[root@localhost rpm]# cat /root/.mysql_secret  #查看root账号密码,若无此文件可以直接使用无密码登录，若无密码登录失败，则需要在my.cnf文件中加入skip-grant-tables，并重启mysql服务</span><br><span class="line"><span class="meta">#</span><span class="bash"> The random password <span class="built_in">set</span> <span class="keyword">for</span> the root user at Wed Dec 11 23:32:50 2013 (<span class="built_in">local</span> time): qKTaFZnl</span></span><br><span class="line">[root@localhost ~]# mysql -uroot –pqKTaFZnl</span><br><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> SET PASSWORD = PASSWORD(<span class="string">'123456'</span>);    <span class="comment">#设置密码为123456</span></span></span><br><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> <span class="built_in">exit</span></span></span><br><span class="line">[root@localhost ~]# mysql -uroot -p123456</span><br></pre></td></tr></table></figure></p>
<p>f. 允许远程登陆<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> use mysql;</span></span><br><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> select host,user,password from user;</span></span><br><span class="line">+-----------------------+------+-------------------------------------------+</span><br><span class="line">| host                  | user | password                                  |</span><br><span class="line">+-----------------------+------+-------------------------------------------+</span><br><span class="line">| localhost             | root | *6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9 |</span><br><span class="line">| localhost.localdomain | root | *1237E2CE819C427B0D8174456DD83C47480D37E8 |</span><br><span class="line">| 127.0.0.1             | root | *1237E2CE819C427B0D8174456DD83C47480D37E8 |</span><br><span class="line">| ::1                   | root | *1237E2CE819C427B0D8174456DD83C47480D37E8 |</span><br><span class="line">+-----------------------+------+-------------------------------------------+</span><br><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> update user <span class="built_in">set</span> password=password(<span class="string">'123456'</span>) <span class="built_in">where</span> user=<span class="string">'root'</span>;</span></span><br><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> update user <span class="built_in">set</span> host=<span class="string">'%'</span> <span class="built_in">where</span> user=<span class="string">'root'</span> and host=<span class="string">'localhost'</span>;</span></span><br><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> flush privileges;</span></span><br><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> <span class="built_in">exit</span></span></span><br></pre></td></tr></table></figure></p>
<p>g. 设置开机自启动<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# chkconfig mysql on</span><br><span class="line">[root@localhost ~]# chkconfig --list | grep mysql</span><br><span class="line">mysql           0:off   1:off   2:on    3:on    4:on    5:on    6:off</span><br></pre></td></tr></table></figure></p>
<p>h.MySQL的默认安装位置<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/var/lib/mysql/               #数据库目录</span><br><span class="line">/usr/share/mysql              #配置文件目录</span><br><span class="line">/usr/bin                     #相关命令目录</span><br><span class="line">/etc/init.d/mysql              #启动脚本</span><br></pre></td></tr></table></figure></p>
<p>i.修改字符集和数据存储路径<br>配置/etc/my.cnf文件,修改数据存放路径、mysql.sock路径以及默认编码utf-8.<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[client]</span><br><span class="line">password        = 123456</span><br><span class="line">port            = 3306</span><br><span class="line">default-character-set=utf8</span><br><span class="line">[mysqld]</span><br><span class="line">port            = 3306</span><br><span class="line">character_set_server=utf8</span><br><span class="line">character_set_client=utf8</span><br><span class="line">collation-server=utf8_general_ci</span><br><span class="line"><span class="meta">#</span><span class="bash">(注意linux下mysql安装完后是默认：表名区分大小写，列名不区分大小写； 0：区分大小写，1：不区分大小写)</span></span><br><span class="line">lower_case_table_names=1</span><br><span class="line"><span class="meta">#</span><span class="bash">(设置最大连接数，默认为 151，MySQL服务器允许的最大连接数16384; )</span></span><br><span class="line">max_connections=1000</span><br><span class="line">[mysql]</span><br><span class="line">default-character-set = utf8</span><br></pre></td></tr></table></figure></p>
<p>j. 查看字符集<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">show variables like '%collation%';</span><br><span class="line">show variables like '%char%';</span><br></pre></td></tr></table></figure></p>
]]></content>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker给运行中的容器添加映射端口</title>
    <url>/post/46919.html</url>
    <content><![CDATA[<h1 id="声明"><a href="#声明" class="headerlink" title="声明"></a>声明</h1><p>​    这篇文章选自<a href="http://udn.yyuap.com/thread-27147-1-1.html" target="_blank" rel="noopener">[教程技巧] DOCKER 给运行中的容器添加映射端口</a></p>
<h1 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h1><p><strong>Docker 给运行中的容器添加映射端口方法1</strong>　　</p>
<ol>
<li><p>获得容器IP</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker inspect `container_name` | grep IPAddress</span></span><br></pre></td></tr></table></figure>
<p>  比如我的容器叫<code>mysqlserver</code>么就输入下列代码来获取该容器的ip地址</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker inspect mysqlserver | grep IPAddress</span></span><br></pre></td></tr></table></figure>
<p>  执行完之后会发现我的<code>mysqlserver</code>docker容器的ip地址为<code>192.168.0.2</code></p>
<p>  ​</p>
<a id="more"></a></li>
<li><p>iptables转发端口</p>
<p> 比如我将容器的3306端口映射到主机的37221端口，那么ip对应就写入我的docker容器IP即可</p>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">iptables -t nat -A DOCKER -p tcp --dport 37221 -j DNAT --to-destination 192.168.0.2:3306</span><br></pre></td></tr></table></figure>
<p><strong>Docker 给运行中的容器添加映射端口方法2</strong></p>
<ol>
<li><p>提交一个运行中的容器为镜像</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker commit containerid foo/live</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>运行镜像并添加端口</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker run -d -p 8000:80  foo/live /bin/bash</span></span><br></pre></td></tr></table></figure>
<p>​</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title>使用Scala基于词法单元的解析器定制EBNF范式文法解析</title>
    <url>/post/33864.html</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>近期在做<strong>Oracle</strong>迁移到<strong>Spark</strong>平台的项目上遇到了一些平台公式翻译为<strong>SparkSQL(on Hive)</strong>的需求，而<strong>Spark</strong>采用亲妈语言<strong>Scala</strong>进行开发。分析过大概需求过后，拟使用编译原理中的<strong>EBNF范式</strong>模式，进行基于词法的文法解析。于是拟采用传统的<strong>正则词法解析</strong>到<strong>EBNF文法解析</strong>的套路来实现,直到发现了<code>StandardTokenParsers</code>这个Scala基于词法单元的解析器类。</p>
<h1 id="平台公式及翻译后的SparkSQL"><a href="#平台公式及翻译后的SparkSQL" class="headerlink" title="平台公式及翻译后的SparkSQL"></a>平台公式及翻译后的SparkSQL</h1><p>平台公式的样子如下所示：<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">if (XX1_m001[D003]="邢おb7肮α䵵薇" || XX1_m001[H003]&lt;"2") &amp;&amp; XX1_m001[D005]!="wed" then XX1_m001[H022,COUNT]</span><br></pre></td></tr></table></figure></p>
<p>这里面字段值”邢おb7肮α䵵薇”为这个的目的是为了测试各种字符集是否都能匹配满足。<br>那么对应的<strong>SparkSQL</strong>应该是这个样子的,由于是使用的<strong>Hive on Spark</strong>，因而长得跟<strong>Oracle的SQL语句</strong>差不多：<br><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">COUNT</span>(H022) <span class="keyword">FROM</span> XX1_m001 <span class="keyword">WHERE</span> (XX1_m001.D003=<span class="string">'邢おb7肮α䵵薇'</span> <span class="keyword">OR</span>  XX1_m001.H003&lt;<span class="string">'2'</span>)  <span class="keyword">AND</span>  XX1_m001.D005&lt;<span class="string">'wed'</span></span><br></pre></td></tr></table></figure></p>
<p>总体而言比较简单，因为我只是想在这里做一个Demo。<br><a id="more"></a></p>
<h1 id="平台公式的EBNF范式及词法解析设计"><a href="#平台公式的EBNF范式及词法解析设计" class="headerlink" title="平台公式的EBNF范式及词法解析设计"></a>平台公式的EBNF范式及词法解析设计</h1><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">expr-condition ::= tableName <span class="string">"["</span> valueName <span class="string">"]"</span> comparator <span class="type">Condition</span></span><br><span class="line">expr-front ::= expr-condition ((<span class="string">"&amp;&amp;"</span>|<span class="string">"||"</span>)expr-front)*</span><br><span class="line">expr-back ::= tableName <span class="string">"["</span> valueName <span class="string">","</span> operator <span class="string">"]"</span></span><br><span class="line">expr ::= <span class="string">"if"</span> expr-front <span class="string">"then"</span> expr-back</span><br></pre></td></tr></table></figure>
<p>其中词法定义如下<br><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">operator =&gt; [<span class="type">SUM</span>,<span class="type">COUNT</span>]</span><br><span class="line">tableName,valueName =&gt;ident  #ident为关键字</span><br><span class="line">comparator =&gt; [<span class="string">"="</span>,<span class="string">"&gt;="</span>,<span class="string">"&lt;="</span>,<span class="string">"&gt;"</span>,<span class="string">"&lt;"</span>,<span class="string">"!="</span>]</span><br><span class="line"><span class="type">Condition</span> =&gt; stringLit  #stringLit为字符串常量</span><br></pre></td></tr></table></figure></p>
<h1 id="使用Scala基于词法单元的解析器解析上述EBNF文法"><a href="#使用Scala基于词法单元的解析器解析上述EBNF文法" class="headerlink" title="使用Scala基于词法单元的解析器解析上述EBNF文法"></a>使用Scala基于词法单元的解析器解析上述EBNF文法</h1><p>Scala基于词法单元的解析器是需要继承<code>StandardTokenParsers</code>这个类的，该类提供了很方便的解析函数，以及词法集合。<br>我们可以通过使用<code>lexical.delimiters</code>列表来存放在文法翻译器执行过程中遇到的分隔符，使用<code>lexical.reserved</code>列表来存放执行过程中的关键字。<br>比如，我们参照平台公式，看到<code>&quot;=&quot;,&quot;&gt;=&quot;,&quot;&lt;=&quot;,&quot;&gt;&quot;,&quot;&lt;&quot;,&quot;!=&quot;,&quot;&amp;&amp;&quot;,&quot;||&quot;,&quot;[&quot;,&quot;]&quot;,&quot;,&quot;,&quot;(&quot;,&quot;)&quot;</code>这些都是分隔符，其实我们也可以把<code>&quot;=&quot;,&quot;&gt;=&quot;,&quot;&lt;=&quot;,&quot;&gt;&quot;,&quot;&lt;&quot;,&quot;!=&quot;,&quot;&amp;&amp;&quot;,&quot;||&quot;</code>当做是关键字，但是我习惯上将带有英文字母的单词作为关键字处理。因而，这里的关键字集合便是<code>&quot;if&quot;,&quot;then&quot;,&quot;SUM&quot;,&quot;COUNT&quot;</code>这些。<br>表现在代码中是酱紫的：<br><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">lexical.delimiters += (<span class="string">"="</span>,<span class="string">"&gt;="</span>,<span class="string">"&lt;="</span>,<span class="string">"&gt;"</span>,<span class="string">"&lt;"</span>,<span class="string">"!="</span>,<span class="string">"&amp;&amp;"</span>,<span class="string">"||"</span>,<span class="string">"["</span>,<span class="string">"]"</span>,<span class="string">","</span>,<span class="string">"("</span>,<span class="string">")"</span>)</span><br><span class="line">lexical.reserved   += (<span class="string">"if"</span>,<span class="string">"then"</span>,<span class="string">"SUM"</span>,<span class="string">"COUNT"</span>)</span><br></pre></td></tr></table></figure></p>
<p>是不是so easy~。<br>我们再来看一下如何使用基于词法单元的解析器解析前面我们设计的EBNF文法呢。我在这里先上代码：<br><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ExprParsre</span> <span class="keyword">extends</span> <span class="title">StandardTokenParsers</span></span>&#123;</span><br><span class="line">  lexical.delimiters += (<span class="string">"="</span>,<span class="string">"&gt;="</span>,<span class="string">"&lt;="</span>,<span class="string">"&gt;"</span>,<span class="string">"&lt;"</span>,<span class="string">"!="</span>,<span class="string">"&amp;&amp;"</span>,<span class="string">"||"</span>,<span class="string">"["</span>,<span class="string">"]"</span>,<span class="string">","</span>,<span class="string">"("</span>,<span class="string">")"</span>)</span><br><span class="line">  lexical.reserved   += (<span class="string">"if"</span>,<span class="string">"then"</span>,<span class="string">"SUM"</span>,<span class="string">"COUNT"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">expr</span></span>: <span class="type">Parser</span>[<span class="type">String</span>] = <span class="string">"if"</span> ~ expr_front ~ <span class="string">"then"</span> ~ expr_back ^^&#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="string">"if"</span> ~ exp1 ~ <span class="string">"then"</span> ~ exp2 =&gt; exp2 + <span class="string">" WHERE "</span> +exp1</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">expr_priority</span></span>: <span class="type">Parser</span>[<span class="type">String</span>] = opt(<span class="string">"("</span>) ~ expr_condition ~ opt(<span class="string">")"</span>) ^^&#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Some</span>(<span class="string">"("</span>) ~ conditions ~ <span class="type">Some</span>(<span class="string">")"</span>) =&gt; <span class="string">"("</span> + conditions +<span class="string">")"</span></span><br><span class="line">    <span class="keyword">case</span> <span class="type">Some</span>(<span class="string">"("</span>) ~ conditions ~ <span class="type">None</span> =&gt; <span class="string">"("</span> + conditions</span><br><span class="line">    <span class="keyword">case</span> <span class="type">None</span> ~ conditions ~ <span class="type">Some</span>(<span class="string">")"</span>) =&gt; conditions +<span class="string">")"</span></span><br><span class="line">    <span class="keyword">case</span> <span class="type">None</span> ~ conditions ~ <span class="type">None</span> =&gt; conditions</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">expr_condition</span></span>: <span class="type">Parser</span>[<span class="type">String</span>] = ident ~ <span class="string">"["</span> ~ ident ~ <span class="string">"]"</span> ~ (<span class="string">"="</span>|<span class="string">"&gt;="</span>|<span class="string">"&lt;="</span>|<span class="string">"&gt;"</span>|<span class="string">"&lt;"</span>|<span class="string">"!="</span>) ~ stringLit ^^&#123;</span><br><span class="line">    <span class="keyword">case</span> ident1~<span class="string">"["</span>~ident2~<span class="string">"]"</span>~<span class="string">"="</span>~stringList =&gt; ident1 + <span class="string">"."</span> + ident2 +<span class="string">"='"</span> + stringList +<span class="string">"'"</span></span><br><span class="line">    <span class="keyword">case</span> ident1~<span class="string">"["</span>~ident2~<span class="string">"]"</span>~<span class="string">"&gt;="</span>~stringList =&gt; ident1 + <span class="string">"."</span> + ident2 +<span class="string">"&gt;='"</span> + stringList +<span class="string">"'"</span></span><br><span class="line">    <span class="keyword">case</span> ident1~<span class="string">"["</span>~ident2~<span class="string">"]"</span>~<span class="string">"&lt;="</span>~stringList =&gt; ident1 + <span class="string">"."</span> + ident2 +<span class="string">"&lt;='"</span> + stringList +<span class="string">"'"</span></span><br><span class="line">    <span class="keyword">case</span> ident1~<span class="string">"["</span>~ident2~<span class="string">"]"</span>~<span class="string">"&gt;"</span>~stringList =&gt; ident1 + <span class="string">"."</span> + ident2 +<span class="string">"&gt;'"</span> + stringList +<span class="string">"'"</span></span><br><span class="line">    <span class="keyword">case</span> ident1~<span class="string">"["</span>~ident2~<span class="string">"]"</span>~<span class="string">"&lt;"</span>~stringList =&gt; ident1 + <span class="string">"."</span> + ident2 +<span class="string">"&lt;'"</span> + stringList +<span class="string">"'"</span></span><br><span class="line">    <span class="keyword">case</span> ident1~<span class="string">"["</span>~ident2~<span class="string">"]"</span>~<span class="string">"!="</span>~stringList =&gt; ident1 + <span class="string">"."</span> + ident2 +<span class="string">"!='"</span> + stringList +<span class="string">"'"</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">comparator</span></span>: <span class="type">Parser</span>[<span class="type">String</span>] = (<span class="string">"&amp;&amp;"</span>|<span class="string">"||"</span>) ^^&#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="string">"&amp;&amp;"</span> =&gt; <span class="string">" AND "</span></span><br><span class="line">    <span class="keyword">case</span> <span class="string">"||"</span> =&gt; <span class="string">" OR "</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">expr_front</span></span>: <span class="type">Parser</span>[<span class="type">String</span>] = expr_priority ~ rep(comparator ~ expr_priority) ^^&#123;</span><br><span class="line">    <span class="keyword">case</span> exp1 ~ exp2  =&gt; exp1 +  exp2.map(x =&gt;&#123;x._1 + <span class="string">" "</span> + x._2&#125;).mkString(<span class="string">" "</span>)  </span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">expr_back</span></span>: <span class="type">Parser</span>[<span class="type">String</span>] = ident ~ <span class="string">"["</span> ~ ident ~ <span class="string">","</span> ~ (<span class="string">"SUM"</span>|<span class="string">"COUNT"</span>) ~ <span class="string">"]"</span> ^^ &#123;</span><br><span class="line">    <span class="keyword">case</span> ident1~<span class="string">"["</span>~ident2~<span class="string">","</span>~<span class="string">"COUNT"</span>~<span class="string">"]"</span> =&gt; <span class="string">"SELECT COUNT("</span>+ ident2.toString() +<span class="string">") FROM "</span> + ident1.toString()</span><br><span class="line">    <span class="keyword">case</span> ident1~<span class="string">"["</span>~ident2~<span class="string">","</span>~<span class="string">"SUM"</span>~<span class="string">"]"</span> =&gt; <span class="string">"SELECT SUM("</span>+ ident2.toString() +<span class="string">") FROM "</span> + ident1.toString()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">parserAll</span></span>[<span class="type">T</span>]( p : <span class="type">Parser</span>[<span class="type">T</span>], input :<span class="type">String</span>) = &#123;</span><br><span class="line">    phrase(p)( <span class="keyword">new</span> lexical.<span class="type">Scanner</span>(input))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
]]></content>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux内存占用过高？非也</title>
    <url>/post/24621.html</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><blockquote>
<p>今天在我的RPi2上测试GPIO程序，忽然发现机器超卡，重启之后依然如此。于是我top了一下发现了一个问题就是内存爆满！！可我还啥都没干呢这是咋了？于是我呵呵地开始查资料，终于找到了问题所在。</p>
</blockquote>
<h1 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h1><ul>
<li>先来在阿里的ECS上top一下感受内存爆满的感觉，终端输入<code>top</code></li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#top</span></span><br></pre></td></tr></table></figure>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1548332209/1.png" alt="图片标题"></p>
<a id="more"></a>
<ul>
<li>结合操作系统，计组等课程，以及多年以来windows培养给我的直觉，认为<strong>0.96G（1016272K）</strong>的总内存，使用了<strong>0.84G(880960K)</strong>的内存，使用率高达<strong>88%</strong>。然而我还啥都没干，怎么会这样呢？<br>仔细查看还会发现后面有一个<strong>buffers</strong>，Swap后面还有一个<strong>Cached Mem</strong>。<br>现在我们用<code>free</code>来观察下</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#free -m</span><br></pre></td></tr></table></figure>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1548332233/2.png" alt="图片标题"></p>
<ul>
<li>虽然Mem显示了<strong>0.9G</strong>左右的used，但是<strong>-/+ buffers/cache</strong>(减去buffers和cache的结果)可以看到，当前进程实际占用内存是<strong>0.24G(256348K)</strong>，而可用空闲（free）内存为<strong>0.72G(759924K)</strong>。<br>可以这么理解：在linux的内存分配机制中，优先使用物理内存，当物理内存还有空闲时（还够用），不会释放其占用内存，就算占用内存的程序已经被关闭了，该程序所占用的内存用来做缓存使用，对于开启过的程序、或是读取刚存取过得数据会比较快。<br><br></li>
<li><p>如上面的例子：共<strong>1G</strong>的内存，<strong>0.9G</strong>被占用，但是<strong>buffer</strong>和<strong>cached mem</strong>部分作为缓存，可以使用命中率的方式提高使用效率，而且这部分缓存是根据指令随时可以释放的，我们可以认为这部分内存没有实际被使用，也可以认为它是空闲的。<br>因此查看目前进程正在实际被使用的内存，是<code>used-(buffers+cache)</code>，也可以认为如果swap没有大量使用，mem还是够用的，只有mem被当前进程实际占用完（没有了buffers和cache），才会使用到swap的。<br><br></p>
</li>
<li><p>再举个栗子： 这个是我在RPi一群看到的一个群友发的探针监测截图<br><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1548332258/3.png" alt="图片标题"></p>
</li>
<li><p>观察内存使用状况一栏，发现物理内存功<strong>925.89M</strong>，已用<strong>911.74M</strong>，Cache化的内存是<strong>676.46M</strong>，Buffers为<strong>61.3M</strong>，现在用上述公式：</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">真实的内存使用=used-(buffers+cache)</span><br></pre></td></tr></table></figure>
<p><br></p>
<p>带入：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">真实使用内存 = 911.74-676.46-61.3 = 173.98</span><br><span class="line">与第三行的灰条的173.98相符</span><br></pre></td></tr></table></figure>
<p><br></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><blockquote>
<p>很高兴对于linux的内存分配有了新的认识 多谢一群的 粵-打雜小白-503 Service Unavailable 的技术支持 多谢Licess’s Blog的精彩分析</p>
</blockquote>
]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>在raspbian-jessie上搭建nat123自启动</title>
    <url>/post/34931.html</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p><strong>什么是nat123? </strong><br>    现在我们的树莓派都是在路由器内网里面的，需要使用nat123来实现外网映射(类似花生壳)，但是nat123免费好用，这个原理在计算机网络中叫做隧道或者叫穿透。那么本文主要讲述如何在树莓派上配置nat123。</p>
<p><strong>必要环境 </strong></p>
<p>​    现在本文所用的平台是树莓派2代b+，其实那个版本的RPi都OK，主要是raspbian-jessie的系统，不过貌似是raspbian的系统都适合使用，这里主要是nat123的环境必须配置好。</p>
<ul>
<li>安装好mono环境</li>
</ul>
<ul>
<li>安装好nat123客户端</li>
</ul>
<p>本文中我的nat123客户端安装在了官网所指示的<code>/mnt</code> 目录下，如图。<br><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1548332305/QQ_E6_88_AA_E5_9B_BE20160421001227.png" alt="图片标题"><br><a id="more"></a></p>
<h1 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h1><p>1.安装nat123客户端<br>    首先在这里还是要给出官网所给的安装方法<a href="http://www.nat123.com/pages_17_600.jsp" target="_blank" rel="noopener">linux下安装nat123客户端</a>，但是按照官网所述的方法，我总是卡在mono的安装过程上，如果哪位读者能够用那种方法配置成功请您在底下mark我一下~~。下面我说一下配置过程。</p>
<ul>
<li><p>安装mono</p>
<p>​    这里可能由于是我的软件源出了问题，无论如何也不能按照官网的方式在我的树莓派上装mono环境，那么我只好采取mono源码编译的方式来安装。</p>
<p>​    简单说一下每个步骤，首先通过<code>wget</code>获取mono源码，如果没有wget的请手动执行<code>sudo apt-get install wget</code>进行安装，然后使用<code>tar</code>解压，然后<code>cd</code>进入解压文件夹，然后使用<code>./configure --prefix=/usr/local</code>配置编译安装路径，最后<code>make</code>编译，<code>make install</code>安装mono。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> wget http://download.mono-project.com/sources/mono/mono-4.0.1.44.tar.bz2</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> tar -xvf mono-4.0.1.44.tar.bz2</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> mono-4.0.1</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ./configure --prefix=/usr/<span class="built_in">local</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> make</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> make install</span></span><br></pre></td></tr></table></figure>
<p>​    这样执行完成后输入<code>mono -V</code>出现版本信息就OK了，可以按照官网的说明继续执行。那么在这里我继续总结一下nat123的全部安装步骤。<br>&lt;/br&gt;</p>
</li>
<li><p>安装nat123客户端(本文安装路径为<code>/mnt</code>)</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">cd</span>  /mnt  </span><br><span class="line">$ wget  http://www.nat123.com/down/nat123linux.tar.gz </span><br><span class="line">$ tar  -zxvf  nat123linux.tar.gz</span><br></pre></td></tr></table></figure>
</li>
<li><p>客户端启动</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo apt-get install screen </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> /mnt  </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> screen -S nat123  </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo mono nat123linux.sh</span></span><br></pre></td></tr></table></figure>
<p>​       这里需要注意的是，在执行<code>screen  -S  nat123</code> 语句时-S是大写的S，如提示没有screen，则执行<code>sudo apt-get install screen</code>安装screen后再执行screen。然后就是要特别注意mono那句话前面一定要加sudo，除非你是root用户，否则会报奇葩错误。如果一切顺利，你将看到下面的样子。</p>
<p> <img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1548332330/QQ_E6_88_AA_E5_9B_BE20160421003933.png" alt="图片标题"></p>
<p>​       这里<code>please enter your nat123 username(enter x to exit):</code> 输入你的nat123用户名<code>please enter your nat123 password(enter x to exit):</code>输入你的nat123密码，吐槽坑点，密码竟然是明文。</p>
<p>​       如果你的用户名密码都正确，你会看到下面的界面(官网盗图)，按住Ctrl键，并依次先按A，再按D，退出当前窗口就OK了。</p>
</li>
</ul>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1548332368/QQ_E6_88_AA_E5_9B_BE20160421004234.png" alt="图片标题"><br>     那么此时基础环境就算搭建完了，下面你需要的是一个nat123的端口映射配置。</p>
<p>2.配置nat123端口</p>
<ul>
<li><p>首先你需要登录你的nat123官网账户 <a href="http://www.nat123.com/" target="_blank" rel="noopener">点我带你飞</a></p>
</li>
<li><p>然后在用户中心中选择端口映射添加</p>
</li>
</ul>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1548332379/QQ_E6_88_AA_E5_9B_BE20160421004747.png" alt="图片标题"></p>
<ul>
<li>然后按照下图所示进行填写</li>
</ul>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1548332392/QQ_E6_88_AA_E5_9B_BE20160421004909.png" alt="图片标题"><br>  ​        这里说明一下，应用类型填写的是<strong>其他(非网站)</strong>，映射路线默认选择nat123，除非你有VIP专线，应用名称就随你咯。值得注意的是，这里的内网端口这样填写，由于我做映射的目的是能够在外网SSH到我的树莓派，所以需要将我的localhost上的22端口映射出去，因而这里我将内网端口设为22，内网地址设为localhost。</p>
<p>  ​        再举个栗子，现在我的树莓派上有一个运行在localhost的5000端口上的flask服务器(一个python框架)程序，那么我想在外网请求这个服务器，那么我们就将内网端口设为5000，内网地址设为localhost。</p>
<p>  ​         然后其他的填写默认就行，如果像做域名解析的，请按照官网的自主域名到万网上去解析，这里不再多言~。</p>
<ul>
<li>点击确认保存</li>
</ul>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1548332420/QQ_E6_88_AA_E5_9B_BE20160421005439.png" alt="图片标题"><br>  ​        那么从上图可以看到现在我有两个端口映射一个是flask的5000端口映射，一个是SSH的22端口映射。那么如果一切顺利的话(已经开了nat123客户端服务并且做好了端口映射)那么现在你就可以通过在SSH应用中输入你的<strong>外网域名</strong>，和<strong>外网端口</strong>连接到你的树莓派中去了，或者通过<strong>外网域名</strong>，<strong>外网端口</strong>访问你的服务器应用。</p>
<p>3.将树莓派上的nat123服务设为开机自启动或断网重连</p>
<p>​    其实到上面为止，已经达到了我们想要的外网访问内网服务器的效果了，那么现在我们还不满足，想让他只用一次配置就可以永久稳定的生效，那么我们就把它配置成开机自启动并支持断网重连。之前看了官网上的方法，发现并没卵用，然后在刷官方论坛的时候发现了一个解决方案(<a href="http://www.natbbs.com/thread-6289-2-1.html" target="_blank" rel="noopener">论坛15楼的办法</a>)，虽然可能是因为版本老了的原因，直接用并不成功，但是给了我一个很好的思路。下面介绍一下我的方法。</p>
<ul>
<li>首先<code>sudo apt-get install expect</code>安装expect支持</li>
</ul>
<ul>
<li><p>然后进入nat123安装目录(本文是<code>/mnt</code>)，新建一个脚本起名为expect.sh，执行这条命令<code>sudo vim /mnt/expect.sh</code></p>
</li>
<li><p>然后将下列代码输入到这个脚本中去:</p>
<p>​       这里需要注意的是<code>username=&quot;&quot;</code>“”里面输入你的nat123用户名<code>mypwd=&quot;&quot;</code>“”里面输入你的nat123账号密码，值得一提的是，请看<code>\&quot;please enter your nat123 username(enter x to exit):\&quot; {</code>和<code>\&quot;please enter your nat123 password(enter x to exit):\&quot; {</code>这两句话一定要和你执行了<code>sudo mono nat123linux.sh</code>之后的输入提示相一致，具体在哪呢请看代码下面的那张图片。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">username="你的nat123用户名"</span><br><span class="line">mypwd="你的nat123密码"</span><br><span class="line">cmdnat123="sudo mono /mnt/nat123linux.sh"</span><br><span class="line"></span><br><span class="line">expect -c"</span><br><span class="line">spawn $cmdnat123</span><br><span class="line">while &#123; 1 &#125; &#123;</span><br><span class="line">        expect &#123;</span><br><span class="line">               \"please enter your nat123 username(enter x to exit):\" &#123;</span><br><span class="line">                        send \"$username\r\";</span><br><span class="line">                &#125;</span><br><span class="line">                \"please enter your nat123 password(enter x to exit):\" &#123;</span><br><span class="line">                        send \"$mypwd\r\";</span><br><span class="line">                &#125;</span><br><span class="line">                eof &#123;</span><br><span class="line">                        send \"exit\r\";</span><br><span class="line">                &#125;</span><br><span class="line">          &#125;</span><br><span class="line">          sleep 5;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><img src="https://res.cloudinary.com/djhkiiiap/image/upload/v1548332456/QQ_E6_88_AA_E5_9B_BE20160421011051.png" alt="图片标题"></p>
<ul>
<li><p>写好之后可以执行<code>bash expect.sh</code>语句来执行以下这个脚本，如果执行之后它带你来到了让你输入ctrl+AD退出的那个界面，就说明这个脚本写的成功了。</p>
</li>
<li><p>然后将脚本执行写入开机启动bash</p>
<p>打开rc.local写入开机执行命令。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo vim /etc/rc.local</span></span><br></pre></td></tr></table></figure>
<p>这里在<code>exit(0)</code>这句之前一行写上<code>sudo bash /mnt/expect.sh</code>就OK了</p>
</li>
<li><p>那么如果顺利的话重启树莓派等待30s左右时间，你就可以直接通过外网访问你的树莓派了~，然后你也可以把网线拔了重插，等个5s左右，发现也可以重连，那么就一切OK了。</p>
</li>
</ul>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>此文第二遍写于2016年4月21日10:10:13，倒霉的typora昨晚上写到2点多变成乱码了。</p>
]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>我的博客</title>
    <url>/post/57092.html</url>
    <content><![CDATA[]]></content>
  </entry>
</search>
