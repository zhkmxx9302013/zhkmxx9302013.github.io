<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[[论文] Data-Efficient Hierarchical Reinforcement Learning(HIRO)]]></title>
    <url>%2Fpost%2Fe567270d.html</url>
    <content type="text"><![CDATA[一、概述 提出一种较为通用的，去除繁杂假设的，高效的算法层次强化学习算法框架； 框架 $High-Level \space\space\space\space\space {———&gt;}^{auto} \space\space\space\space Goal ————&gt; ^{supervised} \space\space\space\space Low-Level controller$ 使用Off-Policy进行High-level及Low-level训练，通用化的设计，使得较好的在low-level controller中使用DDPG，TD3等确定性、off-policy的算法，兼容性强； 提出针对于High-level的Off-Policy Correction。 二、 方法HIRO(HIerarchical Reinforcement learning with Off-policy correction) [Low-level $\mu^{lo}$] 使用 Parameterized reward function 来表达特定的底层策略(潜在的无穷集合)； 通过训练相关策略，使其能够将obs $S_t$ 与期望Goal $g_t$相匹配。 [High-level $\mu^{hi}$] 为temporally-extended experience选择目标序列； 使用off-policy correction使得其可以使用过往经验，来适应新的底层控制器策略。 [综合] 高层控制器构建较粗的抽象的目标给底层； 这些目标直接对应于底层控制器想要尝试去达到的states； 每一个timestep, 高层控制器使用由环境交互产生的state，并通过 ​ ①每隔一段时间从高层控制器网络$\mu^{hi}$中进行采样($g_t\sim\mu^{hi}$)，或者 ​ ②使用一个固定的目标转换函数$g_t=h(s_{t-1},g_{t-1},s_t)$ 等方式来生成high-level action(or goal) $g_t\in\mathbb{R}^{d_s}$； 对于上述产生的目标成为临时目标，每固定c步将进行重新采样； 底层控制器通过当前的状态及目标进行决策到仿真环境的动作$a_t \sim \mu^{lo}(s_t,g_t)$； 高层控制器使用一个固定的Parameterized reward function $r$ 向底层控制器提供一个内在的回报函数(intrinsic reward) $r_t=r(s_t,g_t,a_t,s_{t+_1})$； 底层策略将存储经验$(s_t,g_t,a_t,r_t,s_{t+1},h(s_t,g_t,s_{t+1}))$用于off-policy training； 高层控制器每隔c步获取环境回报$R_t$，存储高层状态转移序列$(s_{t:t+c-1},g_{t:t+c-1},a_{t:t+c-1},R_{t:t+c-1},s_{t+c})$用于off-policy training。 三、具体策略 Parameterized reward function (1) 如何理解目标$g_t$ ​ 高层控制器产生的Goal $g_t$ 意味着对于当前的观测状态的期望相对变化，即$g_t$代表着从当前$s_t$开始，底层控制器应该如何决策才能使得c步之后的$s_{t+c}$与$s_t+g_t$能够足够的接近。 (2) 如何设计一个比较泛化的目标空间 ​ 为了在不考虑状态变化的情况下保持目标的相同绝对位置，目标转换模型$h$定义为: h(s_t,g_t,s_{t+1})=s_t+g_t-s_{t+1}(3) 如何设计高层到底层的内在回报函数Parameterized reward function ​ 该回报函数通过定义当前状态与目标状态之间的欧式距离来表示: r(s_t,g_t,a_t,s_{t+1})=-||s_t+g_t-s_{t+1}||_2(4) 底层控制器的策略算法如何选择 ​ 通过简单的将目标$g_t$加入观测元组的方式，使得底层框架可以使用强化学习的标准方法进行学习。 ​ 比如使用DDPG那么loss就加入目标： L_{DDPG}=[(r(s_t,g_t,a_t,s_{t+1})+\gamma Q_\theta^{lo}(s_{t+1},g_{t+1},\mu_φ^{lo}(s_{t+1},g_{t+1})-Q_{\theta}^{lo}(s_t,g_t,a_t)]^2(5) 与以往工作的不同 ​ Parameterized reward function 在之前的工作中也用过，但是他们用的时候目标都是间接通过表征学习给出的，而本文直接将目标当做一种预期的状态给到底层，更加直接。 高层训练使用的Off-policy Correction (1)以往的工作大多基于on-policy ，这里引入了off-policy进行学习会带来怎样的问题 ​ 引入off-policy的话，由于底层策略网络$\mu^{lo}$在当前和从replay buffer中采样的时候的策略网络是不同的，因而对于同一个目标，在前后两种策略网络下的状态转移序列是不同的。 (2) 如何解决以上问题 ​ 高层策略网络的状态转移序列为$(s_{t:t+c-1},g_{t:t+c-1},a_{t:t+c-1},R_{t:t+c-1},s_{t+c})$，我们将其转换为可以push到replay buffer中的state-action-reward 转移序列$(s_t,g_t\sum R_{t:t+c-1},s_{t+c})$，以兼容各种标准off-policy强化学习方法。 ​ 更改$g_t$使其能够适应新的$\mu^{lo}$: 先前的目标$g_t$诱导底层产生$a_{t:t+c−1} ∼ \mu^{lo}(s_{t:t+c−1}, g_{t:t+c−1})$行为。 调整上述目标变成$\tilde {g_t}$使其能够诱导新的底层策略产生最大化概率：$\mu^{lo}(a_{t:t+c−1}|s_{t:t+c−1}, \tilde{g}_{t:t+c−1})$ $\tilde{g}$使用固定的目标转移函数$h$来得到 (3) 如何获得新目标$\tilde{g}​$的最大化概率 ​ 对于一般的强化学习算法而言策略的log概率可以表示为: log \mu^{lo}(a_{t:t+c−1}|s_{t:t+c−1}, \tilde{g}_{t:t+c−1}) ∝ −\frac{1}{2}\sum^{ t+c−1}_{i=t} ||a_i − \mu^{lo}(s_i, \tilde{g}_i)||_2^2 + const.​ 这里文章通过离散采样的方式选择一个较大的值作为最大概率的估计，采样10个点为： * $g_t​$ * 按高斯分布在以$s_{t+c} − s_{t}​$为中心的分布上采样得到8个点 * $s_{t+c} − s_t$ ​]]></content>
      <tags>
        <tag>分层强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【确定性策略梯度类】 DPG,DDPG,TD3]]></title>
    <url>%2Fpost%2Fdad17569.html</url>
    <content type="text"><![CDATA[参考 【强化学习】确定性策略强化学习-DPG&amp;DDPG算法推导及分析 Deep Reinforcement Learning - 1. DDPG原理和算法 一、确定性策略梯度​ Deepmind的D.Silver等在2014年提出DPG： Deterministic Policy Gradient， 即确定性的行为策略，每一步的行为通过函数$μ$直接获得确定的值： a_t=μ(s_t|θ_μ)​ 这个函数$μ$即最优行为策略，不再是一个需要采样的随机策略。为何需要确定性的策略？简单来说，PG方法有以下缺陷： ​ 即使通过PG学习得到了随机策略之后，在每一步行为时，我们还需要对得到的最优策略概率分布进行采样，才能获得action的具体值；而action通常是高维的向量，比如25维、50维，在高维的action空间的频繁采样，无疑是很耗费计算能力的。在PG的学习过程中，每一步计算policy gradient都需要在整个action space进行积分: \nabla_θ=∫_S∫_Aρ(s)π_θ(a|s)Q_π(s,a)dads​ 这个积分我们一般通过Monte Carlo 采样来进行估算，需要在高维的action空间进行采样，耗费计算能力。如果采取简单的Greedy策略，即每一步求解$ argmax_a Q(s,a)$也不可行，因为在连续的、高维度的action空间，如果每一步都求全局最优解，太耗费计算性能。​ 将DPG算法融合进actor-critic框架，结合Q-learning或者Gradient Q-learning这些传统的Q函数学习方法，经过训练得到一个确定性的最优行为策略函数。 二、DPGDPG算法本身采用的是PG方法,且是Off-Policy方法(也可以是On-Policy)，因而直接对轨迹的价值回报进行求导。如下式求导，其中$\mu_\theta (s)$为生成确定性行动的策略函数。 \nabla_\theta v_{\mu_{\theta}} (s)=\nabla_\theta[q_\mu(s,\mu_\theta(s))]根据链式法则，由于$\mu_\theta(s)$与确定性策略价值函数$q_\mu​$有关,因而： \nabla_\theta v_\mu(s) = \nabla_{\mu_\theta(s)}[q_\mu(s, \mu_\theta(s))]\nabla_\theta\mu_\theta(s)由于是确定性策略，在价值函数$q(s,μ_\theta(s))$中有策略参数$\theta$，因此需要将价值函数对策略求导。 相较于随机策略梯度算法而言,如下是随机性策略梯度的目标函数梯度: ∇_θJ(θ)=\mathbb{E}_{τ∼π_θ}[(\sum_{t=0}^T∇_θ\logπ_θ(a_{i,t}∣s_{i,t}))(\sum_{t=0}^Tr(s_{i,t},a_{i,t}))]在DPG这个梯度公式中，没有了与动作有关的期望项，因此相对于随机性策略，确定性策略需要的学习数据少，算法效率高，尤其对于动作空间维数很大的情况。 三、策略模型参数的一致性​ 在DPG中为了更好地使用Off-policy,并使用TD降低方差，定义函数 $Q^\omega (s,a):S×A→R$ 用来拟合真实的状态动作值函数$\hat{Q}^\pi(s,a)$，如果$Q^\omega$收敛，那么L2范数梯度将满足如下公式: \begin{aligned} \nabla_\omega Loss &∝ \nabla_\omega\mathbb{E}_{s,a}[\hat{Q}^\pi(s,a)-Q^\omega(s,a)]^2\\&= \mathbb{E}_{s,a\sim\tau}[(\hat{Q}^\pi(s,a)-Q^\omega(s,a))-\nabla _\omega Q^\omega(s,a)]\\&=0 \end{aligned} \nabla_\omega Q^\omega(s,a)=\nabla_\theta\log\pi_\theta(s,a)=\nabla_\theta\pi_\theta(s,a)\frac{1}{\pi_\theta(s,a)}​ 在计算梯度时可以使用$Q^ω:S×A→R $代替真实的动作状态值函数$Q(s,a) $。并且神经网络满足这个性质，因此可以使用神经网络拟合动作状态值函数。这样价值模型不需要遵循某个具体的策略，因此可以使用off-policy的方式进行学习更新。 四、DPG目标函数 on-policy的确定性策略梯度算法 off-policy的确定性策略梯度算法(Replay buffer中的数据是通过$\beta$采样得到的) ​ 上两式的区别就是使用了不同的数据采样分布。可以看到off-policy缺少了重要性采样，这是由于确定性策略的动作是固定值，不是一个分布；其次是因为确定性策略值函数的评估采用的是Q-learning的方法，即使用TD(0)估计动作值函数并忽略重要性权重，值函数不依赖于任何策略，并贪心获取下一个动作。 ​ 这个$β$不是我们想要得到的最优策略，仅仅在训练过程中，生成下达给环境的action， 从而获得我们想要的数据集，比如状态转换(transitions)、或者agent的行走路径等，然后利用这个数据集去 训练策略$μ$，以获得最优策略。在test 和 evaluation时，使用$μ$，不会再使用$β$。 [Actor]衡量一个策略网络的表现(策略网络目标函数)，最大化策略目标: (根据上面推导的DPG) \begin{aligned} J_\beta (\theta)&=\int_S ρ^\beta(s)Q^\omega(s, \mu(s))ds\\ &=\mathbb{E}_{s\simρ^\beta}[Q^\omega(s, \mu(s))] \end{aligned}​ $s$是环境的状态，这些状态(或者说agent在环境中走过的状态路径)是基于agent的behavior策略产生的，它们的分布函数(pdf) 为$ρ^β$；​ $Q^\omega(s,μ(s))$ 是在每个状态下，如果都按照$μ$策略选择acton时，能够产生的$Q$值。 也即，$J_β(μ)$是在$s$根据$ρ^β$分布时，$Q^\omega(s,μ(s))​$ 的期望值。 [Critic]最小化值网络目标: \hat{Q}^\omega(s_t, a_t)=\mathbb{E}[r(s_t,a_t)+\gamma Q^\omega(s_{t+1},a_{t+1})] J_\beta(\omega)=\mathbb{E}_{s\simρ^\beta}[\frac{1}{2}(r_t+\gamma Q^\omega(s_{t+1},a_{t+1})-Q^\omega(s_t,a_t))^2] 最终DPG的目标函数为:（$\omega$是值网络(SGD优化)，$\theta$是策略网络(SGA优化)） ​ 参数更新为： 五、DDPGDDPG借用了target net还有滑动平均方法更新behavior网络参数 其更新方法如下： \begin{align} &\delta_t=r_t+\gamma Q^{\omega'}(s_{t+1},\mu_{\theta'}(s_{t+1}))-Q^\omega(s_t,a_t)\\ &\omega_{t+1} = \omega_t+\alpha_\omega \delta_t \nabla_\omega Q^\omega(s_t,a_t) \\ &\theta_{t+1} = \theta_t+\alpha_\theta \nabla_\theta\mu_\theta(s_t)\nabla_a Q^\omega(s_t,a_t)|_{a=\mu_\theta(s_t)} \\ &\theta'=\tau\theta+(1-\tau)\theta' \\ &\omega ' = \tau\omega+(1-\tau)\omega' \end{align} 对比一下DQN 六、TD3 参考：TD3 TD3(Twin Delayed Deep Deterministic policy gradient algorithm )主要对DDPG做了一些改进: 引入DoubleDQN的思想，来消除过拟合问题(两个Critic, 一个Actor) DoubleDQN中使用Current Net（Behavior net）来代替TargetNet，以减小Bias。 y_j = r_{j+1}+\gamma Q(s_{j+1},argmax_{a'}Q(s_{j+1},a';\theta);\theta^-)映射到DPG过程中,其中$\pi_φ(s’)$是CurrentNet: y = r + \gamma Q_\theta(s',\pi_φ(s'))即critic用更新较慢的target network，actor还是更新快的；但由于本身actor更新也不快，所以没啥效果。 如果类比double Q-learning，使用两个actor、两个critic写出来的更新目标为 本着“宁可低估，也不要高估”的想法（因为actor会选择高的，因此高估的会累积起来），再把目标改写成 最后发现两个actor也没啥用，就用一个actor，这个actor根据 $Q_{\theta_1}​$ 来更新。两个critic的更新目标都是一样的，即 $y_2 = y_1​$ 。这样的算法相比于改变之前的就等于多了一个和原来critic同步更新的辅助critic $Q_{\theta_2} ​$，在更新target的时候用来取min。 使用TargetNet 实验结果表明，当policy固定不变的时候，是否使用target network其价值函数都能最后收敛到正确的值；但是actor和critic同步训练的时候，不用target network可能使得训练不稳定或者发散。因此算法的中critic的更新目标都由target network计算出来 并且，价值函数估计准确之后再来更新policy会更好，因此采用了delayed policy update，即以较高的频率更新价值函数，以较低的频率更新policy。 使用Target Policy 平滑正则 希望学到的价值函数在action的维度上更平滑，因此价值函数的更新目标每次都在action上加一个小扰动]]></content>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聚类方法]]></title>
    <url>%2Fpost%2F2f107dda.html</url>
    <content type="text"><![CDATA[距离衡量指标 参考：常用样本相似性和距离度量方法 聚类效果衡量指标 参考: 聚类模型评估 常用评估: 类别信息已知 调和兰德系数 (ARI) 调和互信息 (AMI) 调和平均 (V-Measure) 类别信息未知 轮廓系数 (Silhouette Coefficient) 基于划分的聚类 KMeans 参考:机器学习算法系列（11）：聚类（2）—Kmeans K-mediods聚类，将Kmeans的平均值换成中值，避免噪声的干扰 KMeans++: 优化KMeans的聚类中心初始化，选择距离当前聚类中心的距离概率最大可能的点作为下一个聚类中心。参考:K-Means++算法 层次聚类参考 自上而下的分裂层次聚类(DIANA) 自下而上的凝聚层次聚类(AGNES) 密度聚类 DBSCAN : 参考 机器学习算法系列（11）：聚类（3）—DBSCAN 密度最大值聚类: 参考 机器学习算法系列（11）：聚类（4）—密度最大值聚类 AP ：参考 affinity propagation 近邻传播算法 谱聚类 谱聚类基本原理: 参考 谱聚类算法(Spectral Clustering) 谱聚类与PCA的异同: 参考 特征值与特征向量，PCA和谱聚类 一般步骤： 1）输入：相似度矩阵S（Rn∗n）、目标聚类数目k （在此之前需要完成两项工作： 1.选择合适的相似度函数，2.选择合适的聚类数目k）2）构造出相似图及其赋权的邻接矩阵（weighted adjacency matrix） （这一步需要选择：相似图的类型以及相应的参数）3）计算出相似图的Laplacian矩阵 （这一步需要选择：Laplacian矩阵的类型）4）计算Laplacian矩阵的前k个特征值对应的特征向量，以这k个特征向量为列，拼出新的矩阵Un∗k）5）视矩阵U的每一行为Rk中的一个点，对这n个点y1，y2，…yn进行k−means聚类，得到k个聚类C1，C2，…Ck6）输出聚类结果A1,A2,…Ak：yi被分到Cj中的哪一类，xi就被分到相应的Aj类]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[动态规划]]></title>
    <url>%2Fpost%2Fa80d0031.html</url>
    <content type="text"><![CDATA[[Leetcode 198] 打家劫舍 解法：一个经典的dp题，从选与不选的角度进行考虑， OPT(i)=max\{(OPT(i-2)+arr[i]), OPT(i-1)\}终止条件是第0个的时候只有一个可以选，有两个的时候，选二者中大的那个。 参考：动态规划（第2讲 第一个demo） 代码： 123456789101112131415161718class Solution &#123;public: int rob(vector&lt;int&gt;&amp; nums) &#123; if(nums.empty()) return 0; if(nums.size()==1) return nums[0]; vector&lt;int &gt; dp(nums.size(), 0); //构造dp数组 // dp终止条件 dp[0] = nums[0]; dp[1] = max(nums[1], nums[0]); // dp递推 for(int i=2 ; i&lt;nums.size(); i++)&#123; dp[i] = max((dp[i-2] + nums[i]), (dp[i-1])); &#125; return dp[nums.size()-1]; &#125;&#125;; [Leetcode 213] 打家劫舍 II 解法：由于有环，那么给他拆分成[0,n-1] 和 [1,n]两个部分进行分别dp，最后取两个区间中取值大的。 代码： 123456789101112131415161718192021222324252627282930class Solution &#123;public: int rob(vector&lt;int&gt;&amp; nums) &#123; if(nums.empty()) return 0; if(nums.size() == 1) return nums[0]; if(nums.size() == 2) return max(nums[0], nums[1]); vector&lt;int&gt; dp_n_1(nums.size(), 0); //[0,n-1] vector&lt;int&gt; dp_n(nums.size(), 0); //[1, n] dp_n[0] = 0; dp_n[1] = nums[1]; dp_n[2] = max(nums[1], nums[2]); dp_n_1[0] = nums[0]; dp_n_1[1] = max(nums[0], nums[1]); dp_n_1[2] = max(dp_n_1[1], nums[2]); for(int i = 2; i&lt;nums.size()-1; i++)&#123; dp_n_1[i] = max((dp_n_1[i-2] + nums[i]),(dp_n_1[i-1])); &#125; for(int i = 3; i&lt;nums.size(); i++)&#123; dp_n[i] = max((dp_n[i-2] + nums[i]),(dp_n[i-1])); &#125; return max(dp_n_1[nums.size()-2], dp_n[nums.size()-1]); &#125;&#125;; [Leetcode 139] 单词拆分 解法(4种) 解法一: DFS 解法二: 记忆化DFS 解法三: bottom up DP [子问题定义] : DP子问题是从0开始到当前位置的子串是否可分(dp[i] == true?)，当前位置总共有n个可能，所以子问题的个数是n个。 使用hashset转储dict 构建dp数组，默认初始空串是可分的即dp[0]=1, 遍历初始串，验证从前面可分的子串尾部到当前位置的字符串([j为dp[j]==1, i])是否在字典中，如果在字典中则记录当前位置可分 解法四: Bottom up DP + max trick 代码: 123456789101112131415161718192021222324class Solution &#123;public: bool wordBreak(string s, vector&lt;string&gt;&amp; wordDict) &#123; unordered_set&lt;string&gt; wordSet(wordDict.cbegin(), wordDict.cend()); vector&lt;int &gt; dp(s.size()+1, 0); //加1是给空串留位置 dp[0] = 1;// 空串可分 //解n个子问题 for(int i = 1; i &lt; s.size()+1; i++)&#123; for(int j = 0; j &lt; i; j++)&#123; // j之前可分，判断j~i这段是不是可分的，可分的话，i的位置记为1 if(dp[j])&#123; string tmp = s.substr(j, i-j); if(wordSet.find(tmp) != wordSet.end())&#123; dp[i] = 1; break; &#125; &#125; &#125; &#125; return dp[s.size()]; &#125;&#125;; [Leetcode 124] 二叉树中的最大路径和 解法: 找二叉树中的最大路径和，首先要考虑清楚是从上往下找，还是从下往上找，通过观察树的结构， 我们发现从下到上最好找。 1、最优子结构 因为树是由一个个更小的结点树组成，所以我们可以把问题分解成一个个更小的树。 当树的结点只有一个时，最大的路径就是他自身，让树的高度为2时，根节点的最大路径为左右结点中的最大值加上根节点本身的值：max(l, r) + root.val， 如果左右结点都为负数，还没有自身的值大呢，所以我们取其中的最大值。maxSubSubTree = max(max(l, r) + root.val, root.val) 知道了二叉树的最优左右路径，我们需要比较整体路径，maxSubTree = max(maxSubSubTree, l+r+root.val)。 再将以该结点为根节点的二叉树的最大路径和，和全局的路径和比较，取两者最大值，res = max(res, maxSubTree)2、重叠子问题 从下往上走，当底层的最优路径找出来了， 上一层结点就能直接用下一层的结果,依次向上递推，求解过程都简化成了对若干个个高度为2 的二叉树的操作。当递归完成时，根节点的值就是整颗二叉树的最大路径和。 代码: 123456789101112131415161718192021222324252627282930313233343536/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: int maxPathSum(TreeNode* root) &#123; int res=INT_MIN; getMaxPath(root, res); return res; &#125; int getMaxPath(TreeNode* root, int &amp;res)&#123; if(!root) return 0; int left = getMaxPath(root-&gt;left, res); int right = getMaxPath(root-&gt;right, res); //下一级到上一级的最优解 //下一级的最优解和自身相比，如果下一级没有，或者都为负数，则为自身 int maxSubSubTree = max(max(left, right) + root-&gt;val, root-&gt;val); //表示所考虑是以该结点为根的路径和 int maxSubTree = max(maxSubSubTree, root-&gt;val + left+ right); //更新整个树中的最大路径和 res = max(maxSubTree, res); return maxSubSubTree; //返回单个子子树最优解 &#125; &#125;; [Leetcode 128] 最长连续序列 解法：(参考花花酱) 使用hashmap来构建，将数组中的值作为键，连续元素数目作为边界值，并存储在hashmap的连续区域边界节点中。 节点更新有三种情况: 当前元素在hashmap中没有左右邻居，那么该节点的hashmap值为1 ; 当前元素在hashmap中有左或右邻居(只存在一边)，那么该节点的hashmap值为其对应左右边界的连续值加1 当前元素能够桥接两个连续区域，那么该节点的hashmap值可以不关心，但是其左侧的左边界值与右侧部分的右边界值对应加一。 代码： 1234567891011121314151617181920212223242526272829303132class Solution &#123;public: int longestConsecutive(vector&lt;int&gt;&amp; nums) &#123; unordered_map&lt;int, int&gt; hashMap; int res = 0; for(auto item: nums)&#123; if(hashMap.count(item)) continue; auto it_left = hashMap.find(item-1); auto it_right = hashMap.find(item+1); //获取当前元素左右的边界值，左右无邻居则对应的l,r 为0 int l = it_left!=hashMap.end() ? it_left-&gt;second : 0; int r = it_right!=hashMap.end() ? it_right-&gt;second : 0; //[case3]当前元素能够桥接两个连续区域，那么该节点的hashmap值可以不关心(程序中得填充表示，默认就是跟左右边界相等就行)，但是其左侧的左边界值与右侧部分的右边界值对应加一。 if(l!=0 &amp;&amp; r!=0)&#123; hashMap[item] = hashMap[item-l] = hashMap[item+r] = r + l + 1; &#125;else if(l!=0 &amp;&amp; r == 0)&#123; //[case2] 当前元素在hashmap中有左或右邻居(只存在一边)，那么该节点的hashmap值为其对应左右边界的连续值加1 hashMap[item] = hashMap[item-l] = l + 1; &#125;else if(l==0 &amp;&amp; r!=0)&#123; //[case2] 当前元素在hashmap中有左或右邻居(只存在一边)，那么该节点的hashmap值为其对应左右边界的连续值加1 hashMap[item] = hashMap[item+r] = r + 1; &#125;else&#123; //[case1] 当前元素在hashmap中没有左右邻居，那么该节点的hashmap值为1 &lt;currNodeVal, 1&gt;; hashMap[item] = 1; &#125; &#125; // 遍历选择边界值最大的 for(const auto&amp; item: hashMap)&#123; res = max(res, item.second); &#125; return res; &#125;&#125;;]]></content>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A3C、PPO、GAE笔记]]></title>
    <url>%2Fpost%2F3cc694a0.html</url>
    <content type="text"><![CDATA[一、 重要性采样TRPO和PPO主要思想的数学基础是重要性采样 重要性采样：$x_i $是从$p(x)$分布中采样得到的， 但是$p(x)$的值往往无法直接获得，需要通过其他分布$q(x)$进行间接采样获得。 \begin{aligned} \mathbb{E}_{x\sim p}[f(x)] &=\int f(x)p(x) dx \\ &=\int f(x) \frac{p(x)}{q(x)}q(x)dx \\ &=\mathbb{E}_{x\sim q}[f(x)\frac{p(x)}{q(x)}] \end{aligned} 条件： $p$分布与$q$分布需要相近，才能得到较好的效果。 用在强化学习里面: 由于策略梯度原始公式中的 新策略分布难以得到，因而使用旧策略进行间接采样，以使得未知项变成可估计的已知项进行计算。 二、 梯度与参数更新1. 回报的期望：最大化全部采样轨迹上的策略回报值，$R(\tau)$ 表示某一个轨迹$\tau$的回报值 argmax_\theta \space \mathbb{E}[{R_\theta}]=\sum_\tau R(\tau)p_\theta(\tau)2. 回报的期望的梯度：(第三个等号用到的公式：$\nabla f(x) = f(x) \nabla \log f(x)​$) \begin{aligned} \nabla \mathbb{E}[{R_\theta}]&=\sum_\tau R(\tau) \nabla p_{\theta}(\tau) \\ &= \sum_\tau R(\tau)p_\theta(\tau)\frac{\nabla p_\theta(\tau)}{p_\theta(\tau)} \\ &= \sum_\tau R(\tau)p_\theta(\tau){\nabla \log p_\theta(\tau)}\\ &= \mathbb{E}_{\tau \sim p_\theta{(\tau)}}[R(\tau){\nabla \log p_\theta(\tau)}] \\ &≈ \frac{1}{N} \sum_{n=1}^{N}R(\tau^n)\nabla \log p_{\theta}(\tau^n) \\ &=\frac{1}{N}\sum_{n=1}^{N} \sum_{t=1}^{T_n} R(\tau^n)\nabla \log p_\theta(a_t^n|s_t^n) \end{aligned}式中 $N​$表示采样了$N​$条trajectory, $T_n​$表示每条trajectory的step数量。 关于$p_{\theta}(\tau)$ \begin{aligned} p_{\theta}(\tau) &= p(s_1)p_\theta(a_1|s_1)p(s_2|s_1,a_1)p_\theta(a_2|s_2)p(s_3|s_2,a_2) \space\space...\space\space \\ &=p(s_1) \prod_{t=1}^T p_{\theta}(a_t|s_t)p(s_{t+1}|s_t, a_t) \end{aligned}由两部分组成一部分是来自环境的 $p_\theta(s_{t+1}|s_t, a)$， 一部分是来自agent的 $p_\theta {(a_t|s_t)}$, 其中来自环境的部分不带入计算，策略更新只考虑agent这部分。所以最后一步并没有$t+1$这部分。 3. 参数更新： \theta = \theta+\eta \nabla \bar{R_\theta} 三、 实际算法中对策略梯度的处理方法 策略梯度方法： 加入baseline \nabla \bar{R_\theta}=\frac{1}{N} \sum_{n=1}^{N}(R(\tau^n)-b)\nabla \log p_{\theta}(\tau^n) \\ b≈\mathbb{E}[R(\tau)] $b$ 的加入保证reward不是恒大于0的，若reward一直大于0，则会导致未被采样的action无法得到提升，但其实该action并不是不好而是未被采样。 状态值函数估计轨迹回报： $R(\tau^n)-b$ 部分使用状态值函数来替代 q(s,a) 优势函数估计轨迹回报： $R(\tau^n)-b​$ 部分用以下Advantage function来替代 A(s_t,a_t)= q(s,a)-V(s) TD-Error估计轨迹回报：(A3C)使用值网络估计值，引入bias减小variance $R(\tau^n)-b​$ 部分用以下TD-Error 代替 r(s_t. a_t)+v(s_{t+1})-v(s)​ 四、GAE(Generalized Advantage Estimation) GAE的作用 GAE的意思是泛化优势估计，因而他是用来优化Advantage Function优势函数的。 GAE的存在是用来权衡variance和bias问题的： On-policy直接交互并用每一时刻的回报作为长期回报的估计$\sum_{t’=t}^{T} \gamma^{t’-t}r_{t’}$ 会产生较大的方差，Variance较大。 而通过基于优势函数的AC方法来进行回报值估计，则会产生方差较小，而Bias较大的问题。 GAE 推导 满足$\gamma$-just条件。(未完待续) GAE形式 GAE的形式为多个价值估计的加权平均数。 TD Error=\delta_t=r_t+\gamma v(s_{t+1})-v(s_t)运用GAE公式进行优势函数的估计： \sum_{l=0}^\infin(\gamma\lambda)^l \delta_{t+1}^V​ 为了快速估计序列中所有时刻的估计值，采用倒序计算，从t+1时刻估计t时刻： \hat{A_t}^{GAE(\gamma,\lambda)}=\sum_{l=0}^{\infin}(\gamma\lambda)^l \delta_{t+1}^V=\delta_t^V+\gamma\lambda\hat{A}_{t+1}^{GAE(\gamma,\lambda)} 五、PPO关于策略梯度的目标函数以上所述的策略梯度算法属于on-policy的算法，而PPO属于off-policy的算法 on-policy: 使用当前策略$\pi_\theta$收集数据，当参数$\theta$更新后，必须重新采样。 \nabla \bar{R_\theta}=\mathbb{E}_{\tau \sim p_\theta{\tau}}[R(\tau){\nabla \log p_\theta(\tau)}] off-policy: 可以从可重用的样本数据中获取样本来训练当前的策略$\pi _\theta​$，下式用了重要性采样。 \nabla \bar{R_\theta}=\mathbb{E}_{\tau \sim p_{\theta^\prime}{\tau}}[\frac{p_\theta(\tau)}{p_{\theta^\prime}(\tau)} R(\tau){\nabla \log p_\theta(\tau)}] 1. PPO目标函数 对于PPO而言，轨迹回报通过采用Advantage function的方式进行估计，因而其梯度更新方式为： \begin{aligned} \nabla \bar{R_\theta} &=\mathbb{E}_{(s_t,a_t)\sim\pi_\theta}[A^\theta(s_t,a_t)\nabla \log p_\theta({a_t^n|s_t^n})] \\ &=\mathbb{E}_{(s_t,a_t)\sim\pi_\theta^\prime}[\frac{p_\theta(s_t,a_t)}{p_\theta^\prime(s_t,a_t)}A^{\theta^\prime}(s_t,a_t)\nabla \log p_\theta({a_t^n|s_t^n})] \\ &=\mathbb{E}_{(s_t,a_t)\sim\pi_\theta^\prime}[\frac{p_\theta(a_t|s_t)}{p_\theta^\prime(a_t|s_t)}\frac{p_\theta(s_t)}{p_\theta^\prime(s_t)}A^{\theta^\prime}(s_t,a_t)\nabla \log p_\theta({a_t^n|s_t^n})] \\ &≈\mathbb{E}_{(s_t,a_t) \sim \pi_\theta^\prime}[\frac{\nabla p_\theta(a_t|s_t)}{p_\theta^\prime(a_t|s_t)}A^{\theta^\prime}(s_t,a_t)] \end{aligned}​ 其中，从第二个等式用的是重要性采样，第三到第四个约等式由于$\frac{p_\theta(s_t)}{p_\theta^\prime(s_t)}​$这一项来源于重要性采样，前提假设两个分布差别不大，近似为1，且不易计算，故省略，后面的$\nabla \log p_\theta({a_t^n|s_t^n})​$ ,根据公式$\nabla f(x) = f(x) \nabla \log f(x)​$转换。 ​ 因而，定义目标函数为： J^{\theta^{\prime}} (\theta)=\mathbb{E}_{(s_t,a_t) \sim \pi_\theta^\prime}[\frac{p_\theta(a_t|s_t)}{p_\theta^\prime(a_t|s_t)}A^{\theta^\prime}(s_t,a_t)]2. PPO对于重要性采样约束的处理 ​ 为了保证$p_\theta(s_t,a_t) ​$ 与 $p_\theta^\prime(s_t,a_t)​$ 分布的差别不会太大，采用以下约束： TRPO： 使用约束 $KL(\theta,\theta’)&lt;\delta$，在分布上进行约束。 PPO1(Adaptive KL)：使用$J_{PPO}^{\theta’}(\theta)=J^{\theta’}(\theta)-\beta KL(\theta,\theta’)$，在目标函数上加一个正则项进行约束，注意，这里KL散度衡量的是action之间的距离，而不是参数$\theta$与$\theta’$之间的距离。 PPO2 (Clip，论文中推荐的)：使用$J_{PPO_2}^{\theta’}(\theta)=\sum_{(s_t,a_t)}\min\{([\frac{p_\theta(a_t|s_t)}{p_\theta^\prime(a_t|s_t)}A^{\theta^\prime}(s_t,a_t)], [clip(\frac{p_\theta(a_t|s_t)}{p_\theta^\prime(a_t|s_t)},1-\epsilon,1+\epsilon)A^{\theta^\prime}(s_t,a_t)])\}​$, 来约束分布距离。 使用GAE对优势函数进行优化 1234567891011121314151617def get_gaes(self, rewards, v_preds, v_preds_next): """ GAE :param rewards: r(t) :param v_preds: v(st) :param v_preds_next: v(st+1) :return: """ deltas = [r_t + self.gamma * v_next - v for r_t, v_next, v in zip(rewards, v_preds_next, v_preds)] #计算GAE(lambda = 1), 参见 ppo paper eq(11) gaes = copy.deepcopy(deltas) # 倒序计算GAE for t in reversed(range(len(gaes) - 1)): gaes[t] = gaes[t] + self.gamma * gaes[t + 1] return gaes 六、 PPO的目标函数PPO的最终目标函数由三部分组成，可使用梯度下降求解，而不是像TRPO一样使用共轭梯度法： 策略梯度目标函数： $L_t^{CLIP}(\theta)​$ 值函数目标函数：$L_t^{VF}(\theta)=(V_\theta(s_t)-V_t^{target})^2=((r+\gamma v(s_{t+1}))-v(s_t))^2$ 策略模型的熵: $S_{[\pi_\theta]}(s_t)=-\pi_\theta(a|s)\log\pi_\theta(a|s)​$ 完整的形式如下： L_t^{PPO_2}(\theta)=\hat{\mathbb{E}}_t[L_t^{CLIP}(\theta)-c_1L_t^{VF}(\theta)+c_2S_{[\pi_\theta]}(s_t)]这部分相应的代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263with tf.variable_scope('assign_op'): self.assign_ops = [] for v_old, v in zip(old_pi_trainable, pi_trainable): self.assign_ops.append(tf.assign(v_old, v))# inputs for train_opwith tf.variable_scope('train_inp'): self.actions = tf.placeholder(dtype=tf.int32, shape=[None], name='actions') self.rewards = tf.placeholder(dtype=tf.float32, shape=[None], name='rewards') self.v_preds_next = tf.placeholder(dtype=tf.float32, shape=[None], name='v_preds_next') self.gaes = tf.placeholder(dtype=tf.float32, shape=[None], name='gaes')act_probs = self.Policy.act_probsact_probs_old = self.Old_Policy.act_probs# agent通过新策略选择action的概率 probabilities of actions which agent took with policyact_probs = act_probs * tf.one_hot(indices=self.actions, depth=act_probs.shape[1])act_probs = tf.reduce_sum(act_probs, axis=1)# agent通过旧策略选择action的概率 probabilities of actions which agent took with old policyact_probs_old = act_probs_old * tf.one_hot(indices=self.actions, depth=act_probs_old.shape[1])act_probs_old = tf.reduce_sum(act_probs_old, axis=1)with tf.variable_scope('PPO_loss'): """ 策略目标函数 """ # # ratios = tf.divide(act_probs, act_probs_old) # r_t(θ) = π/πold 为了防止除数为0，这里截取一下值，然后使用e(log减法)来代替直接除法 ratios = tf.exp( tf.log(tf.clip_by_value(act_probs, 1e-10, 1.0)) - tf.log(tf.clip_by_value(act_probs_old, 1e-10, 1.0))) # L_CLIP 裁剪优势函数值 clipped_ratios = tf.clip_by_value(ratios, clip_value_min=1 - clip_value, clip_value_max=1 + clip_value) self.loss_clip = tf.minimum(tf.multiply(self.gaes, ratios), tf.multiply(self.gaes, clipped_ratios)) self.loss_clip = tf.reduce_mean(self.loss_clip) """ 策略模型的熵 """ # 计算新策略πθ的熵 S = -p log(p) 这里裁剪防止p=0 self.entropy = -tf.reduce_sum( self.Policy.act_probs * tf.log(tf.clip_by_value(self.Policy.act_probs, 1e-10, 1.0)), axis=1) self.entropy = tf.reduce_mean(self.entropy, axis=0) # mean of entropy of pi(obs) """ 值目标函数 """ # L_vf = [(r+γV(π(st+1))) - (V(π(st)))]^2 v_preds = self.Policy.v_preds self.loss_vf = tf.squared_difference(self.rewards + self.gamma * self.v_preds_next, v_preds) self.loss_vf = tf.reduce_mean(self.loss_vf) # construct computation graph for loss # L(θ) = E_hat[L_CLIP(θ) - c1 L_VF(θ) + c2 S[πθ](s)] # L = 策略目标函数 + 值目标函数 + 策略模型的熵 self.loss = self.loss_clip - c_1 * self.loss_vf + c_2 * self.entropy # minimize -loss == maximize loss self.loss = -self.lossoptimizer = tf.train.RMSPropOptimizer(learning_rate=args.ppo_lr, epsilon=1e-5)self.gradients = optimizer.compute_gradients(self.loss, var_list=pi_trainable)self.train_op = optimizer.minimize(self.loss, var_list=pi_trainable) 七、Actor-CriticA2C、A3C等方法采用的是TD方法来替代R-b部分 A3C 方法: 启动N个线程，Agent在N线程中同时进行环境交互收集样本； 收集完样本后，每一个线程将独立完成训练并得到参数更新量，异步更新到全局的模型参数中； 下一次训练的时候，线程的模型参数将与全局参数完成同步，使用新的参数进行下一次训练。 目标函数: 使用TD-$\lambda$减小TD带来的偏差，可以在训练早期更快的提升价值模型。为了增加模型的探索性，目标函数中引入了策略的熵。 \nabla_\theta J(\theta)=\frac{1}{T}\sum_t^T\nabla_\theta \log \pi(a_t|s_t;\theta)(\sum_{i=0}^n\gamma^{i-1}r_{t+1}+v(s_{t+n})-v(s_t))+\beta\nabla_\theta H(s_t;\theta) A2C 与A3C不同的是参数更新全部在全局master完成，每个子线程只负责env.step()进行探索。]]></content>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生成对抗网络(笔记)]]></title>
    <url>%2Fpost%2Fd42ca19b.html</url>
    <content type="text"><![CDATA[参考 令人拍案叫绝的Wasserstein GAN WGAN最新进展：从weight clipping到gradient penalty Ten paper: GAN-GP(Gradient Penalty) 深度有趣 | 16 令人拍案叫绝的WGAN 开发者自述：我是这样学习 GAN 的 推荐：生成对抗网络综述 一、 基本概念 符号定义 $D$ 判别模型， $G$ 生成模型 $x​$ 数据集中的数据分布，$z​$ 某种随机分布 目标函数（详细参见第四部分） min_Gmax_DV(G,D)=\mathbb{E}_x[log(D(x))]+\mathbb{E}_z[log(1-D(G(z)))] D Loss (MC采样，相当于训练二分类器$x \sim P_{data}$一类，$\hat{x} \sim G(z)$一类): max_DV(G,D)=\mathbb{E}_x[log(D(x))]+\mathbb{E}_z[log(1-D(G(z)))] G Loss原始 （MiniMax GAN[MMGAN]）(判别器越好，生成器梯度消失越严重) \mathbb{E}{_{x\sim P_g}}[log(1-D(x))] G Loss改进 (Non-saturating GAN[NSGAN] )(-log trick) \mathbb{E}{_{x\sim P_g}}[-log(D(x))] 释义 $G$的目标是最大化生成数据与数据集数据的似然，减小生成数据与数据集数据之间的差距。 $D$要解决的问题是一个二分类问题，$V(D,G)$ 为二分类问题中常见的交叉熵损失。对于生成器$ G $来说，为了尽可能欺骗$ D$，所以需要最大化生成样本的判别概率 $D(G(z))$，即最小化$ log(1-D(G(z)))$，注意：$log(D(x)) $一项与生成器$ G $无关，所以可以忽略。 二、 原理推导 参考: 开发者自述：我是这样学习 GAN 的 ​ 真实数据的分布$ P_{data}(x)$，$x$ 是一个真实数据，是一个向量，这个向量集合的分布就是 $P_{data}$。我们需要生成一些也在这个分布内的数据，如果直接就是这个分布的话，怕是做不到的。 ​ 现有的 Generator 生成的分布可以假设为 $P_G(x;θ)$，这是一个由 $ θ $ 控制的分布，$θ$ 是这个分布的参数（如果是高斯混合模型，那么 $ θ$ 就是每个高斯分布的均值和方差) 。 ​ 假设我们在真实分布中取出一些数据，${x_1, x_2, … , x_m}$，我们想要计算一个似然 $P_G(x_i; θ)$。 ​ 对于这些数据，在生成模型中的似然就是 L=\prod_{i=1}^mP_G(x^i;\theta) GAN原理 最大化上面这个似然，等价于让 Generator 生成那些真实数据分布的概率最大。这就变成了一个最大似然估计的问题了，我们需要找到一个 $θ^*$ 来最大化这个似然。 固定$G$,求一个最优的$D^*$ \begin{aligned} V&=\mathbb{E}_{x\sim P_{data}}[logD(x)]+\mathbb{E}_{x\sim P_G}[log(1-D(x))]\\ &=\int_x P_{data}(x)\log D(x)dx+\int_xP_G(x)\log (1-D(x))dx \\ &=\int_x[P_{data}(x)\log D(x)+P_G(x)\log(1-D(x))]dx \end{aligned}那么转为优化$f(D) = P_{data}(x)\log D(x)+P_G(x)\log(1-D(x))$ 对$f(D)$求偏导: \frac{df(D)}{dD} = P_{data}(x)×\frac{1}{D}+P_G(x)×\frac{1}{1-D}×(-1) = 0可以解得: D^*(x) = \frac{P_{data}(x)}{P_{data}(x)+P_G(x)}对于一个给定的 x，得到最优的 D 如上图，范围在 (0,1) 内，把最优的 D 带入V \begin{aligned} &max_DV(G,D)&=V(G,D^*)\\ &=\mathbb{E}_{x\sim P_{data}}[logD^*(x)]+\mathbb{E}_{x\sim P_G}[log(1-D^*(x))]\\ &=\mathbb{E}_{x\sim P_{data}}[log\frac{P_{data}(x)}{P_{data}(x)+P_G(x)}]+\mathbb{E}_{x\sim P_G}[log(1-\frac{P_{G}(x)}{P_{data}(x)+P_G(x)})] \\ &=\int_xP_{data}(x)log\frac{P_{data}(x)}{P_{data}(x)+P_G(x)}dx+\int_xP_G(x)log(1-\frac{P_{G}(x)}{P_{data}(x)+P_G(x)})dx\\ &=-2log2+\int_xP_{data}(x)log\frac{P_{data}(x)}{(P_{data}(x)+P_G(x))/2}dx+\int_xP_G(x)log(1-\frac{P_{G}(x)}{(P_{data}(x)+P_G(x))/2})dx \\ &=-2log2+KL(P_{data}(x)||\frac{P_{data}(x)+P_G(x)}{2})+KL(P_G(x)||\frac{P_{data}+P_G(x)}{2}) \\ &= -2log2+2JSD(P_{data}(x)||P_G(x)) \end{aligned}所以最终 \mathbb{E}_{x\sim P_r}[\log D^*(x)] + \mathbb{E}_{x\sim P_g}[\log(1-D^*(x))] = 2JS(P_r || P_g) - 2\log 2表示两个分布之间的差异，最小值是$ -2log2​$，最大值为 $0​$。观察上式，当 $P_G(x)=P_{data}(x)​$ 时，G 是最优的。 三、GAN的训练有了上面推导的基础之后，我们就可以开始训练 GAN 了。结合我们开头说的，两个网络交替训练，我们可以在起初有一个 $G_0$ 和 $D_0$，先训练 $D_0$ 找到 ： 然后固定 $D_0$ 开始训练 $G_0$， 训练的过程都可以使用 gradient descent，以此类推，训练 $D_1,G_1,D_2,G_2,…$ 但是这里有个问题就是，你可能在 $D_0^*​$ 的位置取到了： 然后更新 $G_0$ 为 $G_1$，可能会出现： 但是并不保证会出现一个新的点$ D_1^*$ 使得： 这样更新 $G$ 就没达到它原来应该要的效果，如下图所示： 避免上述情况的方法就是更新 $G $的时候，不要更新 $G $太多。 知道了网络的训练顺序，我们还需要设定两个 loss function，一个是$ D$ 的 loss，一个是 $G$ 的 loss。下面是整个 GAN 的训练具体步骤：(多练D，少练G以免JS散度太小) 上述步骤在机器学习和深度学习中也是非常常见，易于理解。 四、存在的问题 G的Loss function收敛速度问题： $G​$ 的 loss function $\mathbb{E}{_{x\sim P_g}}[log(1-D(x))]​$ 还是有一点小问题，下图是两个函数的图像： ​ [问题原因] $log(1-D(x)) $是我们计算时 G 的 loss function，但是我们发现，在$ D(x) $接近于 0 的时候，这个函数十分平滑，梯度非常的小。这就会导致，在训练的初期，$G $想要骗过 $D$，变化十分的缓慢，而上面的函数，趋势和下面的是一样的，都是递减的。但是它的优势是在 $D(x) $接近 0 的时候，梯度很大，有利于训练，在 $D(x) $越来越大之后，梯度减小，这也很符合实际，在初期应该训练速度更快，到后期速度减慢。 ​ [解决方案] 所以我们把$ G$ 的 loss function 修改为: minimizeV=-\frac{1}{m}\sum_{i=1}^m \log(D(x^i)) Loss 没有变化，一直都是平的 [问题] 此时$max_DV(G,D)=0$, $JS=log2$, $P_G$和$P_{data}$由于$D$过拟合导致完全没有交集，但是实际上两个分布是有交集的，造成这个的原因是因为，我们无法真正计算期望和积分，只能使用 sample 的方法，如果训练的过拟合了，D 还是能够完全把两部分的点分开: [问题原因] 对于这个问题，我们是否应该让 $D$变得弱一点，减弱它的分类能力，但是从理论上讲，为了让它能够有效的区分真假图片，我们又希望它能够，所以这里就产生了矛盾。 还有可能的原因是，虽然两个分布都是高维的，但是两个分布都十分的窄，可能交集相当小，这样也会导致 JS divergence 算出来为$log2$，约等于没有交集。 [解决方案] 解决的一些方法，有添加噪声，让两个分布变得更宽，可能可以增大它们的交集，这样 JS divergence 就可以计算，但是随着时间变化，噪声需要逐渐变小。 Mode Collapse 五、 WGAN 参考：令人拍案叫绝的Wasserstein GAN 1. 形式改变： 判别器最后一层去掉sigmoid 生成器和判别器的loss不取log 每次更新判别器的参数之后把它们的绝对值截断到不超过一个固定常数c 不要用基于动量的优化算法（包括momentum和Adam），推荐RMSProp，SGD也行 2. Wasserstein距离的好处： Wasserstein距离： [公式释义]： 在所有的$P_r$、$P_g$的联合分布集合中选取真实样本x与生成样本y之间的距离期望最小的分布(取下界)。 [优点]：Wasserstein距离相比KL散度、JS散度的优越性在于，即便两个分布没有重叠，Wasserstein距离仍然能够反映它们的远近。（没有条件性突变发生） 3. Wasserstein距离在WGAN中的使用 Wasserstein距离中的下界inf难以计算，论文中，将其转换为了 Lipschitz连续(导数有上界，限制了函数的大幅度震动) |f(x_1)-f(x_2)|≤K|x_1-x_2| \space\space\space\space\space\space\space\space (K≥0)所以对于上面变换以后的Wasserstein距离，要求函数$f$的Lipschitz常数$||f||_L$不超过$K$的条件下，对所有可能满足条件的$f$取到$\mathbb{E}_{x \sim P_r} [f(x)] - \mathbb{E}_{x \sim P_g} [f(x)]$的上界，然后再除以$K$。 Weight Clipping 对于$||f_w||_L \leq K$这个限制。我们其实不关心具体的$K$是多少，只要它不是正无穷就行，因为它只是会使得梯度变大$K$倍，并不会影响梯度的方向。所以作者采取了一个非常简单的做法，就是限制神经网络$f_\theta$的所有参数$w_i$的不超过某个范围$[-c, c]$，比如$w_i \in [- 0.01, 0.01]$，此时关于输入样本$x$的导数$\frac{\partial f_w}{\partial x}$也不会超过某个范围，所以一定存在某个不知道的常数$K$使得$f_w$的局部变动幅度不会超过它，Lipschitz连续条件得以满足。具体在算法实现中，只需要每次更新完$w​$后把它clip回这个范围就可以了。 原始GAN的判别器做的是真假二分类任务，所以最后一层是sigmoid，但是现在WGAN中的判别器$f_w$做的是近似拟合Wasserstein距离，属于回归任务，所以要把最后一层的sigmoid拿掉。 六、WGAN-GP(GAN的稳定性)WGAN梯度裁剪的问题 梯度裁剪会导致最优化困难。在梯度裁剪约束下，大多数神经网络架构只有在学习极其简单地函数时才能达到k地最大梯度范数。因此，通过梯度裁剪来实现k-Lipschitz约束将会导致critic偏向更简单的函数。如下图所示，在小型数据集上，权重剪枝不能捕捉到数据分布的高阶矩。 算法： 先随机采一对真假样本，还有一个0-1的随机数: x_r\sim P_r, x_g \sim P_g, \epsilon \sim Uniform[0,1]在$x_r$和$x_g​$的连线上进行随机线性插值采样: \\ \hat{x}=\epsilon x_r+(1-\epsilon)x_g最终的Loss: L=\mathbb{E}_{\hat{x}\sim\mathbb{P}_g}[D(\tilde{x})]-\mathbb{E}_{x\sim\mathbb{P}_r}[D(x)]+\lambda\mathbb{E}_{\hat{x}\sim\mathbb{P}_\hat{x}}[(||\nabla_{\hat{x}}D(\hat{x})||_2-1)^2] Gradient Penalty 项： $\lambda\mathbb{E}_{\hat{x}\sim\mathbb{P}_\hat{x}}[(||\nabla_{\hat{x}}D(\hat{x})||_2-1)^2]​$ Wasserstein 距离项: $\mathbb{E}_{\hat{x}\sim\mathbb{P}_g}[D(\tilde{x})]-\mathbb{E}_{x\sim\mathbb{P}_r}[D(x)]​$ 12345678910111213141516# [GP] 先随机采一对真假样本，还有一个0-1的随机数:epsilon = tf.random_uniform(shape=[batch_size, 1], minval=0., maxval=1.)# [GP] 在x_r和x_g的连线上进行随机线性插值采样:X_hat_State = self.expert_s + epsilon * (self.agent_s - self.expert_s)X_hat_Action = expert_a_one_hot + epsilon * (agent_a_one_hot - expert_a_one_hot)X_hat_s_a = tf.concat([X_hat_State, X_hat_Action], axis=1)...with tf.variable_scope('Discriminator_loss'): wasserstein = tf.reduce_mean(crit_A) - tf.reduce_mean(crit_e) # Wasserstein 距离 grad_D_X_hat = tf.gradients(X_hat_crit, [X_hat_s_a])[0] slopes = tf.sqrt(tf.reduce_sum(tf.square(grad_D_X_hat), reduction_indices=[1])) gradient_penalty = tf.reduce_mean((slopes - 1.) ** 2) #[GP] Gradient Penalty loss = wasserstein + LAMBDA * gradient_penalty tf.summary.scalar('discriminator', loss)]]></content>
      <tags>
        <tag>GANs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hybrid A* 路径规划]]></title>
    <url>%2Fpost%2Fc0e17f81.html</url>
    <content type="text"><![CDATA[参考: Hybrid A Star 路径规划 Hybrid A* Path Planner for the KTH Research Concept Vehicle 分层有限状态机和无人车行为规划 [PAPER] : Path Planning in Unstructured Environments Hybrid A* 的使用场景​ 在斯坦福大学2007年参加的DARPA无人车城市挑战赛时使用的Junior，其在行为规划层提出了分层有限状态机的方式，如下图所示。 ​ 其中，BAD_RNDF状态表示的是，当前道路与系统的路网图不同的时候，无人车将采用Hybrid A*来进行规划路径。 Hybrid A 与 AHybrid A* 的主要特点是： 考虑物体的实际运动方向约束，不像A*假定所有的相邻节点都可以顺利转移 A 的物体总是出现在栅格中心，而 Hybrid A 则不一定 Hybrid A* 是连续路径 Hybrid A 基于A 算法流程:主要流程如下图所示，需要说明的是： ​ 算法考虑了车辆的x,y坐标，偏航角。 G值更新策略如下： 123456789101112131415161718192021222324252627282930313233//###################################################// MOVEMENT COST//###################################################void Node3D::updateG() &#123; // 前向行驶 if (prim &lt; 3) &#123; // penalize turning 当前一个节点的prim与当前节点的prim不相等时，判断发生偏转。 if (pred-&gt;prim != prim) &#123; // 方向变化 if (pred-&gt;prim &gt; 2) &#123; g += dx[0] * Constants::penaltyTurning * Constants::penaltyCOD; &#125; else &#123; g += dx[0] * Constants::penaltyTurning; &#125; &#125; else &#123; g += dx[0]; &#125; &#125; // 倒车行驶 else &#123; // penalize turning and reversing if (pred-&gt;prim != prim) &#123; // penalize change of direction if (pred-&gt;prim &lt; 3) &#123; g += dx[0] * Constants::penaltyTurning * Constants::penaltyReversing * Constants::penaltyCOD; &#125; else &#123; g += dx[0] * Constants::penaltyTurning * Constants::penaltyReversing; &#125; &#125; else &#123; g += dx[0] * Constants::penaltyReversing; &#125; &#125;&#125; H值更新策略如下： 可以支持倒车时计算当前节点到终点的Reeds-Shepp 曲线，仅支持前向行驶时计算Dubins曲线； H值为Reeds-Shepp曲线、Dubins曲线、曼哈顿距离三种cost解算出来的最大值。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061//###################################################// COST TO GO//###################################################void updateH(Node3D&amp; start, const Node3D&amp; goal, Node2D* nodes2D, float* dubinsLookup, int width, int height, CollisionDetection&amp; configurationSpace, Visualize&amp; visualization) &#123; float dubinsCost = 0; float reedsSheppCost = 0; float twoDCost = 0; float twoDoffset = 0; // if dubins heuristic is activated calculate the shortest path // constrained without obstacles if (Constants::dubins) &#123; ompl::base::DubinsStateSpace dubinsPath(Constants::r); State* dbStart = (State*)dubinsPath.allocState(); State* dbEnd = (State*)dubinsPath.allocState(); dbStart-&gt;setXY(start.getX(), start.getY()); dbStart-&gt;setYaw(start.getT()); dbEnd-&gt;setXY(goal.getX(), goal.getY()); dbEnd-&gt;setYaw(goal.getT()); dubinsCost = dubinsPath.distance(dbStart, dbEnd); &#125; // if reversing is active use a if (Constants::reverse &amp;&amp; !Constants::dubins) &#123; // ros::Time t0 = ros::Time::now(); ompl::base::ReedsSheppStateSpace reedsSheppPath(Constants::r); State* rsStart = (State*)reedsSheppPath.allocState(); State* rsEnd = (State*)reedsSheppPath.allocState(); rsStart-&gt;setXY(start.getX(), start.getY()); rsStart-&gt;setYaw(start.getT()); rsEnd-&gt;setXY(goal.getX(), goal.getY()); rsEnd-&gt;setYaw(goal.getT()); reedsSheppCost = reedsSheppPath.distance(rsStart, rsEnd); // ros::Time t1 = ros::Time::now(); // ros::Duration d(t1 - t0); // std::cout &lt;&lt; "calculated Reed-Sheep Heuristic in ms: " &lt;&lt; d * 1000 &lt;&lt; std::endl; &#125; // if twoD heuristic is activated determine shortest path // unconstrained with obstacles if (Constants::twoD &amp;&amp; !nodes2D[(int)start.getY() * width + (int)start.getX()].isDiscovered()) &#123; // ros::Time t0 = ros::Time::now(); // create a 2d start node Node2D start2d(start.getX(), start.getY(), 0, 0, nullptr); // create a 2d goal node Node2D goal2d(goal.getX(), goal.getY(), 0, 0, nullptr); // run 2d astar and return the cost of the cheapest path for that node nodes2D[(int)start.getY() * width + (int)start.getX()].setG(aStar(goal2d, start2d, nodes2D, width, height, configurationSpace, visualization)); &#125; if (Constants::twoD) &#123; // offset for same node in cell twoDoffset = sqrt(((start.getX() - (long)start.getX()) - (goal.getX() - (long)goal.getX())) * ((start.getX() - (long)start.getX()) - (goal.getX() - (long)goal.getX())) + ((start.getY() - (long)start.getY()) - (goal.getY() - (long)goal.getY())) * ((start.getY() - (long)start.getY()) - (goal.getY() - (long)goal.getY()))); twoDCost = nodes2D[(int)start.getY() * width + (int)start.getX()].getG() - twoDoffset; &#125; // return the maximum of the heuristics, making the heuristic admissable start.setH(std::max(reedsSheppCost, std::max(dubinsCost, twoDCost)));&#125; 代码参考：Hybrid A* Path Planner for the KTH Research Concept Vehicle 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156//###################################################// 3D A*//###################################################Node3D* Algorithm::hybridAStar(Node3D&amp; start, const Node3D&amp; goal, Node3D* nodes3D, Node2D* nodes2D, int width, int height, CollisionDetection&amp; configurationSpace, float* dubinsLookup, Visualize&amp; visualization) &#123; // PREDECESSOR AND SUCCESSOR INDEX int iPred, iSucc; float newG; // Number of possible directions, 3 for forward driving and an additional 3 for reversing int dir = Constants::reverse ? 6 : 3; // Number of iterations the algorithm has run for stopping based on Constants::iterations int iterations = 0; // VISUALIZATION DELAY ros::Duration d(0.003); // OPEN LIST AS BOOST IMPLEMENTATION typedef boost::heap::binomial_heap&lt;Node3D*, boost::heap::compare&lt;CompareNodes&gt; &gt; priorityQueue; priorityQueue O; // OPEN_LIST 优先队列 // update h value updateH(start, goal, nodes2D, dubinsLookup, width, height, configurationSpace, visualization); // mark start as open start.open(); // push on priority queue aka open list O.push(&amp;start); iPred = start.setIdx(width, height); nodes3D[iPred] = start; // NODE POINTER Node3D* nPred; Node3D* nSucc; // float max = 0.f; // continue until O empty while (!O.empty()) &#123; // pop node with lowest cost from priority queue nPred = O.top(); // set index iPred = nPred-&gt;setIdx(width, height); iterations++; // RViz visualization if (Constants::visualization) &#123; visualization.publishNode3DPoses(*nPred); visualization.publishNode3DPose(*nPred); d.sleep(); &#125; // _____________________________ // LAZY DELETION of rewired node // if there exists a pointer this node has already been expanded if (nodes3D[iPred].isClosed()) &#123; // pop node from the open list and start with a fresh node O.pop(); continue; &#125; // _________________ // EXPANSION OF NODE else if (nodes3D[iPred].isOpen()) &#123; // add node to closed list nodes3D[iPred].close(); // remove node from open list O.pop(); // _________ // GOAL TEST 检测当前节点是否是终点或者是否超出了解算最大时间 if (*nPred == goal || iterations &gt; Constants::iterations) &#123; // DEBUG return nPred; &#125; // ____________________ // CONTINUE WITH SEARCH else &#123; // _______________________ // SEARCH WITH DUBINS SHOT if (Constants::dubinsShot &amp;&amp; nPred-&gt;isInRange(goal) &amp;&amp; nPred-&gt;getPrim() &lt; 3) &#123; nSucc = dubinsShot(*nPred, goal, configurationSpace); if (nSucc != nullptr &amp;&amp; *nSucc == goal) &#123; //DEBUG // std::cout &lt;&lt; "max diff " &lt;&lt; max &lt;&lt; std::endl; return nSucc; &#125; &#125; // ______________________________ // SEARCH WITH FORWARD SIMULATION for (int i = 0; i &lt; dir; i++) &#123; // 创建下一个扩展节点，这里有三种可能的方向，如果可以倒车的话是6种方向 nSucc = nPred-&gt;createSuccessor(i); // 设置节点遍历标识 iSucc = nSucc-&gt;setIdx(width, height); // 判断扩展节点是否满足约束，能否进行遍历 if (nSucc-&gt;isOnGrid(width, height) &amp;&amp; configurationSpace.isTraversable(nSucc)) &#123; // 确定新扩展的节点不在close list中，或者没有在之前遍历过 if (!nodes3D[iSucc].isClosed() || iPred == iSucc) &#123; // 更新G值 nSucc-&gt;updateG(); newG = nSucc-&gt;getG(); // 如果扩展节点不在OPEN LIST中，或者找到了更短G值的路径 if (!nodes3D[iSucc].isOpen() || newG &lt; nodes3D[iSucc].getG() || iPred == iSucc) &#123; // calculate H value updateH(*nSucc, goal, nodes2D, dubinsLookup, width, height, configurationSpace, visualization); // if the successor is in the same cell but the C value is larger 如果扩展节点与当前节点在同一个栅格，并且cost值更大，则略过 if (iPred == iSucc &amp;&amp; nSucc-&gt;getC() &gt; nPred-&gt;getC() + Constants::tieBreaker) &#123; delete nSucc; continue; &#125; // if successor is in the same cell and the C value is lower, set predecessor to predecessor of predecessor else if (iPred == iSucc &amp;&amp; nSucc-&gt;getC() &lt;= nPred-&gt;getC() + Constants::tieBreaker) &#123; nSucc-&gt;setPred(nPred-&gt;getPred()); &#125; if (nSucc-&gt;getPred() == nSucc) &#123; std::cout &lt;&lt; "looping"; &#125; // put successor on open list nSucc-&gt;open(); nodes3D[iSucc] = *nSucc; O.push(&amp;nodes3D[iSucc]); delete nSucc; &#125; else &#123; delete nSucc; &#125; &#125; else &#123; delete nSucc; &#125; &#125; else &#123; delete nSucc; &#125; &#125; &#125; &#125; &#125; if (O.empty()) &#123; return nullptr; &#125; return nullptr;&#125; Rviz实验效果]]></content>
      <tags>
        <tag>路径规划算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RNN反向传播推导]]></title>
    <url>%2Fpost%2Fcf8dbd9a.html</url>
    <content type="text"><![CDATA[参考： 循环神经网络(RNN)模型与前向反向传播算法 RNN面试总结 RNN正向传播推导RNN原理图： 变量说明 $W,U,V​$为权重矩阵，在整个RNN网络中是共享的。 $h^t​$代表在序列$t​$时模型的隐藏状态。$h^t​$由$x^t​$和$h^{t-1}​$共同决定。 $o^{t}$代表在序列$t$时模型的输出。$o^t$只由模型当前的隐藏状态$h^{t}$决定。 $x^t$代表在序列$t$时训练样本的输入。 $L_t​$代表在序列$t​$时模型的损失函数。 $y^t$代表在序列$t​$时训练样本序列的真实输出。 前向公式 \begin{aligned} &h^t=\sigma(Wh^{t-1}+Ux^{t}+B) \\ &o^t=Vh^t+C\\ &\hat{y}^t=\sigma_{softmax}(o^t) \end{aligned}RNN反向传播推导 L=\sum_TL_t $L$对$C$的梯度： \frac{\partial{L}}{\partial{C}} =\sum_T\frac{\partial{L_t}}{\partial{o^t}} \cdot \frac{\partial{o^t}}{\partial{C}}=\sum_T(\hat{y}^t-y^t) $L$ 对V的梯度： \frac{\partial{L}}{\partial{V}}=\sum_T\frac{\partial{L_t}}{\partial o^t}\cdot\frac{\partial o^t}{\partial V}=\sum_T(\hat{y}^t-y^t)(h^t)^T $L$ 对$h^t$的梯度： \begin{aligned} \delta^{t}&=\frac{\partial L}{\partial h^{t}} \\ &=\frac{\partial L}{\partial o^{t}} \frac{\partial o^{t}}{\partial h^{t}} + \frac{\partial L}{\partial h^{t+1}}\frac{\partial h^{t+1}}{\partial h^{t}} \\ &=V^T(\hat{y}^{(t)} - y^{(t)}) + W^T\delta^{(t+1)}diag(1-(h^{(t+1)})^2) \end{aligned}​ 其中： ​ (1) \frac{\partial L}{\partial o^{t}} \frac{\partial o^{t}}{\partial h^{t}} = V^T(\hat{y}^{t} - y^{t})​ (2) \begin{aligned} \frac{\partial L}{\partial h^{t+1}}\frac{\partial h^{t+1}}{\partial h^{t}}&=\delta^{t+1} \frac{\partial h^{t+1}}{\partial h^{t}}\\ &=W^T\delta^{t+1}diag(1-(h^{t+1})^2) \end{aligned}​ 详细推导： $L​$ 对$W​$的梯度： \frac{\partial L}{\partial W} = \sum\limits_{t=1}^{\tau}\frac{\partial L}{\partial h^{t}} \frac{\partial h^{t}}{\partial W} = \sum\limits_{t=1}^{\tau}diag(1-(h^{t})^2)\delta^{t}(h^{t-1})^T $L$ 对$B$的梯度： \frac{\partial L}{\partial B}= \sum\limits_{t=1}^{\tau}\frac{\partial L}{\partial h^{t}} \frac{\partial h^{t}}{\partial b} = \sum\limits_{t=1}^{\tau}diag(1-(h^{(t)})^2)\delta^{t} $L$ 对$U$的梯度： \frac{\partial L}{\partial U} = \sum\limits_{t=1}^{\tau}\frac{\partial L}{\partial h^{t}} \frac{\partial h^{t}}{\partial U} = \sum\limits_{t=1}^{\tau}diag(1-(h^{t})^2)\delta^{t}(x^{t})^T]]></content>
      <tags>
        <tag>循环神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[参数初始化与正则化]]></title>
    <url>%2Fpost%2F7a6400a0.html</url>
    <content type="text"><![CDATA[参考文献： 神经网络调优 深度学习之参数初始化（一）——Xavier初始化 一、参数初始化参数初始化的作用： 目的是为了让神经网络在训练过程中学习到有用的信息 参数梯度不应该全部为0 各层激活值不会出现饱和现象 各层激活值不为0 1.1 标准初始化方法 特点： 隐层的状态的均值为0，方差为常量$\frac{1}{3}$，和网络的层数无关 标准初始化只适用于满足Glorot假设的激活函数，比如tanh。 推导： 符合输入参数的均匀分布，$n$是输入层神经元个数。 W_{ij} \sim U[-\frac{1}{\sqrt n}, \frac{1}{\sqrt n}]​ 权重值的方差为（均匀分布 $D(x)=(b-a)²/12$）: Var(W_{ij})=\frac{1}{3n}​ 现在把输入$X$的每一维度$x$看做一个随机变量，并且假设$E(x)=0$, $Var(x)=1$。假设$w$和$x$相互独立，则隐层状态的方差为 : \begin{align} Var(z_k)= & Var(\sum_{i=0}^{n}W_{ki}x_i) \\ = &\sum_{i=0}^{n}Var(W_{ki})Var(x_i) \\ = & \sum_{i=0}^{n}Var(W_{ki}) \\ = & \sum_{i=0}^{n}\frac{1}{3n} \\ = &\frac{1}{3} \end{align}​ 可以看出标准初始化方法得到一个非常好的特性：隐层的状态的均值为0，方差为常量$\frac{1}{3}$，和网络的层数无关，这意味着对于sigmoid函数来说，自变量落在有梯度的范围内。 1.2 Glorot条件上述参数梯度不应该全部为0的条件只能保证网络能学到东西，而Glorot认为，优秀的初始化应该使得各层的激活值和状态梯度的方差在传播过程中的方差保持一致。 条件： 输入假设 输入的每个特征方差一样：Var(x) 激活函数假设： 激活函数$f(x)$对称：这样就可以假设每层的输入均值都是0 $f\prime(0)=1$ 初始时，状态值落在激活函数的线性区域：$f\prime(s_k^i)\approx 1$ 符合该条件的特征： 激活值方差和层数相关，反向传播的梯度方差和层数是有关系的，而参数梯度的方差和层数无关 相关推导过程: Xavier初始化推导 1.3 Xavier初始化为了保证前向传播和反向传播时每一层的方差一致，可以将Glorot条件转换成($n_i$为第$i$层的神经元个数)： \forall i, n_iVar(W^{i+1})=1 \forall i, n_{i+1}Var(W^{i+1})=1输入与输出的个数往往不相等，于是为了均衡考量，根据由Glorot条件，得到方差设定符合： \forall i, Var(W^{i+1})=\frac{2}{n_i+n_{i+1}}该方差对应的均匀分布即为Xavier初始化分布： W\sim U\left[ -\frac{\sqrt{6}}{\sqrt{n_i + n_{i+1}}},\frac{\sqrt{6}}{\sqrt{n_i + n_{i+1}}} \right] 特点: 激活值的方差和层数无关，反向传播梯度的方差和层数无关 二、正则化正则化的目的是对网络参数进行惩罚，特别是阶数较高项$x^n$ $ (网络表达式： w_1x^n+w_2x^{n-1}+…+w_n)$的系数参数，因为如果该项系数较大，是的拟合曲线复杂，会导致网络过拟合。引入正则化，则不会因为$x$的较小变动导致输出的较大变化，对噪声的容忍程度比较好。 没有正则化：($\eta$是学习率) \begin{equation}w\to w'=w-\eta\frac{\partial C_0}{\partial w}\end{equation}2.1 L1正则 （带有特征选择能力）$C$是正则化后的损失函数，$C_0$是正则化前的损失函数。 \begin{aligned} &C=C_0+\frac{\lambda}{n}\sum_{i=1}^{n}|w_i| \\ \end{aligned} \frac{\partial C}{\partial w}=\frac{\partial C_0}{\partial w} + \frac{\lambda}{n}sgn(w) sgn(w)=\begin{equation} \left\{ \begin{aligned} 1 ,w≥1\\ 0, w=0\\ \end{aligned} \right. \end{equation} \\那么权重更新为： w \to w-\frac{\eta\lambda}{n}sgn(w)-\eta\frac{\partial C_0}{\partial w}=w\pm\frac{\eta\lambda}{n}-\eta\frac{\partial C_0}{\partial w} L1正则的作用是使$w​$在每一次迭代时都变化一个常数：$\frac{\eta\lambda}{n}​$。 当$w​$本身比较小时，L1正则比L2正则衰减得更厉害。L1正则的效果是使不重要的$w​$几乎衰减为0。 因而L1具有一定的稀疏性，并有一定的特征选择的功能。 2.2 L2正则 (与权重衰减不完全等价) C=C_0+\frac{\lambda}{n}\sum_{i=1}^n{w_i^2}$C_0$是正则化之前的损失函数，$λ$是正则项系数，$n$是参数$w$的个数。最小化损失函数$C$的同时也会使$\sum_{i=1}^n{w_i^2}​$尽可能小。 加上正则项后梯度变为: \frac{\partial C}{\partial w}=\frac{\partial C_0}{\partial w}+\frac{\lambda}{2n}w \begin{equation}w\to w'=w-\eta\frac{\partial C}{\partial w}=\left(1-\frac{\eta\lambda}{n}\right)w-\eta\frac{\partial C_0}{\partial w}\end{equation} L2正则项的作用是使$w$在每次迭代时都变小了$\frac{\eta\lambda}{n}$倍。 如果要使这个倍率不变，那么当神经元个数增多（即$n$变大）时，正则项系数$λ$也应该相应调大。 2.3 L2 正则 vs 权值衰减​ L2正则化是在目标函数中直接加上一个正则项，直接修改了我们的优化目标。 ​ 权值衰减是在训练的每一步结束的时候，对网络中的参数值直接裁剪一定的比例，优化目标的式子是不变的。 ​ 在使用朴素的梯度下降法时二者是同一个东西，因为此时L2正则化的正则项对梯度的影响就是每次使得权值衰减一定的比例。 ​ 但是在使用一些其他优化方法的时候，就不一样了。比如说使用Adam方法时，每个参数的学习率会随着时间变化。这时如果使用L2正则化，正则项的效果也会随之变化；而如果使用权值衰减，那就与当前的学习率无关了，每次衰减的比例是固定的。]]></content>
      <tags>
        <tag>深度学习基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[反向传播公式推导]]></title>
    <url>%2Fpost%2F9e924caa.html</url>
    <content type="text"><![CDATA[参考博客：反向传播算法（过程及公式推导） 1. 符号定义 $w_{jk}^L$ 表示第$L-1$层的第$j$个神经元到第$L$层的第$k$个神经元映射的权值。 $b_k^L$ 表示第$L$层的第$k$个神经元的偏置量。 $z_k^L=\sum_j w_{jk}^L a_j^{L-1}+b_k^L​$ 表示第$L​$层的未经激活函数的输出。 $a_k^{L}=\sigma(z_j^L)$ 表示第$L$层经过sigmoid函数后的输出。 2. 损失函数定义二次代价函数：($x$代表输入的样本，$y(x)$代表标签值) C=\frac{1}{2n}\sum_x||y(x)-a^L(x)||^2当只关注某一个样本$x_i​$的时候，有： C=\frac{1}{2}(y-a)^23. 反向传播推导 计算最后一层神经网络产生的错误 对于每一个$L$层的神经元有： \delta^L_k=\frac{\partial{C}}{\partial{z_k^L}}=\frac{\partial{C}}{\partial{a_{k}^L}}\cdot\frac{\partial{a_k^L}}{\partial{z_{k}^{L}}}​ 则整个$L​$层可以用矩阵的Hadamard积(矩阵行行对应相乘)来进行计算： \delta^L=\nabla_aC\odot\sigma^\prime(z^L) 反向传播 计算每一层的每个神经元产生的误差，推广到每一层的每个神经元有： \begin{align} \delta_k^L=\frac{\partial C}{\partial z_k^L}&=\sum_m\frac{\partial C}{\partial z_m^{L+1}}\cdot\frac{\partial{z_m^{L+1}}}{\partial{a_{k}^{L}}}\cdot\frac{\partial a_k^L}{\partial z_k^L}\\ &=\sum_m{\delta_m^{L+1}}\cdot\frac{w_{km}^{L+1}a_k^{L}+b_m^{L+1}}{\partial{a_k^L}}\cdot\sigma'(z_k^L)\\ &=\sum_m\delta_m^{L+1}\cdot w_{km}^{L+1}\cdot\sigma'(z_k^L) \end{align}推广到整个一层有： \delta^L=((W^{L+1})^T\cdot \delta^{L+1})\odot \sigma'(z^L) 计算权重的梯度 \frac{\partial C}{\partial w_{jk}^L}=\frac{\partial C}{\partial{z_k^L}}\cdot \frac{\partial{z_k^L}}{\partial{w_{jk}^L}}=\delta_k^L\cdot\frac{\partial{(w_{jk}^La_j^{L-1}+b_k^L)}}{\partial w_{jk}^L}=a_j^{L-1}\cdot\delta_k^L 计算偏置的梯度 \frac{\partial C}{\part b_k^L}=\frac{\partial{C}}{\partial z_{k}^L} \cdot \frac{\partial z_k^L}{\partial b_k^L}=\delta_k^L\cdot\frac{\partial{(w_{jk}^La_j^{L-1}+b_k^L)}}{\partial b_{k}^L}=\delta_k^L 4.总结反向传播四个公式： 输出层误差： \delta^L=\nabla_aC\odot\sigma^\prime(z^L) 反向传播每一层误差： \delta^L=((W^{L+1})^T\cdot \delta^{L+1})\odot \sigma'(z^L) 权重梯度： \frac{\partial C}{\partial w_{jk}^L}=a_j^{L-1}\cdot\delta_k^L 偏置梯度： \frac{\partial C}{\partial b_k^L}=\delta_k^L]]></content>
      <tags>
        <tag>深度学习基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习常见Loss推导]]></title>
    <url>%2Fpost%2F19fc17a4.html</url>
    <content type="text"><![CDATA[1. Softmax Loss 推导 (1) $N$为类别数 (2) $a$为输出向量，$a_j$为向量$a$的第$j$个值 参考：卷积神经网络系列之softmax loss对输入的求导推导 1.1 Softmax S_i=\frac{e^{a_i}}{\sum_{j}e^{a_j}} 1.2 Cross-entropy Loss L=\sum_{j}-y_ilnP_j1.3 Softmax Loss 当Cross-entropy的$P_j=S_i$ ，即Softmax输出的时候。 L_{softmax}=\sum_{j}-y_ilnS_i1.4 Softmax对Softmax输入的导数 $S_i$对$a_j$求导： \frac{\partial{S_i}}{\partial{a_j}}=\frac{\partial{\frac{e^{a_i}}{\sum_ke^{a_k}}}}{\partial a_j} 这里求导有两种情况 1）当$i=j$时： \begin{aligned} \frac{\partial{S_i}}{\partial{a_j}} &=\frac{\partial{S_i}}{\partial{a_i}} \\ &=\frac{\frac{\partial{e^{a_i}}}{\partial{a_i}}\sum_ke^{a_k}-\frac{\sum_ke^{a_k}}{\partial{a_i}}e^{a_i}}{(\sum_ke^{a_k})^2} \\ &=\frac{e^{a_i}\sum_ke^{a_k}-e^{a_i}e^{a_i}}{\sum_k{e^{2a_k}}} \\ &=\frac{e^{a_i}}{\sum_k{e^{a_k}}}-\frac{e^{a_i}}{\sum_k{e^{a_k}}}\cdot{\frac{e^{a_i}}{\sum_k{e^{a_k}}}} \\ &=S_i-S_i^2\\ &=S_i(1-S_i) \end{aligned} 2）当$i≠j$时： \begin{aligned} \frac{\partial{S_i}}{\partial{a_j}} &=\frac{\frac{\partial{e^{a_i}}}{\partial{a_j}}\sum_ke^{a_k}-\frac{\sum_ke^{a_k}}{\partial{a_j}}e^{a_i}}{(\sum_ke^{a_k})^2} \\ &=\frac{-e^{a_j}e^{a_i}}{\sum_k{e^{2a_k}}} \\ &=-S_jS_i \end{aligned} 1.5 Softmax Loss对softmax输入的导数 第③个等号就用到了上面$S_i$对$a_j$求导的结论，第三个等号结果的左半部分是$i=k$的时候$S_i$对$a_j$求导的导数，右半部分是$i≠k$的时候S_i$对​$a_j$求导的导数。 第⑥、⑦个等号是将$y_iS_i​$合并到∑里面。最后一个等号的成立是建立在假设$∑y_k=1​$的前提下，这个对于常见的单标签分类任务而言都是成立的。 \begin{aligned} \frac{\partial{L_{softmax}}}{\partial{a_k}}&=-\sum_ky_k\frac{\partial{lnS_k}}{\partial{a_i}} \\ &=-\sum_ky_k\frac{1}{S_k}\frac{\partial{S_k}}{\partial{a_i}} \\ &=-y_i(1-S_i)-\sum_{k≠i}y_k\frac{1}{S_k}({-S_kS_i}) \\ &=-y_i(1-S_i)+\sum_{k≠i}y_kS_i \\ &=-y_i+y_iS_i+\sum_{k≠i}y_kS_i \\ &=\sum_ky_kS_i-y_i \\ &=S_i-y_i \end{aligned}1.6 总结因此假设一个5分类任务，经过Softmax层后得到的概率向量$S$是$[0.1,0.2,0.25,0.4,0.05]$，真实标签$y$是$[0,0,1,0,0]$，那么损失回传时该层得到的梯度就是$p-y=[0.1,0.2,-0.75,0.4,0.05]$。这个梯度就指导网络在下一次forward的时候更新该层的权重参数。]]></content>
      <tags>
        <tag>深度学习基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习优化器]]></title>
    <url>%2Fpost%2F1eda57ac.html</url>
    <content type="text"><![CDATA[参考资料：深度学习最全优化方法总结比较（SGD，Adagrad，Adadelta，Adam，Adamax，Nadam） 1. 随机梯度下降 批梯度下降(Gradient Descent) 随机批梯度下降(Stotastic Gradient Descent) 每次梯度计算只使用一个随机样本(可能是噪声样本) 避免在类似的样本上进行冗余计算 增加了跳出当前局部最小值的可能 可以通过减小学习率，来使其能够与GD有相同的收敛速度 小批量随机梯度下降(Mini batch SGD) 每次梯度计算使用小批量的样本 梯度计算比单样本计算更加稳定 便于使用矩阵计算 适当的batch size训练效率高 2. 随机梯度下降的困难 局部梯度的反方向不一定是函数整体的下降方向。 学习率衰减法，难以根据数据进行自适应。 对不同的参数采取不同的学习率(数据稀疏，不平衡)。 容易困在局部最小点，甚至是鞍点。 3. 动量方法(Momentum) 目的: 解决随机梯度的局部梯度的反方向不一定是函数整体的下降方向问题。 方法： 动量法：(Momentum)（适用于隧道型曲面） 方法： 每次更新都吸收上一次更新的余势。使得主体方向得到了更好的保留，使得效果被不断的放大。 v_t = \gamma v_{t-1}+\eta\nabla_{\theta}J(\theta) \\ \theta_t=\theta_{t-1}-v_t 缺点： 在前期下降比较快，收敛速度较好，但到最优值附近时容易由于动量过大而导致优化过度。 改进动量法：(Nesterov) 方法：利用主体的下降方向，预判下一步优化的位置，根据预判的位置计算优化的梯度。 v_t = \gamma v_{t-1} + \eta \nabla_{\theta}J(\theta-\gamma v_{t-1}) \\ \theta=\theta-v_tmomentum首先计算一个梯度(短的蓝色向量)，然后在加速更新梯度的方向进行一个大的跳跃(长的蓝色向量)，nesterov项首先在之前加速的梯度方向进行一个大的跳跃(棕色向量)，计算梯度然后进行校正(绿色梯向量) 4. 自适应梯度方法(Ada) 目的： 解决学习率衰减法，难以根据数据进行自适应的问题。 更新频繁的参数使用较小的学习率。 更新较少的参数使用较大的学习率。 方法： Adagrad方法： 思路：Adagrad对每个参数的历史梯度更新进行叠加，并以此作为下一次更新的惩罚系数。（约束学习率） 算法： 梯度：$g_{t,i}=\nabla_{\theta}J(\theta_i)$ 梯度历史矩阵: $G_t$是对角阵，其中$G_{t,ii}=\sum_{k}g_{k,i}^2$ 参数更新：（历史梯度大，则$\eta$项越小） \theta_{t+1,i}=\theta_{t,i}-\frac{\eta}{\sqrt{G_{t,ii}+\epsilon}}\cdot g_{t, i} 存在的问题： 随着训练的进行，学习率衰减过快。 梯度与参数单位不匹配 RMSprop（Adadelta方法第一版）： 目的：解决随着训练的进行，学习率衰减过快。 思路：使用梯度平方的移动平均来取代全部的历史平方和。 算法： 梯度：$g_{t,i}=\nabla_{\theta}J(\theta_i)$ 移动平均: $\mathbb{E}_{t}[g^2]=\gamma \mathbb{E}_{t-1}[g^2] + (1-\gamma) g_{t}^2$ 参数更新：（更新系数分母换了） \theta_{t+1, i} = \theta_{t,i} - \frac{\eta}{\sqrt{\mathbb{E}_{t,ii}+\epsilon}} \cdot g_{t,i} 特点： 其实RMSprop依然依赖于全局学习率 RMSprop算是Adagrad的一种发展，和Adadelta的变体，效果趋于二者之间 适合处理非平稳目标 对于RNN效果很好 Adadelta方法第二版: 目的：梯度与参数单位不匹配 思路：使用参数更新的移动平均来取代学习率$\eta$ 算法： 参数更新: （学习率换成参数的移动平均自适应） \theta_{t+1, i} = \theta_{t,i} - \frac{\sqrt{\mathbb{E}_{t-1}[\Delta \theta]}}{\sqrt{\mathbb{E}_{t,ii}+\epsilon}} \cdot g_{t,i} 特点： 训练初中期，加速效果不错，很快 训练后期，反复在局部最小值附近抖动 5. 动量+自适应方法(Adam) Adam （带动量项的RMSprop） 思路：在Adadelta的梯度平方和(二阶矩)的基础上引入动量方法的的一阶矩(梯度) 算法： 一阶矩(动量)： $m_t=\beta_1 m_{t-1}+(1-\beta_1)g_t$ （保持下降速度） 二阶矩(Adadelta)：$v_t=\beta_2 v_{t-1}+(1-\beta_2)g_t^2$ （保持参数自适应） 参数更新： \theta_{t+1}=\theta_{t} - \frac{\eta}{\sqrt{v_{t}}+\epsilon}m_t 特点： 经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。 适用于大数据集和高维空间 NAdam (引入Nesterov) 对学习率有了更强的约束 6. 小结 对于稀疏数据，尽量使用学习率可自适应的优化方法，不用手动调节，而且最好采用默认值 SGD通常训练时间更长，容易陷入鞍点，但是在好的初始化和学习率调度方案的情况下，结果更可靠 如果在意更快的收敛，并且需要训练较深较复杂的网络时，推荐使用学习率自适应的优化方法。 Adadelta，RMSprop，Adam是比较相近的算法，在相似的情况下表现差不多。 在想使用带动量的RMSprop，或者Adam的地方，大多可以使用Nadam取得更好的效果 鞍点：]]></content>
      <tags>
        <tag>深度学习基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[刷题-搜索]]></title>
    <url>%2Fpost%2Fd42edc1.html</url>
    <content type="text"><![CDATA[1. [Leetcode 200] 岛屿的个数 解法： DFS：一次遍历，遇到1则进行深搜，遍历过的1设为2，遇到非1或者越界则深搜停止，统计深搜的次数，即为岛屿数量。(这里采用) BFS: 循环遍历每个点，如果该点是0，则跳过，如果是1，岛屿数目加1，并加入队列，并将该点改为0，避免重复访问，然后进行广搜，取出队首元素，搜索该点周围四个点，如果有1就加入队列，并将1改为0，否则就跳过，当队列空时，一块岛屿搜索完毕，进入下一块搜索。(代码可参考：岛屿的个数 BFS) 并查集: 将二维数组重新编号，从0开始，从左到右，从上到下，直到n*m-1（其中n为行数，m为列数），对于位置(i,j)则编号为i*m+j，那么相邻（左右，上下）的为同一个值，则认为他们相通。那么最终只要统计一下father[i]==i且对应值为1的个数即可。(代码可参考: 岛屿的个数 Disjoint Set) 代码： 123456789101112131415161718192021222324252627282930313233class Solution &#123;public: int numIslands(vector&lt;vector&lt;char&gt;&gt;&amp; grid) &#123; if(grid.empty()) return 0; int res = 0; for(int i = 0; i &lt; grid.size(); i++)&#123; for(int j = 0; j &lt; grid[0].size(); j++)&#123; if(grid[i][j] == '1')&#123; dfs(grid, i, j); res++; &#125; &#125; &#125; return res; &#125; void dfs(vector&lt;vector&lt;char&gt;&gt;&amp; grid, int row, int col)&#123; if(row &lt; 0 || col &lt; 0 || row &gt;= grid.size() || col &gt;= grid[0].size() || grid[row][col] == '0' || grid[row][col] == '2') return; if(grid[row][col] == '1') grid[row][col] = '2'; dfs(grid, row-1, col); dfs(grid, row+1, col); dfs(grid, row, col-1); dfs(grid, row, col+1); &#125; &#125;; 2. [Leetcode 77] 组合 模板题：重点在于dfs中的startPos=i+1 代码： 1234567891011121314151617181920212223242526class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; combine(int n, int k) &#123; vector&lt;vector&lt;int&gt;&gt; res; vector&lt;int&gt; inRes; if(n&lt;=0 || k&lt;=0) return &#123;&#125;; dfs(res, inRes, n, k, 1); return res; &#125; void dfs(vector&lt;vector&lt;int&gt;&gt; &amp;res, vector&lt;int&gt; &amp;inRes, int n, int k, int startPos)&#123; if(inRes.size()==k)&#123; res.emplace_back(inRes); return; &#125; for(int i = startPos; i&lt;=n; i++)&#123; inRes.emplace_back(i); dfs(res, inRes, n, k, i+1); //重点在于i+1 inRes.pop_back(); &#125; &#125;&#125;; 3. [Leetcode 39] 组合总和(可重复选取数字) 基于模板加一些改动即可，由于可重复选择数字，将startPos=i+1 改成startPos=i，判停条件满足目标和就可以了,(前提是有序数组，先拍下序) 代码： 123456789101112131415161718192021222324252627282930313233343536class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; combinationSum(vector&lt;int&gt;&amp; candidates, int target) &#123; if(candidates.size() &lt;= 0) return &#123;&#125;; vector&lt;vector&lt;int&gt;&gt; res; vector&lt;int&gt; tmp; sort(candidates.begin(), candidates.end()); //保证有序 dfs(res, target, tmp, candidates, 0); return res; &#125; void dfs(vector&lt;vector&lt;int&gt;&gt; &amp;res,int target, vector&lt;int&gt; tmp, vector&lt;int&gt;&amp; candidates, int startPos) &#123; int sum = 0; for(auto i : tmp) sum += i; if (sum &gt; target) &#123; return; &#125; if (sum == target) //求和判停 &#123; res.emplace_back(tmp); return; &#125; for(int i = startPos; i &lt; candidates.size(); i++) &#123; tmp.emplace_back(candidates[i]); dfs(res, target, tmp, candidates, i); tmp.pop_back(); &#125; &#125;&#125;; 4. [Leetcode 40] 组合总和 II (不重复) 解法：在上一题的基础上使用Set进行去重 代码: 123456789101112131415161718192021222324252627282930class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; combinationSum2(vector&lt;int&gt;&amp; candidates, int target) &#123; if (candidates.size() &lt;= 0) return &#123;&#125;; set&lt;vector&lt;int&gt;&gt; res; vector&lt;int&gt; curr; sort(candidates.begin(), candidates.end()); dfs(res, target, 0, curr, candidates); return vector&lt;vector&lt;int&gt;&gt;(res.begin(), res.end()); &#125; void dfs(set&lt;vector&lt;int&gt;&gt; &amp;res, int target, int s, vector&lt;int&gt; curr, vector&lt;int&gt;&amp; candidates) &#123; if(target == 0) &#123; res.insert(curr); return; &#125; for(int i = s; i &lt; candidates.size(); i++) &#123; if(target - candidates[i] &lt; 0) break; curr.emplace_back(candidates[i]); dfs(res, target - candidates[i], i+1, curr, candidates); curr.pop_back(); &#125; &#125;&#125;; 5.[Leetcode 216] 组合总和 III 解法: 代码 1234567891011121314151617181920212223242526272829class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; combinationSum3(int k, int n) &#123; vector&lt;int&gt; nums; vector&lt;vector&lt;int&gt;&gt; res; vector&lt;int&gt; curr; for(int i = 0; i &lt; 9; i++) nums.emplace_back(i+1); dfs(res, k, n, 0, 0, curr, nums); return res; &#125; void dfs(vector&lt;vector&lt;int&gt;&gt; &amp;res, int k, int target, int s, int depth, vector&lt;int&gt; curr, vector&lt;int&gt;&amp; nums) &#123; if(target == 0 &amp;&amp; depth == k) &#123; res.emplace_back(curr); return; &#125; for(int i = s; i &lt; nums.size(); i++) &#123; if(target - nums[i] &lt; 0) break; curr.emplace_back(nums[i]); dfs(res, k, target - nums[i], i+1 , depth+1 , curr, nums); curr.pop_back(); &#125; &#125;&#125;; 6.[Leetcode 131] 分割回文串 解法: 按解组合的方式解，解的过程中判断回文串，组合的元素集合是按照从子串的第一个元素开始，到第i个元素截断作为一个新子串进行回文判断， 代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263class Solution &#123;public: vector&lt;vector&lt;string&gt;&gt; partition(string s) &#123; vector&lt;vector&lt;string &gt;&gt; res; vector&lt;string&gt; tmp; dfs(res, tmp, 0, s.length()-1, s); return res; &#125; void dfs(vector&lt;vector &lt;string&gt; &gt;&amp; res, vector&lt;string &gt;&amp; tmp, int startPos, int endPos, string &amp;s)&#123; //当原字符串都遍历完成后，结束一次搜索。 if(startPos &gt; endPos)&#123; res.emplace_back(tmp); return; &#125; //遍历子串，从0~1，0~2 ... 到0~n, 递归的起始位置为截断后的第一位 for(int i =1; i &lt;= endPos-startPos+1; i++)&#123; if(isPalindrome(s.substr(startPos, i)))&#123; tmp.emplace_back(s.substr(startPos, i)); //递归的起始位置为截断后的第一位 dfs(res, tmp, startPos+i, endPos, s); tmp.pop_back(); &#125; &#125; &#125; // 判断回文串，这里用的是可解决大小写字母，数字的，过滤其他符号。 bool isPalindrome(string s) &#123; int pStart = 0; int pEnd = s.length()-1; while(pStart &lt; pEnd)&#123; while (!checkChar(s[pStart]) &amp;&amp; pStart &lt; pEnd) pStart++; while (!checkChar(s[pEnd]) &amp;&amp; pStart &lt; pEnd) pEnd--; toLower(s[pStart]); toLower(s[pEnd]); if(s[pStart] == s[pEnd])&#123; pStart ++; pEnd--; &#125;else&#123; return false; &#125; &#125; return true; &#125; void toLower(char&amp; s)&#123; if(s &gt;= 'A' &amp;&amp; s &lt;= 'Z') s = s+32; &#125; bool checkChar(const char&amp; s)&#123; if ((s &gt;= 'a' &amp;&amp; s &lt;= 'z') || (s &gt;= '0' &amp;&amp; s &lt;= '9') || (s &gt;= 'A' &amp;&amp; s &lt;= 'Z')) return true; return false; &#125;&#125;; 7.机器人运动范围 规规矩矩没啥说的, 主要注意边界条件 代码: 1234567891011121314151617181920212223242526272829303132333435363738394041class Solution &#123;public: int movingCount(int threshold, int rows, int cols) &#123; int res = 0; vector&lt;vector&lt;int&gt;&gt; visited (rows, vector&lt;int&gt;(cols, 0)); Dfs(threshold, rows, cols, 0, 0, res, visited); return res; &#125; void Dfs(const int&amp; threshold, const int&amp; rows, const int&amp; cols, int rowIdx, int colIdx, int&amp; res, vector&lt;vector&lt;int&gt;&gt;&amp; visited)&#123; if (JudgeOverThreshold(threshold, rowIdx, colIdx) || JudgeOutside(rows, cols, rowIdx, colIdx) || visited[rowIdx][colIdx] ) return; visited[rowIdx][colIdx] = 1; res ++; Dfs(threshold, rows, cols, rowIdx+1, colIdx, res, visited); Dfs(threshold, rows, cols, rowIdx, colIdx+1, res, visited); Dfs(threshold, rows, cols, rowIdx-1, colIdx, res, visited); Dfs(threshold, rows, cols, rowIdx, colIdx-1, res, visited); &#125; bool JudgeOverThreshold(const int&amp; threshold, int rowIdx, int colIdx)&#123; int sum = 0; while(rowIdx)&#123; sum += rowIdx%10; rowIdx /= 10; &#125; while(colIdx)&#123; sum += colIdx%10; colIdx /= 10; &#125; if(sum &gt; threshold) return true; else return false; &#125; bool JudgeOutside(const int&amp; rows, const int&amp; cols, const int&amp; rowIdx, const int&amp; colIdx)&#123; if(rowIdx &lt; 0 || rowIdx &gt; rows-1 || colIdx &lt; 0 || colIdx &gt; cols-1) return true; else return false; &#125;&#125;;]]></content>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2Fpost%2F0.html</url>
    <content type="text"><![CDATA[title: “[刷题]链表专题”tags: 数据结构与算法abbrlink: 6db30a46date: 2019-02-12 21:23:22 [Leetcode 141, 142] 环形链表，环形链表 II （剑指Offer面试题23） 解法：(链表遍历，快慢指针) 判环：快慢指针，快指针+2， 慢指针+1，若快指针在到达链表尾(不带环的才有链表尾)之前与慢指针相遇，则有环。 找入口：快慢指针第一次相遇节点与头结点之间的节点数与环中节点数相同，两个指针一个从链表头开始，一个从相遇节点开始遍历，两个指针再次相遇节点为入口节点。 代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: // 找入口 ListNode *detectCycle(ListNode *head) &#123; if(!JudgeCycle(head)) return nullptr; ListNode *pFirstptr = head; //第一次相遇节点位置就是环内节点数 while(pFirstptr != pMeetNode)&#123; pFirstptr = pFirstptr -&gt; next; pMeetNode = pMeetNode -&gt; next; &#125; return pFirstptr; &#125; // 判环 bool JudgeCycle(ListNode *head)&#123; if(head == nullptr) return false; ListNode *pFastptr = head; ListNode *pSlowptr = head; while(pFastptr -&gt; next &amp;&amp; pFastptr -&gt; next -&gt; next)&#123; pFastptr = pFastptr -&gt; next -&gt; next; pSlowptr = pSlowptr -&gt; next; if(pFastptr == pSlowptr)&#123; pMeetNode = pFastptr; return true; &#125; &#125; return false; &#125;private: ListNode *pMeetNode;&#125;; [Leetcode 206] 反转链表 (剑指Offer面试题24) 解法： 代码： 1234567891011121314class Solution &#123;public: ListNode* ReverseList(ListNode* pHead) &#123; ListNode* h = NULL; ListNode* p = pHead; while(p)&#123; ListNode* tmp = p -&gt; next; p -&gt; next = h; h = p; p = tmp; &#125; return h; &#125;&#125;; [Leetcode 21] 合并两个有序链表 (剑指Offer面试题 25) 解法： 优先队列重排 (可参考 提交记录99%代码) 比较插入列表 （这里使用） 代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* mergeTwoLists(ListNode* l1, ListNode* l2) &#123; if(l1 == nullptr &amp;&amp; l2 == nullptr) return nullptr; if(l1 == nullptr &amp;&amp; l2 != nullptr) return l2; if(l2 == nullptr &amp;&amp; l1 != nullptr) return l1; ListNode* res; ListNode* pMergeNode; if(l1-&gt;val &lt; l2-&gt;val)&#123; pMergeNode = l1; l1 = l1-&gt;next; &#125; else&#123; pMergeNode = l2; l2 = l2-&gt;next; &#125; res = pMergeNode; while(true)&#123; if(l1 == nullptr &amp;&amp; l2 != nullptr)&#123; pMergeNode-&gt;next = l2; break; &#125; if(l2 == nullptr &amp;&amp; l1 != nullptr)&#123; pMergeNode-&gt;next = l1; break; &#125; if(l1 == nullptr &amp;&amp; l2 == nullptr)&#123; break; &#125; if(l1-&gt;val &lt; l2-&gt;val)&#123; pMergeNode-&gt;next = l1; l1 = l1-&gt;next; &#125;else&#123; pMergeNode-&gt;next = l2; l2 = l2-&gt;next; &#125; pMergeNode = pMergeNode-&gt;next; &#125; return res; &#125;&#125;; [Leetcode23] 合并K个排序链表 解法： 优先队列或列表遍历插入，优先队列效率相对高一些，这里列出的是列表遍历比较插入的代码，优先队列可参考提交记录99%代码。 代码： 123456789101112131415161718192021222324252627282930313233343536373839404142/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* mergeKLists(vector&lt;ListNode*&gt;&amp; lists) &#123; ListNode* res; ListNode* pMerge = new ListNode(0); res = pMerge; while(!isAllNodeEnd(lists))&#123; int tmp = INT_MAX; int index = 0; for(int i = 0; i &lt; lists.size() ; i++)&#123; if(lists[i] != nullptr)&#123; if(lists[i]-&gt;val &lt; tmp)&#123; tmp = lists[i]-&gt;val; index = i; &#125; &#125; &#125; pMerge-&gt;next = lists[index]; pMerge = pMerge-&gt;next; lists[index] = lists[index]-&gt;next; &#125; return res-&gt;next; &#125; bool isAllNodeEnd(vector&lt;ListNode*&gt;&amp; lists)&#123; for(auto item : lists)&#123; if(item != nullptr) return false; &#125; return true; &#125;&#125;;]]></content>
  </entry>
  <entry>
    <title><![CDATA[[刷题]递归回溯专题]]></title>
    <url>%2Fpost%2F8cb158ac.html</url>
    <content type="text"><![CDATA[1. [剑指Offer 面试题17] 打印从1到最大的n位数 题目：输入数字n，按顺序打印出从1到最大的n位十进制数。比如输入3， 打印出1,2，…，999。 解法： 全排列解，这里主要用这个方式，有需要在输出时，对高位0进行截断。 大数加法解 代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243class Overview17&#123;public: Overview17()&#123;&#125; ~Overview17()&#123;&#125; void PrintAllNumber(int n)&#123; vector&lt;int&gt; curr; // 递归当前数字 vector&lt;vector&lt;int&gt;&gt; res; //全部数字列表 dfs(res, 0, n, curr); // 打印数字 for (auto item : res)&#123; for (auto it : item) &#123; cout &lt;&lt; it; &#125; cout &lt;&lt; endl; &#125; &#125; void dfs(vector&lt;vector&lt;int&gt;&gt; &amp; res, int depth, int n, vector&lt;int&gt; curr)&#123; if (depth &gt;= n)&#123; res.emplace_back(curr); return; &#125; for (int i = 0; i &lt; 10; i++)&#123; curr.emplace_back(i); dfs(res, depth + 1, n, curr); curr.pop_back(); &#125; &#125;&#125;;int main()&#123; Overview17 ocs; ocs.PrintAllNumber(2); return 0;&#125; 2. [Leetcode 46] 全排列 解法：递归，注意去重 代码： 1234567891011121314151617181920212223242526272829303132333435class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; permute(vector&lt;int&gt;&amp; nums) &#123; if (nums.size() == 0) return&#123;&#125;; vector&lt;vector&lt;int&gt;&gt; res; vector&lt;int&gt; curr_vec; vector&lt;bool&gt; used; for (int i = 0; i &lt; nums.size(); i++) &#123; used.push_back(false); &#125; dfs(res, nums, 0, curr_vec, used); return res; &#125; void dfs(vector&lt;vector&lt;int&gt;&gt;&amp; res, vector&lt;int&gt;&amp; nums, int depth, vector&lt;int&gt; curr_vec, vector&lt;bool&gt; used) &#123; if (depth &gt;= nums.size()) &#123; res.emplace_back(curr_vec); return; &#125; for (int i = 0; i &lt; nums.size(); i++) &#123; if (used[i]) continue; used[i] = true; curr_vec.emplace_back(nums[i]); dfs(res, nums, depth + 1, curr_vec, used); curr_vec.pop_back(); used[i] = false; &#125; &#125;&#125;; 3. [Leetcode 47] 全排列 II 解法: 递归，有可重复数字，不要递归重了。对数列进行排序，让相同的数字连在一起，如果当前位已经使用过，或者该位的前一位与该位相同并且前一位没有被使用过，这时两个排列会相同，因而略过。 代码: 1234567891011121314151617181920212223242526272829303132333435363738394041class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; permuteUnique(vector&lt;int&gt;&amp; nums) &#123; if (nums.size() &lt;= 0) return&#123;&#125;; sort(nums.begin(), nums.end()); vector&lt;vector&lt;int&gt;&gt; res; vector&lt;bool&gt; used; vector&lt;int&gt; curr; for (auto item : nums) &#123; used.push_back(false); &#125; dfs(res, nums, curr, used, 0); return res; &#125; void dfs(vector&lt;vector&lt;int&gt;&gt;&amp; res, vector&lt;int&gt;&amp; nums, vector&lt;int&gt; curr, vector&lt;bool&gt; used, int depth) &#123; if (depth &gt;= nums.size()) &#123; res.emplace_back(curr); return; &#125; for (int i = 0; i &lt; nums.size(); i++) &#123; if (used[i]) continue; if (i &gt; 0 &amp;&amp; nums[i] == nums[i - 1] &amp;&amp; !used[i-1]) continue; used[i] = true; curr.emplace_back(nums[i]); dfs(res, nums, curr, used, depth + 1); curr.pop_back(); used[i] = false; &#125; &#125;&#125;;]]></content>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[刷题-栈和队列]]></title>
    <url>%2Fpost%2Fdb3d6a7.html</url>
    <content type="text"><![CDATA[1. [Leetcode 232] 用栈实现队列 （剑指OFFER面试题 9） 解法：一个栈(push栈)用于接收push，一个栈(pop栈)用于top(peek)和pop 当pop栈为空，且push栈不为空时，将push栈的元素转移到pop栈中 当pop栈不为空时，将pop栈的数据pop出去 push操作只在push栈进行 注意： leetcode上可以不进行异常处理，能a过，但是面试时候最好还是加上空栈的异常处理。 泛型支持。 线程安全。 1234567891011121314151617181920212223242526272829303132333435363738394041424344class MyQueue &#123;private: stack&lt;int&gt; spush; stack&lt;int&gt; spop;public: /** Initialize your data structure here. */ MyQueue() &#123; &#125; /** Push element x to the back of queue. */ void push(int x) &#123; spush.push(x); &#125; /** Removes the element from in front of queue and returns that element. */ int pop() &#123; if (spop.empty())&#123; while (!spush.empty()) &#123; spop.push(spush.top()); spush.pop(); &#125; &#125; int res = spop.top(); spop.pop(); return res; &#125; /** Get the front element. */ int peek() &#123; if (spop.empty())&#123; while (!spush.empty()) &#123; spop.push(spush.top()); spush.pop(); &#125; &#125; return spop.top(); &#125; /** Returns whether the queue is empty. */ bool empty() &#123; return spush.empty()&amp;&amp;spop.empty(); &#125;&#125;; 2.[Leetcode 155] 最小栈 (剑指Offer面试题30) 解法：双栈实现，一个栈用于正常的入栈出栈，一个栈用于记录最小值 代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445class MinStack &#123;public: /** initialize your data structure here. */ MinStack() &#123; &#125; void push(int x) &#123; mainStack.push(x); if(subStack.empty() || x &lt;= subStack.top())&#123; subStack.push(x); &#125; &#125; void pop() &#123; int tmp = mainStack.top(); mainStack.pop(); if(tmp == subStack.top())&#123; subStack.pop(); &#125; &#125; int top() &#123; int res = mainStack.top(); return res; &#125; int getMin() &#123; //if(subStack.empty()) return 0; return subStack.top(); &#125;private: stack&lt;int&gt; mainStack; stack&lt;int&gt; subStack;&#125;;/** * Your MinStack object will be instantiated and called as such: * MinStack obj = new MinStack(); * obj.push(x); * obj.pop(); * int param_3 = obj.top(); * int param_4 = obj.getMin(); */ 3. [Leetcode 946] 验证栈序列 （剑指Offer面试题31） 解法：双栈模拟，或者通过判断输入栈栈顶元素是否与验证栈当前指针所指元素相等。 代码： 123456789101112131415161718class Solution &#123;public: bool validateStackSequences(vector&lt;int&gt;&amp; pushed, vector&lt;int&gt;&amp; popped) &#123; stack &lt;int&gt;s; int count=0; for(int i=0;i&lt;pushed.size();i++)&#123; s.push(pushed[i]); while(!s.empty()&amp;&amp;s.top()==popped[count])&#123; s.pop(); count++; &#125; &#125; if(!s.empty()) return false; else return true; &#125;&#125;;]]></content>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[刷题]二叉树专题]]></title>
    <url>%2Fpost%2F76e180f5.html</url>
    <content type="text"><![CDATA[参考 几道和「二叉树」有关的算法面试题 1. [Leetcode 105] 从前序与中序遍历序列构造二叉树 (剑指OFFER面试题 7) 解法：递归，由先序序列确定子树根节点，由中序序列确定当前节点下的子树范围。 主要参数(递归函数)：先序序列的起点idx与终点idx， 中序序列的起点idx与终点idx。 递归停止条件： 先序序列或中序序列的起点idx &gt; 终点idx。 递归参数更新方法： 左子树更新: 先序起点idx+1。 preL + 1 先序终点为中序确定左子树节点数量num + 先序起点idx。 preL + num 中序起点为之前递归层中的中序起点。 inL 中序终点为根节点在中序序列中的idx - 1。 inRoot - 1 右子树更新: 先序起点idx+1 + 中序确定左子树节点数量num。 preL + num + 1 先序终点为之前递归层中的先序终点。 preR 中序起点为根节点在中序序列中的idx + 1。 inRoot + 1 中序终点为为之前递归层中的中序终点。 inR 1234567891011121314151617181920212223242526272829303132333435363738394041/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: TreeNode* buildTree(vector&lt;int&gt;&amp; preorder, vector&lt;int&gt;&amp; inorder) &#123; if(preorder.empty()|| inorder.empty()) return nullptr; return TreeIterate(0, preorder.size() - 1, 0, inorder.size() - 1, preorder, inorder); &#125; TreeNode* TreeIterate(int preL, int preR, int inL, int inR, const vector&lt;int&gt;&amp; preorder, const vector&lt;int&gt;&amp; inorder)&#123; if(preL &gt; preR || inL &gt; inR) return nullptr; int inRoot = 0; //中序遍历根节点位置 // vector&lt;int&gt;::iterator inRootIter =find(inorder.begin(), inorder.end(), preorder[preL]); // int inRoot std::distance(std::begin(inorder), inRootIter); for(int i = 0; i &lt; inorder.size(); i++) &#123; if(preorder[preL] == inorder[i])&#123; inRoot = i; break; &#125; &#125; int num = inRoot - inL; //中序确定左子树节点数量 TreeNode* currNode = new TreeNode(preorder[preL]); currNode -&gt; left = TreeIterate(preL + 1, preL + num, inL, inRoot - 1 , preorder, inorder); currNode -&gt; right = TreeIterate(preL + num + 1, preR, inRoot + 1, inR, preorder, inorder); return currNode; &#125;&#125;; 2. [Leetcode 572] 另一个树的子树 (剑指Offer面试题 26) 解法：递归寻根相同，根相同后判断子树是否相同。 注意：停止条件，空指针处理 代码： 1234567891011121314151617181920class Solution &#123;public: bool isSame(struct TreeNode* s, struct TreeNode* t) &#123; if(!s &amp;&amp; !t) // 两个子树叶子都是空 return true; if(!s || !t) // 有一个不是空 return false; if(s-&gt;val != t-&gt;val) return false; return isSame(s-&gt;left,t-&gt;left) &amp;&amp; isSame(s-&gt;right,t-&gt;right); &#125; bool isSubtree(struct TreeNode* s, struct TreeNode* t) &#123; if(!s) // 主树为空 return false; if((s-&gt;val == t-&gt;val) &amp;&amp; isSame(s,t)) return true; return isSubtree(s-&gt;left,t) || isSubtree(s-&gt;right,t); &#125;&#125;; 3. [LeetCode 144] 二叉树的前序遍历 解法： 递归，先输出再递归 循环，用栈模拟，节点右子节点入栈，左节点直接输出。 代码： 1234567891011121314151617181920212223242526272829303132333435363738394041/////====递归解class Solution &#123;public: vector&lt;int&gt; preorderTraversal(TreeNode* root) &#123; vector&lt;int&gt; res; if(root == nullptr) return &#123;&#125;; Recurrence(res, root); return res; &#125; void Recurrence(vector&lt;int&gt; &amp;res, TreeNode* root)&#123; if(root == nullptr) return; res.emplace_back(root-&gt;val); Recurence(res, root-&gt;left); Recurence(res, root-&gt;right); &#125;&#125;;////====循环解 前序：根左右，stack(根 右 左)class Solution &#123;public: vector&lt;int&gt; preorderTraversal(TreeNode* root) &#123; if(!root) return &#123;&#125;; vector&lt;int&gt; res; stack&lt;TreeNode* &gt; transStack; transStack.push(root); while(!transStack.empty())&#123; TreeNode* curr = transStack.top(); transStack.pop(); res.emplace_back(curr-&gt;val); if(curr-&gt;right) transStack.push(curr-&gt;right); if(curr-&gt;left) transStack.push(curr-&gt;left); &#125; return res; &#125;&#125;; 4. [LeetCode 94] 二叉树的中序遍历 解法： 递归：遍历左节点后输出值 循环： 代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445class Solution &#123;public: vector&lt;int&gt; inorderTraversal(TreeNode* root) &#123; vector&lt;int&gt; res; if(root == nullptr) return &#123;&#125;; Recurrence(res, root); return res; &#125; void Recurrence(vector&lt;int&gt;&amp; res, TreeNode* root)&#123; if(root == nullptr)&#123; return; &#125; Recurrence(res, root-&gt;left); res.emplace_back(root-&gt;val); Recurrence(res, root-&gt;right); &#125;&#125;;//循环解(当前节点不是空就一直往栈里push左孩子，如果当前节点为空，则出栈，当前节点设为出栈节点的右节点)class Solution &#123;public: vector&lt;int&gt; inorderTraversal(TreeNode* root) &#123; if(root == nullptr) return &#123;&#125;; vector&lt;int&gt; res; stack&lt;TreeNode* &gt; transStack; TreeNode* curr = root; while(curr||!transStack.empty())&#123; if (curr) &#123; transStack.push(curr); curr = curr-&gt;left; &#125; else &#123; curr = transStack.top(); transStack.pop(); res.emplace_back(curr-&gt;val); curr = curr-&gt;right; &#125; &#125; return res; &#125;&#125;; 5. [Leetcode 145] 二叉树的后序遍历 解法：递归/循环，循环解，先左插，再右插，遇到空节点出栈，出栈元素插在结果数组的最前面。 代码: 123456789101112131415161718192021222324252627282930313233343536373839404142434445// 数组class Solution &#123;public: vector&lt;int&gt; postorderTraversal(TreeNode* root) &#123; if(!root) return &#123;&#125;; vector&lt;int&gt; res; stack&lt;TreeNode*&gt; transStack; TreeNode* curr = root; transStack.push(root); while(!transStack.empty())&#123; curr = transStack.top(); transStack.pop(); if(curr-&gt;left) transStack.push(curr-&gt;left); if(curr-&gt;right) transStack.push(curr-&gt;right); res.insert(res.begin(), curr-&gt;val); &#125; return res; &#125;&#125;;// 链表转换 （Leetcode上的时间表现一样）class Solution &#123;public: vector&lt;int&gt; postorderTraversal(TreeNode* root) &#123; if(!root) return &#123;&#125;; list&lt;int &gt; resList; stack&lt;TreeNode*&gt; transStack; TreeNode* curr = root; transStack.push(root); while(!transStack.empty())&#123; curr = transStack.top(); transStack.pop(); if(curr-&gt;left) transStack.push(curr-&gt;left); if(curr-&gt;right) transStack.push(curr-&gt;right); resList.insert(resList.begin(), curr-&gt;val); &#125; vector&lt;int&gt; res(resList.begin(), resList.end()); return res; &#125;&#125;; 5. [LeetCode 226] 翻转二叉树 (剑指Offer面试题27) 解法：递归换序 代码： 1234567891011121314class Solution &#123;public: TreeNode* invertTree(TreeNode* root) &#123; if(root == nullptr) return nullptr; TreeNode* left = invertTree(root-&gt;left); TreeNode* right = invertTree(root-&gt;right); root-&gt;right = left; root-&gt;left = right; return root; &#125; &#125;; 6.[LeetCode 101] 对称二叉树 (剑指Offer面试题28) 解法1： 递归， 123456789101112131415class Solution &#123;public: bool isSymmetric(TreeNode* root) &#123; return isSymmetrical(root, root); &#125; bool isSymmetrical(TreeNode* pRoot1, TreeNode* pRoot2)&#123; if(pRoot1==nullptr &amp;&amp; pRoot2==nullptr) return true; if(pRoot1==nullptr || pRoot2 == nullptr) return false; if(pRoot1-&gt;val != pRoot2-&gt;val) return false; return isSymmetrical(pRoot1-&gt;left, pRoot2-&gt;right) &amp;&amp; isSymmetrical(pRoot1-&gt;right, pRoot2-&gt;left); &#125; &#125;; 解法2：BFS循环 1234567891011121314151617181920class Solution &#123;public: bool isSymmetric(TreeNode* root) &#123; if(!root) return true; stack&lt;TreeNode*&gt; Q; Q.push(root-&gt;left); Q.push(root-&gt;right); while(!Q.empty())&#123; TreeNode* r1=Q.top();Q.pop(); TreeNode* r2=Q.top();Q.pop(); if(!r1 &amp;&amp; !r2) continue; if(r1 &amp;&amp; r2)&#123; if(r1-&gt;val != r2-&gt;val) return false; Q.push(r1-&gt;left);Q.push(r2-&gt;right); Q.push(r1-&gt;right);Q.push(r2-&gt;left); &#125;else return false; &#125; return true; &#125;&#125;; 7. [Leetcode 102] 二叉树的层次遍历 (剑指Offer面试题32) 思路：用一个队列nodeQ维护二叉树BFS顺序，由于题目中需要分层记录，所以需要维护每一层的数据vectorlayerRes以及每一层可扩展节点队列layerQ。 代码： 1234567891011121314151617181920212223242526class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; levelOrder(TreeNode* root) &#123; if(root == nullptr ) return &#123;&#125;; queue&lt;TreeNode* &gt; nodeQ; vector&lt;vector&lt;int&gt;&gt; res; nodeQ.push(root); while(!nodeQ.empty())&#123; vector&lt;int&gt; layerRes; int qSize = nodeQ.size(); while(qSize--)&#123; TreeNode* currNode = nodeQ.front(); nodeQ.pop(); layerRes.emplace_back(currNode-&gt;val); if(currNode-&gt;left) nodeQ.push(currNode-&gt;left); if(currNode-&gt;right) nodeQ.push(currNode-&gt;right); &#125; res.emplace_back(layerRes); &#125; return res; &#125;&#125;; 8. [Leetcode 230] 二叉搜索树中第K小的元素 解法: 循环模拟中序遍历 代码: 12345678910111213141516171819class Solution &#123;public: int kthSmallest(TreeNode* root, int k) &#123; stack&lt;TreeNode*&gt; transStack; while(1)&#123; if(root != NULL)&#123; transStack.push(root); root = root-&gt;left; &#125; else &#123; if(--k == 0)&#123; return transStack.top()-&gt;val; &#125; else &#123; root = (transStack.top())-&gt;right; transStack.pop(); &#125; &#125; &#125; &#125;&#125;;]]></content>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[干货收集]]></title>
    <url>%2Fpost%2Fb8a74939.html</url>
    <content type="text"><![CDATA[2017~2019年度我在Github上面收藏的一些优质干货Repo其中有一些很荣幸作为Repo的贡献者，在issue中也结识了不少好友，共同学习 计算机软件工程类 免费的计算机编程类中文书籍 (44638 star) 设计模式-包教不包会 IntelliJ IDEA 简体中文专题教程 成为专业程序员路上用到的各种优秀资料、神器及框架 scala、spark使用过程中，各种测试用例以及相关资料整理 刷题类 Leetcode刷题(正在完善，可贡献) algorithm 深度学习类 deeplearningbook-chinese 随着整理，持续更新]]></content>
      <tags>
        <tag>干货分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[强化学习笔记专题(二)]Nature DQN]]></title>
    <url>%2Fpost%2F22518.html</url>
    <content type="text"><![CDATA[DQN (Nature) 一、 算法流程： 定义可配置参数 episode 数量 M 最大仿真时间 T，$\epsilon-greedy$参数$\epsilon_{low}$,$\epsilon_{high}$ batch size $N​$ 折扣率 $\gamma$，学习率 $\alpha$等优化器参数 Soft update 频率 $C​$ 初始化 初始化 replay buffer 大小N 初始化 Q 网络 $Q​$ ，使用随机权值 $\theta​$ 初始化 TargetQ 网络 $\hat{Q}$ 权值 $\theta^-$，使用 Q 网络的权值 $\theta$ DQN 一个Episode的流程 使用 $\epsilon-greedy$ 策略 选择一个 action $a_t$ 执行当前 action $a_t$， 获取下一个状态 $s_{t+1}$ 和 reward $r_{t}$ 将当前状态$s_t$赋值为下一个状态 $s_{t+1}$ 将五元组$\langle s_t,a_t,r_t,s_{t+1},done \rangle $存入 replay buffer $D$ 训练Q网络$Q​$: [Pre-condition]训练网络的前提是 replay buffer 的已有五元组数量大于 batch size $N$ 从 replay buffer $D​$中随机选取 batch size $N​$条数据$\langle s_j,a_j,r_j,s_{j+1},done\rangle​$ $D_{selected}​$ 计算目标Q值$y​$， $y​$是一个向量，$\{y_j \in y |j\in[0,N]\} ​$，大小为 batch size $N​$ 当 $D_{selected}​$[j] 中 $done=True​$ 时，即终局状态，此时 $y_j=r_j​$ 当 $D_{selected}$[j] 中 $done=False$ 时，即非终局状态，此时$y_i=r_j+\gamma max_{a’}\hat{Q}(s_{j+1},a’;\theta^-)$， 注意这里是用的 TargetQ 网络进行的 使用优化器进行梯度下降，损失函数是(一个batch里面) $(y-Q(s,a;\theta))^2​$，注意这里使用的是Q网络进行，来让计算出来的目标Q值与当前Q网络输出的Q值进行MSE 每 $C$ 次 episode，soft update 一次 target net 参数，$\theta^- = \theta$ 不断迭代Episode流程$M$次 二、对应代码完整代码地址： Nature DQN 初始化 初始化 replay buffer 大小N 初始化 Q 网络 $Q$ ，使用随机权值$\theta$ 初始化 TargetQ 网络 $\hat{Q}$ 权值 $\theta^-$，使用 Q 网络的权值 $\theta$ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354 def create_Q_network(self): """ Q net 网络定义 :return: """ # 输入状态 placeholder self.state_input = tf.placeholder("float", [None, self.state_dim]) # Q 网络结构 两层全连接 with tf.variable_scope('current_net'): W1 = self.weight_variable([self.state_dim, 100]) b1 = self.bias_variable([100]) W2 = self.weight_variable([100, self.action_dim]) b2 = self.bias_variable([self.action_dim]) h_layer = tf.nn.relu(tf.matmul(self.state_input, W1) + b1) # Q Value self.Q_value = tf.matmul(h_layer, W2) + b2 # Target Net 结构与 Q相同，可以用tf的reuse实现 with tf.variable_scope('target_net'): W1t = self.weight_variable([self.state_dim, 100]) b1t = self.bias_variable([100]) W2t = self.weight_variable([100, self.action_dim]) b2t = self.bias_variable([self.action_dim]) h_layer_t = tf.nn.relu(tf.matmul(self.state_input, W1t) + b1t) # target Q Value self.target_Q_value = tf.matmul(h_layer_t, W2t) + b2t t_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='target_net') e_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='current_net') # soft update 更新 target net with tf.variable_scope('soft_replacement'): self.target_replace_op = [tf.assign(t, e) for t, e in zip(t_params, e_params)]#===============================================================# def weight_variable(self, shape): """ 初始化网络权值(随机, truncated_normal) :param shape: :return: """ initial = tf.truncated_normal(shape) return tf.Variable(initial)#===============================================================# def bias_variable(self, shape): """ 初始化bias(const) :param shape: :return: """ initial = tf.constant(0.01, shape=shape) return tf.Variable(initial) $\epsilon-greedy$ 策略 定义，这里对$\epsilon$进行一个随时间步的迁移而减小的策略，使其动作选择的不确定性逐渐减小。 123456789101112131415def egreedy_action(self, state): """ epsilon-greedy策略 :param state: :return: """ Q_value = self.Q_value.eval(feed_dict=&#123; self.state_input: [state] &#125;)[0] if random.random() &lt;= self.epsilon: self.epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / 10000 return random.randint(0, self.action_dim - 1) else: self.epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / 10000 return np.argmax(Q_value) Replay buffer的定义 1234567891011121314151617181920212223242526def perceive(self, state, action, reward, next_state, done): """ Replay buffer :param state: :param action: :param reward: :param next_state: :param done: :return: """ # 对action 进行one-hot存储，方便网络进行处理 # [0,0,0,0,1,0,0,0,0] action=5 one_hot_action = np.zeros(self.action_dim) one_hot_action[action] = 1 # 存入replay_buffer # self.replay_buffer = deque() self.replay_buffer.append((state, one_hot_action, reward, next_state, done)) # 溢出出队 if len(self.replay_buffer) &gt; REPLAY_SIZE: self.replay_buffer.popleft() # 可进行训练条件 if len(self.replay_buffer) &gt; BATCH_SIZE: self.train_Q_network() Q网络训练 12345678910111213141516171819202122232425262728def train_Q_network(self): """ Q网络训练 :return: """ self.time_step += 1 # 从 replay buffer D中随机选取 batch size N条数据&lt;s_j,a_j,r_j,s_j+1,done&gt; D_selected minibatch = random.sample(self.replay_buffer, BATCH_SIZE) state_batch = [data[0] for data in minibatch] action_batch = [data[1] for data in minibatch] reward_batch = [data[2] for data in minibatch] next_state_batch = [data[3] for data in minibatch] # 计算目标Q值y y_batch = [] Q_value_batch = self.target_Q_value.eval(feed_dict=&#123;self.state_input: next_state_batch&#125;) for i in range(0, BATCH_SIZE): done = minibatch[i][4] if done: y_batch.append(reward_batch[i]) else: y_batch.append(reward_batch[i] + GAMMA * np.max(Q_value_batch[i])) self.optimizer.run(feed_dict=&#123; self.y_input: y_batch, self.action_input: action_batch, self.state_input: state_batch &#125;) Soft update 123def update_target_q_network(self, episode): # 更新 target Q netowrk if episode % REPLACE_TARGET_FREQ == 0: 三、实验结果环境 cart-pole-v0 (期望回报是200) 四、DQN参考论文流程： 五、Double DQNDQN存在的问题是Q function容易过拟合，根据状态 $s_{t+1}$ 选择动作 $a_{t+1}$ 的过程,以及估计 $Q(s_{t+1},a_{t+1})​$ 使用的同一个Q net网络参数，这可能导致选择过高的估计值，从而导致过于乐观的值估计。为了避免这种情况的出现，可以对选择和衡量进行解耦，从而就有了使用 Double DQN 来解决这一问题。 Double DQN与DQN的区别仅在于$y$的求解方式不同，Double DQN根据Q网络参数来选择动作$a_{t+1}$,再用Target Q网络参数来衡量$Q(s_{t+1},a_{t+1})$的值。 Y_t^{DQN}=R_{t+1}+\gamma \hat{Q}(S_{t+1},argmax_a\hat{Q}(S_{t+1},a;\theta_t^-);\theta_t^-)\\ Y_t^{DDQN}=R_{t+1}+\gamma \hat{Q}(S_{t+1},argmax_aQ(S_{t+1},a;\theta_t);\theta_t^-)反映在代码上，就是训练的时候选择Q的时候有点变动： 123456789101112131415161718192021222324252627282930313233343536def train_Q_network(self): """ Q网络训练 :return: """ self.time_step += 1 # 从 replay buffer D中随机选取 batch size N条数据&lt;s_j,a_j,r_j,s_j+1,done&gt;$ D_selected minibatch = random.sample(self.replay_buffer, BATCH_SIZE) state_batch = [data[0] for data in minibatch] action_batch = [data[1] for data in minibatch] reward_batch = [data[2] for data in minibatch] next_state_batch = [data[3] for data in minibatch] # 计算目标Q值y y_batch = [] QTarget_value_batch = self.target_Q_value.eval(feed_dict=&#123;self.state_input: next_state_batch&#125;) Q_value_batch = self.Q_value.eval(feed_dict=&#123;self.state_input: next_state_batch&#125;) for i in range(0, BATCH_SIZE): done = minibatch[i][4] if done: y_batch.append(reward_batch[i]) else: #################用target Q(Q)####################### if DOUBLE_DQN: selected_q_next = QTarget_value_batch[i][np.argmax(Q_value_batch[i])] #################用target Q(target Q)################ else: selected_q_next = np.max(QTarget_value_batch[i]) y_batch.append(reward_batch[i] + GAMMA * selected_q_next) self.optimizer.run(feed_dict=&#123; self.y_input: y_batch, self.action_input: action_batch, self.state_input: state_batch &#125;) 六、DQN，DDQN实验结果对比可以看到DoubleDQN的表现比 DQN稳定 七、 Dueling DDQN八、 Dueling DQN, DDQN, DQN对比]]></content>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[强化学习论文] (HDQN) Integrating Temporal Abstraction and Intrinsic Motivation]]></title>
    <url>%2Fpost%2F10845.html</url>
    <content type="text"><![CDATA[论文 题目: Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation 作者: Tejas D. Kulkarni, Karthik R. Narasimhan, Ardavan Saeedi, Joshua B. Tenenbaum 论文: https://arxiv.org/abs/1604.06057 年份: 2016 参考: https://github.com/aleju/papers/blob/master/neural-nets/Hierarchical_Deep_Reinforcement_Learning.md 总结1.主要贡献 提出了一种分层强化学习方法 该方法使用了长期目标(long-term goal)指导短期动作(short-term choice)的选择 2.主要方法 两个重要组件 Meta-controller 负责生成长期目标long term goal; 通过训练Meta-controller使其能够根据当前state来选取目标goal，使得extrinsic reward最大; 当且仅当底层执行器Controller完成一个episode或者达成某个Meta-controller产生的goal的时候，Meta-controller再去产生新的目标goal。 Controller 从环境中获取当前state，并从Meta-controller中获取当前目标goal; 基于当前goal和当前的state，来选取最大化intrinsic reward期望的action，这里与传统的rf相同只是增加了目标goal，这里通过估计action-value function ( $Q_1(s_t, a_t;\theta_1,g_t)$ )来做; Reward 是 intrinsic的，在agent内部，这个intrinsic reward由Critic网络产生，当且仅当当前的目标达到时，才会产生intrinsic reward。 在蒙特祖玛的复仇(Montezuma’s Revenge)上实验 目标Goal是一些手动设置的特定游戏object，比如钥匙，在实验中，将设置一个与游戏屏幕大小相等的遮罩层，当且仅当目标object的位置上的二进制位是1，其他像素上的二进制值为0； Meta-controller通过Q function选择要到达的下一个Goal; Controller根据Q function选择能够到达Goal的action, 其不断迭代选择action，直到其完成一个episode或到达Goal； 每当达到目标Goal时，Critic都会向Controller提供内在奖励(intrinsic reward); CNN用于Meta-controller和Controller，在架构上类似于Atari-DQN论文(shallow CNNs); 使用两个Replay buffer，一个用于Meta-controller(大小为40k)，一个用于Controller(大小为1M); Meta-controller和Controller两者都遵循epsilon-greedy。Epsilon从1.0开始，减小至0.1; 折扣因子γ为0.9; 使用SGD优化。]]></content>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[强化学习专题笔记(一) 强化学习基础]]></title>
    <url>%2Fpost%2F17172.html</url>
    <content type="text"><![CDATA[一、长期回报对于问题的简化，采用理想的MDP，简化问题到具有马尔科夫性，对于马尔科夫决策过程而言，在理想状态下，每一个行动都要为最终的目标最大化长期回报 而努力。 \max\sum_{t}{r_t}但是很多情况下，仿真的时间维度较大，步数较多，甚至可以无限循环下去，这样的情况下我们需要引入一个可以和收敛的无穷数列，来替代我们最原始的长期回报公式。即对未来的回报乘以一个折扣率，使得长期回报变得更有意义： \sum_{t=0}{\gamma^tr_t} （\gamma < 1）由此我们引出长期回报的概念，即从当前状态开始对之后的所有回报，运用上式进行累加的折扣率计算： Ret_t=\sum_{k=0}\gamma^kr_{t+k+1}但是长期回报需要知道未来的行动情况，我们需要对上式进行一个合理的估计，因而我们定义了策略的价值。 二、值函数由于环境的原因，MDP中的状态转移概率有时候并不能够确定，因而需要基于状态转移来估计长期回报的期望。τ是从某个状态s出发，根据策略与状态转移概率采样得到的序列(trajectory)。那么价值函数可以表示为： v_{\pi}{(s_t)} = \mathbb{E}_{s,a\simτ}[\sum_{k=0}\gamma^kr_{t+k+1}] =\sum_{\tau}{p(\tau)}{\sum_{k=0}^{\infin}{\gamma^k}{r_{t+k+1}}}根据MDP模型的形式，值函数一般分为两种： 状态值函数 $v_{\pi}{(s)}​$: 已知当前状态s，按照某种策略行动产生的长期回报期望； 状态-动作值函数 $q_{\pi}{(s,a)}​$: 已知当前状态s及动作a，按照某种策略行动产生的长期回报期望。 由于符合马尔科夫性，我们可以将值函数的形式进行马尔科夫展开,其中${\pi(a_t|s_t)}$表示，在$s_t$状态下选择策略$\pi$的概率，策略$\pi$将产生行动$a_t$，$p(s_{t+1}|s_t,a_t)$表示在策略$\pi$的情况下，从$s_t，a_t$到达$s_{t+1}$的概率。 v_{\pi}{(s_t)}=\sum_{(s_t,a_t,...)\sim\tau}{\pi(a_t|s_t)}p(s_{t+1}|s_t,a_t)...{\sum_{k=0}^{\infin}{\gamma^k}{r_{t+k+1}}}三、贝尔曼方程 状态值函数的贝尔曼方程 通过代换消元，可以将上式整理为状态值函数的贝尔曼方程： v_{\pi}(s_t)=\sum_{a_t}\pi(a_t|s_t)\sum_{s_{t+1}}{p(s_{t+1}|s_t,a_t)[r_{t+1}+\gamma v_{\pi}(s_{t+1})]}更直观一点可以将贝尔曼方程描述为一种DP的形式，即当前状态$s$下，选择策略$\pi$的长期回报期望。 v_{\pi}(s_t)=\sum_{a_t,s_t+1}\pi{(a_t|s_t)}\mathbb{E}[r_{t+1} + \gamma v_{\pi}(s_{t+1})]按Sutton的书表示： v_π(s)=\mathbb{E}_π[r_{t+1}+γv_π(s_{t+1})|s_t=s] 状态-动作值函数的贝尔曼方程 类似地，可以定义状态-动作值函数的贝尔曼方程： q_{\pi}(s_t,a_t)=\sum_{s_{t+1}}p(s_{t+1}|s_t,a_t)\sum_{a_{t+1}}p(a_{t+1}|s_{t+1})[r_{t+1}+\gamma q_\pi{(s_{t+1},a_{t+1})}]按Sutton的书表示: q_\pi(s, a)=\mathbb{E}[r_{t+1} +\gamma q_{\pi}(s_{t+1}, a_{t+1})|s_{t}=s,a_{t}=a] Bellman optimality equation v_*(s)=\mathbb{E}[r_{t+1} +\gamma max_\pi{v(s_{t+1})|s_t=s}] q_*(s,a)=\mathbb{E}[r_{t+1} +\gamma max_{a_{t+1}}{q(s_{t+1},a_{t+1})|s_t=s,a_t=a}]四、Monte-Carlo与Time Difference MC 方差较大，需要较深的探索获取回报 TD 方差较小，偏差较大，可设定探索深度(1-step, n-step), Q-Learning, SARSA都属于TD 【参考】https://zhuanlan.zhihu.com/p/25239682 Monte-Carlo method适用于“情节式任务”（情节任务序列有终点，与“情节式任务”相对应的是“连续型任务”）。$Q(s,a)$就是整个序列的期望回报。MC增量更新中的Monte-Carlo error ($R-Q(s_t,a_t)$)： Q(s_t,a_t)\Leftarrow Q(s_t,a_t) + \alpha(R-Q(s_t,a_t))TD（Time Difference） method，是MC和DP 方法的一个结合。相比MC方法，TD除了能够适用于连续任务外，和MC的差异从下图可以清楚看到。MC需要回退整个序列更新Q值，而TD只需要回退1步或n步更新Q值。因为MC需要等待序列结束才能训练，而TD没有这个限制，因此TD收敛速度明显比MC快，目前的主要算法都是基于TD。下图是TD和MC的回退图，很显然MC回退的更深。 1-step TD error: ($r_{t+1}+\gamma Q(s_{t+1}a_{t+1}-Q(s_t,a_t)​$)： Q(s_t,a_t) \Leftarrow Q(s_t,a_t)+\alpha(r_{t+1}+\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t))n-steps TD error： TD(λ) error: 事实上，MC error可以视为一个情节任务的max-step TD error。另外，一般来说，在TD error中，n越大，用到的真实回报信息更多，收敛也会越快。 五、推荐文章[强化学习（四）用蒙特卡罗法（MC）求解] 强化学习（五）用时序差分法（TD）求解]]></content>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MPC控制笔记(一)]]></title>
    <url>%2Fpost%2F10696.html</url>
    <content type="text"><![CDATA[笔记参考1：Understanding Model Predictive Control(Youtube 带自动生成字幕)笔记参考2：Understanding Model Predictive Control(B站 生肉) 一、什么是MPC模型预测控制MPC(Model Predict Control)是一种反馈控制(feedback control)算法, 使用模型来预测过程的未来输出。 举例： [场景] 车道保持 [已知模型] 车辆模型，比如速度控制， 转向控制对应的偏航量 [预测] 根据已知模型和所选的控制策略(action)，进行轨迹预测 [优化] 通过优化控制策略，来尽可能的拟合预测的轨迹。 如下图所示为一个MIMO系统u1,u2输入与y1,y2输出相互影响。如果使用PID控制的话，每一个子系统单独设计一个PID控制器，两个相互影响的子系统没有任何的交联，使得系统难以设计，如果像图二一样设计一个较大的系统，则参数较多难以实现，而使用MPC控制器的话可以较好的解决两种问题，综合相互间的影响来设计参数。MPC控制 此外MPC控制还可以方便的支持各种约束条件，具备一定的预测能力(有点像前馈feedforward控制)。 二、MPC的特点综合上述，总结一下MPC的特点： 支持MIMO系统，便于构建输入输出间的相互影响参数 支持方便添加约束条件 具有预测能力、 需要较好的处理器和较大的内存，因为需要大量的在线优化，存储大量的变量 三、MPC参数选择选择一个好的参数不仅影响MPC控制的性能，而且还会影响到MPC每一个timestep内进行在线优化的计算复杂度。这里将会给出关于控制器采样周期、预测及控制范围(prediction and control)、约束及权重。 采样周期的选择采样周期过大，则系统反应过慢导致难以及时进行修正控制，而采样周期过小，则会导致系统产生大量的在线优化计算，给系统带来较大的开销。因而建议采样周期设计采用开环响应时间(10~90%上升时间)的十分之一或二十分之一： 预测范围(prediction horizon)的选择预测范围指的是一次优化后预测未来输出的时间步的个数。建议范围：在开环响应时间内采样20-30个样本的范围 控制范围(control horizon)的选择如下图 [k, k+m]范围为控制范围，之后的红色部分称为 held constant，其中控制范围是要通过优化器来进行优化的参数动作。 过小的控制范围，可能无法做到较好的控制，而较大的控制范围，比如与预测范围相等，则会导致只有前一部分的控制范围才会有较好的效果，而后一部分的控制范围则收效甚微，而且将带来大量的计算开销。 建议控制范围应该在预测范围的10~20%之间,最小值为2~3个timestep时间步 约束对于约束，一般分为Hard约束和Soft约束，Hard约束是不可违背必须遵守的，在控制系统中，输入输出都可能会有约束限制，但是在设计时不建议将输入输出都给予Hard约束，因为这两部的约束有可能是有重叠的，导致优化器会产生不可行解。建议输出采用较小容忍度的Soft约束，而输入的话建议输入和输入参数变化率二者之间不要同时为Hard约束，可以一个Hard一个Soft。 四、Linear MPC (Adaptive MPC 与 Gain-Scheduled MPC)Linear MPC适用于:对于非线性系统而言,需要在不同的operating point处进行线性化处理如下图。 Adaptive MPC在 Adaptive MPC中，当operating condition发生变化的时候，需要进行一次近似线性化计算，在每个时间步中，使用其近似线性模型来更新内部的平台模型(plant model，比如飞控模型，自行车模型等)。在 Adaptive MPC中，在不同的operating point条件下，其优化问题的结构保持不变，即状态数量，约束数量不会随着operating condition而改变。 Gain-Scheduled MPC在 Gain-Scheduled MPC中，在不同的operating point条件下，其优化问题的结构会发生变化，需要为每一个operating point构建一个MPC控制器，且相互之间独立，其状态数量约束数量也可能不同。在 Gain-Scheduled MPC模式下，需要设计调度算法来切换不同的MPC模型。 二者选型[Adaptive MPC] 当能够构建平台(如飞行器，自动车等)的runtime线性模型，且在不同的operating point下优化问题的结构不变。 [Gain-Scheduled MPC] 当能够构建平台(如飞行器，自动车等)的runtime线性模型，且在不同的operating point下优化问题的结构发生变化。 五、Non-Linear MPC (Adaptive MPC 与 Gain-Scheduled MPC)Non-Linear MPC适用于(相对强大，能够提供更准确的预测能力，与决策支持，但是非线性优化的计算开销较大)]]></content>
      <tags>
        <tag>MPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Generative Adversarial Imitation Learning(GAIL) 论文阅读笔记]]></title>
    <url>%2Fpost%2F21152.html</url>
    <content type="text"><![CDATA[GAIL优点相较于IRL，可以省略很多中间步骤，比如通过IRL来学习Reward系统，再通过Reward系统来进行RL学习policy，GAIL可以直接通过expert trajectory 来直接学习policy。 IRL假定cost function的集合为$C $, $\pi_E$为专家策略(一系列采集来的专家策略样本)。IRL的目标是maximum causal entropy IRL其中是策略π的γ-discounted causal entropy，对于每一个cost function $c \in C$都有对于专家策略的cost最小，而其他策略的cost都相对较大。式(1)中包含了一个RL过程，实现了cost function到可以最小化期望cost误差的高熵策略的映射：]]></content>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[无外网情况下RPM方式安装MySQL5.6]]></title>
    <url>%2Fpost%2F20206.html</url>
    <content type="text"><![CDATA[RPM方式安装MySQL5.6a. 检查MySQL及相关RPM包，是否安装，如果有安装，则移除（rpm –e 名称）123[root@localhost ~]# rpm -qa | grep -i mysqlmysql-libs-5.1.66-2.el6_3.x86_64[root@localhost ~]# yum -y remove mysql-libs* b. 下载Linux对应的RPM包，如：CentOS6.4_64对应的RPM包，如下：这里给出我下载好的包，官网下载巨慢。。有可能还需要一个perl库的依赖，这里一并给出。链接: http://pan.baidu.com/s/1skFrEK9 密码: apza12345[root@localhost rpm]# lltotal 74364-rw-r--r--. 1 root root 18442536 Dec 11 20:19 MySQL-client-5.6.15-1.el6.x86_64.rpm-rw-r--r--. 1 root root 3340660 Dec 11 20:06 MySQL-devel-5.6.15-1.el6.x86_64.rpm-rw-r--r--. 1 root root 54360600 Dec 11 20:03 MySQL-server-5.6.15-1.el6.x86_64.rpm c. 安装MySQL(如有必要需要安装perl-libs-5.10.1-141.el6_7.1.x86_64)12345[root@localhost rpm]# rpm -ivh MySQL-server-5.6.15-1.el6.x86_64.rpm[root@localhost rpm]# rpm -ivh MySQL-devel-5.6.15-1.el6.x86_64.rpm[root@localhost rpm]# rpm -ivh MySQL-client-5.6.15-1.el6.x86_64.rpm#修改配置文件位置[root@localhost rpm]# cp /usr/share/mysql/my-default.cnf /etc/my.cnf d. 在my.cnf文件中的[mysqld]下设置这一行：datadir = /usr/local/mysql/var e. 初始化MySQL及设置密码12345678[root@localhost rpm]# /usr/bin/mysql_install_db[root@localhost rpm]# service mysql start[root@localhost rpm]# cat /root/.mysql_secret #查看root账号密码,若无此文件可以直接使用无密码登录，若无密码登录失败，则需要在my.cnf文件中加入skip-grant-tables，并重启mysql服务# The random password set for the root user at Wed Dec 11 23:32:50 2013 (local time): qKTaFZnl[root@localhost ~]# mysql -uroot –pqKTaFZnlmysql&gt; SET PASSWORD = PASSWORD('123456'); #设置密码为123456mysql&gt; exit[root@localhost ~]# mysql -uroot -p123456 f. 允许远程登陆1234567891011121314mysql&gt; use mysql;mysql&gt; select host,user,password from user;+-----------------------+------+-------------------------------------------+| host | user | password |+-----------------------+------+-------------------------------------------+| localhost | root | *6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9 || localhost.localdomain | root | *1237E2CE819C427B0D8174456DD83C47480D37E8 || 127.0.0.1 | root | *1237E2CE819C427B0D8174456DD83C47480D37E8 || ::1 | root | *1237E2CE819C427B0D8174456DD83C47480D37E8 |+-----------------------+------+-------------------------------------------+mysql&gt; update user set password=password('123456') where user='root';mysql&gt; update user set host='%' where user='root' and host='localhost';mysql&gt; flush privileges;mysql&gt; exit g. 设置开机自启动123[root@localhost ~]# chkconfig mysql on[root@localhost ~]# chkconfig --list | grep mysqlmysql 0:off 1:off 2:on 3:on 4:on 5:on 6:off h.MySQL的默认安装位置1234/var/lib/mysql/ #数据库目录/usr/share/mysql #配置文件目录/usr/bin #相关命令目录/etc/init.d/mysql #启动脚本 i.修改字符集和数据存储路径配置/etc/my.cnf文件,修改数据存放路径、mysql.sock路径以及默认编码utf-8.123456789101112131415[client]password = 123456port = 3306default-character-set=utf8[mysqld]port = 3306character_set_server=utf8character_set_client=utf8collation-server=utf8_general_ci#(注意linux下mysql安装完后是默认：表名区分大小写，列名不区分大小写； 0：区分大小写，1：不区分大小写)lower_case_table_names=1#(设置最大连接数，默认为 151，MySQL服务器允许的最大连接数16384; )max_connections=1000[mysql]default-character-set = utf8 j. 查看字符集12show variables like '%collation%';show variables like '%char%';]]></content>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker给运行中的容器添加映射端口]]></title>
    <url>%2Fpost%2F46919.html</url>
    <content type="text"><![CDATA[声明​ 这篇文章选自[教程技巧] DOCKER 给运行中的容器添加映射端口 正文Docker 给运行中的容器添加映射端口方法1 获得容器IP 1$ docker inspect `container_name` | grep IPAddress 比如我的容器叫mysqlserver么就输入下列代码来获取该容器的ip地址 1$ docker inspect mysqlserver | grep IPAddress 执行完之后会发现我的mysqlserverdocker容器的ip地址为192.168.0.2 ​ iptables转发端口 比如我将容器的3306端口映射到主机的37221端口，那么ip对应就写入我的docker容器IP即可 1iptables -t nat -A DOCKER -p tcp --dport 37221 -j DNAT --to-destination 192.168.0.2:3306 Docker 给运行中的容器添加映射端口方法2 提交一个运行中的容器为镜像 1$ docker commit containerid foo/live 运行镜像并添加端口 1$ docker run -d -p 8000:80 foo/live /bin/bash ​]]></content>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Scala基于词法单元的解析器定制EBNF范式文法解析]]></title>
    <url>%2Fpost%2F33864.html</url>
    <content type="text"><![CDATA[前言近期在做Oracle迁移到Spark平台的项目上遇到了一些平台公式翻译为SparkSQL(on Hive)的需求，而Spark采用亲妈语言Scala进行开发。分析过大概需求过后，拟使用编译原理中的EBNF范式模式，进行基于词法的文法解析。于是拟采用传统的正则词法解析到EBNF文法解析的套路来实现,直到发现了StandardTokenParsers这个Scala基于词法单元的解析器类。 平台公式及翻译后的SparkSQL平台公式的样子如下所示：1if (XX1_m001[D003]="邢おb7肮α䵵薇" || XX1_m001[H003]&lt;"2") &amp;&amp; XX1_m001[D005]!="wed" then XX1_m001[H022,COUNT] 这里面字段值”邢おb7肮α䵵薇”为这个的目的是为了测试各种字符集是否都能匹配满足。那么对应的SparkSQL应该是这个样子的,由于是使用的Hive on Spark，因而长得跟Oracle的SQL语句差不多：1SELECT COUNT(H022) FROM XX1_m001 WHERE (XX1_m001.D003='邢おb7肮α䵵薇' OR XX1_m001.H003&lt;'2') AND XX1_m001.D005&lt;'wed' 总体而言比较简单，因为我只是想在这里做一个Demo。 平台公式的EBNF范式及词法解析设计1234expr-condition ::= tableName "[" valueName "]" comparator Conditionexpr-front ::= expr-condition (("&amp;&amp;"|"||")expr-front)*expr-back ::= tableName "[" valueName "," operator "]"expr ::= "if" expr-front "then" expr-back 其中词法定义如下1234operator =&gt; [SUM,COUNT]tableName,valueName =&gt;ident #ident为关键字comparator =&gt; ["=","&gt;=","&lt;=","&gt;","&lt;","!="]Condition =&gt; stringLit #stringLit为字符串常量 使用Scala基于词法单元的解析器解析上述EBNF文法Scala基于词法单元的解析器是需要继承StandardTokenParsers这个类的，该类提供了很方便的解析函数，以及词法集合。我们可以通过使用lexical.delimiters列表来存放在文法翻译器执行过程中遇到的分隔符，使用lexical.reserved列表来存放执行过程中的关键字。比如，我们参照平台公式，看到&quot;=&quot;,&quot;&gt;=&quot;,&quot;&lt;=&quot;,&quot;&gt;&quot;,&quot;&lt;&quot;,&quot;!=&quot;,&quot;&amp;&amp;&quot;,&quot;||&quot;,&quot;[&quot;,&quot;]&quot;,&quot;,&quot;,&quot;(&quot;,&quot;)&quot;这些都是分隔符，其实我们也可以把&quot;=&quot;,&quot;&gt;=&quot;,&quot;&lt;=&quot;,&quot;&gt;&quot;,&quot;&lt;&quot;,&quot;!=&quot;,&quot;&amp;&amp;&quot;,&quot;||&quot;当做是关键字，但是我习惯上将带有英文字母的单词作为关键字处理。因而，这里的关键字集合便是&quot;if&quot;,&quot;then&quot;,&quot;SUM&quot;,&quot;COUNT&quot;这些。表现在代码中是酱紫的：12lexical.delimiters += ("=","&gt;=","&lt;=","&gt;","&lt;","!=","&amp;&amp;","||","[","]",",","(",")")lexical.reserved += ("if","then","SUM","COUNT") 是不是so easy~。我们再来看一下如何使用基于词法单元的解析器解析前面我们设计的EBNF文法呢。我在这里先上代码：12345678910111213141516171819202122232425262728293031323334353637383940class ExprParsre extends StandardTokenParsers&#123; lexical.delimiters += ("=","&gt;=","&lt;=","&gt;","&lt;","!=","&amp;&amp;","||","[","]",",","(",")") lexical.reserved += ("if","then","SUM","COUNT") def expr: Parser[String] = "if" ~ expr_front ~ "then" ~ expr_back ^^&#123; case "if" ~ exp1 ~ "then" ~ exp2 =&gt; exp2 + " WHERE " +exp1 &#125; def expr_priority: Parser[String] = opt("(") ~ expr_condition ~ opt(")") ^^&#123; case Some("(") ~ conditions ~ Some(")") =&gt; "(" + conditions +")" case Some("(") ~ conditions ~ None =&gt; "(" + conditions case None ~ conditions ~ Some(")") =&gt; conditions +")" case None ~ conditions ~ None =&gt; conditions &#125; def expr_condition: Parser[String] = ident ~ "[" ~ ident ~ "]" ~ ("="|"&gt;="|"&lt;="|"&gt;"|"&lt;"|"!=") ~ stringLit ^^&#123; case ident1~"["~ident2~"]"~"="~stringList =&gt; ident1 + "." + ident2 +"='" + stringList +"'" case ident1~"["~ident2~"]"~"&gt;="~stringList =&gt; ident1 + "." + ident2 +"&gt;='" + stringList +"'" case ident1~"["~ident2~"]"~"&lt;="~stringList =&gt; ident1 + "." + ident2 +"&lt;='" + stringList +"'" case ident1~"["~ident2~"]"~"&gt;"~stringList =&gt; ident1 + "." + ident2 +"&gt;'" + stringList +"'" case ident1~"["~ident2~"]"~"&lt;"~stringList =&gt; ident1 + "." + ident2 +"&lt;'" + stringList +"'" case ident1~"["~ident2~"]"~"!="~stringList =&gt; ident1 + "." + ident2 +"!='" + stringList +"'" &#125; def comparator: Parser[String] = ("&amp;&amp;"|"||") ^^&#123; case "&amp;&amp;" =&gt; " AND " case "||" =&gt; " OR " &#125; def expr_front: Parser[String] = expr_priority ~ rep(comparator ~ expr_priority) ^^&#123; case exp1 ~ exp2 =&gt; exp1 + exp2.map(x =&gt;&#123;x._1 + " " + x._2&#125;).mkString(" ") &#125; def expr_back: Parser[String] = ident ~ "[" ~ ident ~ "," ~ ("SUM"|"COUNT") ~ "]" ^^ &#123; case ident1~"["~ident2~","~"COUNT"~"]" =&gt; "SELECT COUNT("+ ident2.toString() +") FROM " + ident1.toString() case ident1~"["~ident2~","~"SUM"~"]" =&gt; "SELECT SUM("+ ident2.toString() +") FROM " + ident1.toString() &#125; def parserAll[T]( p : Parser[T], input :String) = &#123; phrase(p)( new lexical.Scanner(input)) &#125;&#125;]]></content>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux内存占用过高？非也]]></title>
    <url>%2Fpost%2F24621.html</url>
    <content type="text"><![CDATA[前言 今天在我的RPi2上测试GPIO程序，忽然发现机器超卡，重启之后依然如此。于是我top了一下发现了一个问题就是内存爆满！！可我还啥都没干呢这是咋了？于是我呵呵地开始查资料，终于找到了问题所在。 正文 先来在阿里的ECS上top一下感受内存爆满的感觉，终端输入top 1#top 结合操作系统，计组等课程，以及多年以来windows培养给我的直觉，认为0.96G（1016272K）的总内存，使用了0.84G(880960K)的内存，使用率高达88%。然而我还啥都没干，怎么会这样呢？仔细查看还会发现后面有一个buffers，Swap后面还有一个Cached Mem。现在我们用free来观察下 1#free -m 虽然Mem显示了0.9G左右的used，但是-/+ buffers/cache(减去buffers和cache的结果)可以看到，当前进程实际占用内存是0.24G(256348K)，而可用空闲（free）内存为0.72G(759924K)。可以这么理解：在linux的内存分配机制中，优先使用物理内存，当物理内存还有空闲时（还够用），不会释放其占用内存，就算占用内存的程序已经被关闭了，该程序所占用的内存用来做缓存使用，对于开启过的程序、或是读取刚存取过得数据会比较快。 如上面的例子：共1G的内存，0.9G被占用，但是buffer和cached mem部分作为缓存，可以使用命中率的方式提高使用效率，而且这部分缓存是根据指令随时可以释放的，我们可以认为这部分内存没有实际被使用，也可以认为它是空闲的。因此查看目前进程正在实际被使用的内存，是used-(buffers+cache)，也可以认为如果swap没有大量使用，mem还是够用的，只有mem被当前进程实际占用完（没有了buffers和cache），才会使用到swap的。 再举个栗子： 这个是我在RPi一群看到的一个群友发的探针监测截图 观察内存使用状况一栏，发现物理内存功925.89M，已用911.74M，Cache化的内存是676.46M，Buffers为61.3M，现在用上述公式： 1真实的内存使用=used-(buffers+cache) 带入： 12真实使用内存 = 911.74-676.46-61.3 = 173.98与第三行的灰条的173.98相符 总结 很高兴对于linux的内存分配有了新的认识 多谢一群的 粵-打雜小白-503 Service Unavailable 的技术支持 多谢Licess’s Blog的精彩分析]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在raspbian-jessie上搭建nat123自启动]]></title>
    <url>%2Fpost%2F34931.html</url>
    <content type="text"><![CDATA[前言什么是nat123? 现在我们的树莓派都是在路由器内网里面的，需要使用nat123来实现外网映射(类似花生壳)，但是nat123免费好用，这个原理在计算机网络中叫做隧道或者叫穿透。那么本文主要讲述如何在树莓派上配置nat123。 必要环境 ​ 现在本文所用的平台是树莓派2代b+，其实那个版本的RPi都OK，主要是raspbian-jessie的系统，不过貌似是raspbian的系统都适合使用，这里主要是nat123的环境必须配置好。 安装好mono环境 安装好nat123客户端 本文中我的nat123客户端安装在了官网所指示的/mnt 目录下，如图。 正文1.安装nat123客户端 首先在这里还是要给出官网所给的安装方法linux下安装nat123客户端，但是按照官网所述的方法，我总是卡在mono的安装过程上，如果哪位读者能够用那种方法配置成功请您在底下mark我一下~~。下面我说一下配置过程。 安装mono ​ 这里可能由于是我的软件源出了问题，无论如何也不能按照官网的方式在我的树莓派上装mono环境，那么我只好采取mono源码编译的方式来安装。 ​ 简单说一下每个步骤，首先通过wget获取mono源码，如果没有wget的请手动执行sudo apt-get install wget进行安装，然后使用tar解压，然后cd进入解压文件夹，然后使用./configure --prefix=/usr/local配置编译安装路径，最后make编译，make install安装mono。 123456$ wget http://download.mono-project.com/sources/mono/mono-4.0.1.44.tar.bz2$ tar -xvf mono-4.0.1.44.tar.bz2$ cd mono-4.0.1$ ./configure --prefix=/usr/local$ make$ make install ​ 这样执行完成后输入mono -V出现版本信息就OK了，可以按照官网的说明继续执行。那么在这里我继续总结一下nat123的全部安装步骤。&lt;/br&gt; 安装nat123客户端(本文安装路径为/mnt) 123$ cd /mnt $ wget http://www.nat123.com/down/nat123linux.tar.gz $ tar -zxvf nat123linux.tar.gz 客户端启动 1234$ sudo apt-get install screen $ cd /mnt $ screen -S nat123 $ sudo mono nat123linux.sh ​ 这里需要注意的是，在执行screen -S nat123 语句时-S是大写的S，如提示没有screen，则执行sudo apt-get install screen安装screen后再执行screen。然后就是要特别注意mono那句话前面一定要加sudo，除非你是root用户，否则会报奇葩错误。如果一切顺利，你将看到下面的样子。 ​ 这里please enter your nat123 username(enter x to exit): 输入你的nat123用户名please enter your nat123 password(enter x to exit):输入你的nat123密码，吐槽坑点，密码竟然是明文。 ​ 如果你的用户名密码都正确，你会看到下面的界面(官网盗图)，按住Ctrl键，并依次先按A，再按D，退出当前窗口就OK了。 那么此时基础环境就算搭建完了，下面你需要的是一个nat123的端口映射配置。 2.配置nat123端口 首先你需要登录你的nat123官网账户 点我带你飞 然后在用户中心中选择端口映射添加 然后按照下图所示进行填写 ​ 这里说明一下，应用类型填写的是其他(非网站)，映射路线默认选择nat123，除非你有VIP专线，应用名称就随你咯。值得注意的是，这里的内网端口这样填写，由于我做映射的目的是能够在外网SSH到我的树莓派，所以需要将我的localhost上的22端口映射出去，因而这里我将内网端口设为22，内网地址设为localhost。 ​ 再举个栗子，现在我的树莓派上有一个运行在localhost的5000端口上的flask服务器(一个python框架)程序，那么我想在外网请求这个服务器，那么我们就将内网端口设为5000，内网地址设为localhost。 ​ 然后其他的填写默认就行，如果像做域名解析的，请按照官网的自主域名到万网上去解析，这里不再多言~。 点击确认保存 ​ 那么从上图可以看到现在我有两个端口映射一个是flask的5000端口映射，一个是SSH的22端口映射。那么如果一切顺利的话(已经开了nat123客户端服务并且做好了端口映射)那么现在你就可以通过在SSH应用中输入你的外网域名，和外网端口连接到你的树莓派中去了，或者通过外网域名，外网端口访问你的服务器应用。 3.将树莓派上的nat123服务设为开机自启动或断网重连 ​ 其实到上面为止，已经达到了我们想要的外网访问内网服务器的效果了，那么现在我们还不满足，想让他只用一次配置就可以永久稳定的生效，那么我们就把它配置成开机自启动并支持断网重连。之前看了官网上的方法，发现并没卵用，然后在刷官方论坛的时候发现了一个解决方案(论坛15楼的办法)，虽然可能是因为版本老了的原因，直接用并不成功，但是给了我一个很好的思路。下面介绍一下我的方法。 首先sudo apt-get install expect安装expect支持 然后进入nat123安装目录(本文是/mnt)，新建一个脚本起名为expect.sh，执行这条命令sudo vim /mnt/expect.sh 然后将下列代码输入到这个脚本中去: ​ 这里需要注意的是username=&quot;&quot;“”里面输入你的nat123用户名mypwd=&quot;&quot;“”里面输入你的nat123账号密码，值得一提的是，请看\&quot;please enter your nat123 username(enter x to exit):\&quot; {和\&quot;please enter your nat123 password(enter x to exit):\&quot; {这两句话一定要和你执行了sudo mono nat123linux.sh之后的输入提示相一致，具体在哪呢请看代码下面的那张图片。 12345678910111213141516171819202122#!/bin/bashusername="你的nat123用户名"mypwd="你的nat123密码"cmdnat123="sudo mono /mnt/nat123linux.sh"expect -c"spawn $cmdnat123while &#123; 1 &#125; &#123; expect &#123; \"please enter your nat123 username(enter x to exit):\" &#123; send \"$username\r\"; &#125; \"please enter your nat123 password(enter x to exit):\" &#123; send \"$mypwd\r\"; &#125; eof &#123; send \"exit\r\"; &#125; &#125; sleep 5;&#125; 写好之后可以执行bash expect.sh语句来执行以下这个脚本，如果执行之后它带你来到了让你输入ctrl+AD退出的那个界面，就说明这个脚本写的成功了。 然后将脚本执行写入开机启动bash 打开rc.local写入开机执行命令。 1$ sudo vim /etc/rc.local 这里在exit(0)这句之前一行写上sudo bash /mnt/expect.sh就OK了 那么如果顺利的话重启树莓派等待30s左右时间，你就可以直接通过外网访问你的树莓派了~，然后你也可以把网线拔了重插，等个5s左右，发现也可以重连，那么就一切OK了。 总结此文第二遍写于2016年4月21日10:10:13，倒霉的typora昨晚上写到2点多变成乱码了。]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的博客]]></title>
    <url>%2Fpost%2F57092.html</url>
    <content type="text"></content>
  </entry>
</search>
